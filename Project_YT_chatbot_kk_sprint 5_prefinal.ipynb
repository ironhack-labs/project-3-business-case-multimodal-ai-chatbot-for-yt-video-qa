{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' !pip install pytube\\n!pip install openai-whisper\\n!pip install langchain\\n!pip install openai\\n!pip install python-dotenv '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" !pip install pytube\n",
    "!pip install openai-whisper\n",
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install python-dotenv \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' !pip install -U langchain-community\\n!pip install pinecone-client\\n!pip install --upgrade pytube\\n!pip install yt-dlp\\n!pip install imageio[ffmpeg] '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" !pip install -U langchain-community\n",
    "!pip install pinecone-client\n",
    "!pip install --upgrade pytube\n",
    "!pip install yt-dlp\n",
    "!pip install imageio[ffmpeg] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, torch\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Load Environment Variable\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set the environment variable directly in the script\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Langsmith (make sure it's installed first)\n",
    "import langsmith  # Importing Langsmith's library\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "from langsmith.schemas import Example, Run\n",
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"prj-yt-chatbot-qa\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter LANGCHAIN_API_KEY\")        # Authenticate with Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ[\"LANGCHAIN_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 1/8: https://www.youtube.com/watch?v=-RIfno4Sp0g\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=-RIfno4Sp0g\n",
      "[youtube] -RIfno4Sp0g: Downloading webpage\n",
      "[youtube] -RIfno4Sp0g: Downloading ios player API JSON\n",
      "[youtube] -RIfno4Sp0g: Downloading mweb player API JSON\n",
      "[youtube] -RIfno4Sp0g: Downloading player 81ef9024\n",
      "[youtube] -RIfno4Sp0g: Downloading m3u8 information\n",
      "[info] -RIfno4Sp0g: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_0\n",
      "[download] 100% of   48.70MiB in 00:00:00 at 51.13MiB/s    \n",
      "[ExtractAudio] Destination: audio_0.mp3\n",
      "Deleting original file audio_0 (pass -k to keep)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 57.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed transcription for video 1\n",
      "Processing video 2/8: https://www.youtube.com/watch?v=3SCOVB1KDFc\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3SCOVB1KDFc\n",
      "[youtube] 3SCOVB1KDFc: Downloading webpage\n",
      "[youtube] 3SCOVB1KDFc: Downloading ios player API JSON\n",
      "[youtube] 3SCOVB1KDFc: Downloading mweb player API JSON\n",
      "[youtube] 3SCOVB1KDFc: Downloading m3u8 information\n",
      "[info] 3SCOVB1KDFc: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_1\n",
      "[download] 100% of   54.16MiB in 00:00:02 at 18.42MiB/s    \n",
      "[ExtractAudio] Destination: audio_1.mp3\n",
      "Deleting original file audio_1 (pass -k to keep)\n",
      "Completed transcription for video 2\n",
      "Processing video 3/8: https://www.youtube.com/watch?v=qMqP2IwhSnA\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=qMqP2IwhSnA\n",
      "[youtube] qMqP2IwhSnA: Downloading webpage\n",
      "[youtube] qMqP2IwhSnA: Downloading ios player API JSON\n",
      "[youtube] qMqP2IwhSnA: Downloading mweb player API JSON\n",
      "[youtube] qMqP2IwhSnA: Downloading m3u8 information\n",
      "[info] qMqP2IwhSnA: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_2\n",
      "[download] 100% of   60.30MiB in 00:00:03 at 19.41MiB/s    \n",
      "[ExtractAudio] Destination: audio_2.mp3\n",
      "Deleting original file audio_2 (pass -k to keep)\n",
      "Completed transcription for video 3\n",
      "Processing video 4/8: https://www.youtube.com/watch?v=5u9jOA3LCdU\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=5u9jOA3LCdU\n",
      "[youtube] 5u9jOA3LCdU: Downloading webpage\n",
      "[youtube] 5u9jOA3LCdU: Downloading ios player API JSON\n",
      "[youtube] 5u9jOA3LCdU: Downloading mweb player API JSON\n",
      "[youtube] 5u9jOA3LCdU: Downloading m3u8 information\n",
      "[info] 5u9jOA3LCdU: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_3\n",
      "[download] 100% of   65.24MiB in 00:00:02 at 28.32MiB/s  \n",
      "[ExtractAudio] Destination: audio_3.mp3\n",
      "Deleting original file audio_3 (pass -k to keep)\n",
      "Completed transcription for video 4\n",
      "Processing video 5/8: https://www.youtube.com/watch?v=xeYEGePVaks\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=xeYEGePVaks\n",
      "[youtube] xeYEGePVaks: Downloading webpage\n",
      "[youtube] xeYEGePVaks: Downloading ios player API JSON\n",
      "[youtube] xeYEGePVaks: Downloading mweb player API JSON\n",
      "[youtube] xeYEGePVaks: Downloading m3u8 information\n",
      "[info] xeYEGePVaks: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_4\n",
      "[download] 100% of   60.71MiB in 00:00:02 at 24.18MiB/s    \n",
      "[ExtractAudio] Destination: audio_4.mp3\n",
      "Deleting original file audio_4 (pass -k to keep)\n",
      "Completed transcription for video 5\n",
      "Processing video 6/8: https://www.youtube.com/watch?v=EGGUg054ciI\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=EGGUg054ciI\n",
      "[youtube] EGGUg054ciI: Downloading webpage\n",
      "[youtube] EGGUg054ciI: Downloading ios player API JSON\n",
      "[youtube] EGGUg054ciI: Downloading mweb player API JSON\n",
      "[youtube] EGGUg054ciI: Downloading m3u8 information\n",
      "[info] EGGUg054ciI: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_5\n",
      "[download] 100% of   63.79MiB in 00:00:02 at 25.76MiB/s    \n",
      "[ExtractAudio] Destination: audio_5.mp3\n",
      "Deleting original file audio_5 (pass -k to keep)\n",
      "Completed transcription for video 6\n",
      "Processing video 7/8: https://www.youtube.com/watch?v=3f1iv7nsIsA\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3f1iv7nsIsA\n",
      "[youtube] 3f1iv7nsIsA: Downloading webpage\n",
      "[youtube] 3f1iv7nsIsA: Downloading ios player API JSON\n",
      "[youtube] 3f1iv7nsIsA: Downloading mweb player API JSON\n",
      "[youtube] 3f1iv7nsIsA: Downloading m3u8 information\n",
      "[info] 3f1iv7nsIsA: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_6\n",
      "[download] 100% of   51.37MiB in 00:00:07 at 7.01MiB/s     \n",
      "[ExtractAudio] Destination: audio_6.mp3\n",
      "Deleting original file audio_6 (pass -k to keep)\n",
      "Completed transcription for video 7\n",
      "Processing video 8/8: https://www.youtube.com/watch?v=WPSaiShNutU\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=WPSaiShNutU\n",
      "[youtube] WPSaiShNutU: Downloading webpage\n",
      "[youtube] WPSaiShNutU: Downloading ios player API JSON\n",
      "[youtube] WPSaiShNutU: Downloading mweb player API JSON\n",
      "[youtube] WPSaiShNutU: Downloading m3u8 information\n",
      "[info] WPSaiShNutU: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_7\n",
      "[download] 100% of   42.03MiB in 00:00:01 at 23.17MiB/s  \n",
      "[ExtractAudio] Destination: audio_7.mp3\n",
      "Deleting original file audio_7 (pass -k to keep)\n",
      "Completed transcription for video 8\n",
      "Individual transcription files are ready in the transcriptions folder.\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Load VIDEO, Download Audio, Transcribe into different text files!!!\n",
    "\n",
    "import os\n",
    "import yt_dlp\n",
    "from whisper import load_model\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Default configurations\n",
    "VIDEO_LINKS_FILE = \"video_links.txt\"\n",
    "AUDIO_OUTPUT_TEMPLATE = \"audio_{}\"\n",
    "WHISPER_MODEL = \"base\"\n",
    "TRANSCRIPT_DIR = \"transcriptions\"\n",
    "\n",
    "# Ensure directory exists for transcriptions\n",
    "os.makedirs(TRANSCRIPT_DIR, exist_ok=True)\n",
    "\n",
    "# Load YouTube links from the file\n",
    "def load_video_links(filename=VIDEO_LINKS_FILE):\n",
    "    with open(filename, 'r') as file:\n",
    "        return [link.strip() for link in file.readlines() if link.strip()]\n",
    "\n",
    "# Download audio from YouTube video\n",
    "def download_audio(url, output_name):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}],\n",
    "        'outtmpl': f\"{output_name}\",\n",
    "        'ffmpeg_location': '/usr/bin/ffmpeg'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "    return f\"{output_name}.mp3\"\n",
    "\n",
    "# Transcribe audio using Whisper\n",
    "def transcribe_audio_with_whisper(audio_path, model_name=WHISPER_MODEL):\n",
    "    model = load_model(model_name)\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Process each video and save individual transcriptions\n",
    "def transcribe_all_videos():\n",
    "    video_links = load_video_links(VIDEO_LINKS_FILE)\n",
    "\n",
    "    for i, video_url in enumerate(video_links):\n",
    "        print(f\"Processing video {i+1}/{len(video_links)}: {video_url}\")\n",
    "        audio_path = download_audio(video_url, AUDIO_OUTPUT_TEMPLATE.format(i))\n",
    "        \n",
    "        # Transcribe the audio and save each transcription to a separate file\n",
    "        transcription = transcribe_audio_with_whisper(audio_path)\n",
    "        transcript_file = os.path.join(TRANSCRIPT_DIR, f\"transcription_{i+1}.txt\")\n",
    "        \n",
    "        with open(transcript_file, 'w') as f:\n",
    "            f.write(transcription)\n",
    "        \n",
    "        print(f\"Completed transcription for video {i+1}\")\n",
    "\n",
    "# Execute transcription process only, without consolidation\n",
    "if __name__ == \"__main__\":\n",
    "    transcribe_all_videos()\n",
    "    print(\"Individual transcription files are ready in the transcriptions folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Define Pinecone as Vector DB & Define Index\n",
    "\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Securely fetch Pinecone API keys                    \n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Enter Pinecone API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ[\"PINECONE_API_KEY\"])\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index statistics: {'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "# Create INDEX \n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone client with the updated API\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "#spec = ServerlessSpec(cloud=\"aws\", region=\"us-west-1\")          # Specify Pinecone environment\n",
    "index_name = \"transcription-accnt-earn-index-no-consol\"             # Create New Index\n",
    "\n",
    "# Check if the index exists; create if it doesn’t\n",
    "if index_name not in [index_info[\"name\"] for index_info in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,  # OpenAI embedding dimension\n",
    "        metric=\"dotproduct\",\n",
    "        spec=spec\n",
    "    )\n",
    "    # Wait for the index to be ready\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        print(\"Waiting for index to be ready...\")\n",
    "        time.sleep(1)\n",
    "\n",
    "# Connect to the index for upsertion\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "print(\"Index statistics:\", index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization complete for transcription_1.txt.\n",
      "Vectorization complete for transcription_2.txt.\n",
      "Vectorization complete for transcription_3.txt.\n",
      "Vectorization complete for transcription_4.txt.\n",
      "Vectorization complete for transcription_5.txt.\n",
      "Vectorization complete for transcription_6.txt.\n",
      "Vectorization complete for transcription_7.txt.\n",
      "Vectorization complete for transcription_8.txt.\n",
      "All transcription files have been vectorized. Ready for storage in Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Vectorize and Embed each Transcription file\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Function to split and vectorize text from each transcription file\n",
    "def vectorize_individual_transcriptions(transcription_dir=\"transcriptions\"):\n",
    "    # Initialize list to hold vectorized segments for all files\n",
    "    all_vectorized_segments = []\n",
    "    \n",
    "    # Iterate through each transcription file in the directory\n",
    "    for filename in sorted(os.listdir(transcription_dir)):\n",
    "        file_path = os.path.join(transcription_dir, filename)\n",
    "        \n",
    "        # Skip directories and hidden files\n",
    "        if not os.path.isfile(file_path) or filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        # Read transcription text\n",
    "        with open(file_path, 'r') as f:\n",
    "            transcription_text = f.read()\n",
    "        \n",
    "        # Split text into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "        chunks = text_splitter.split_text(transcription_text)\n",
    "        \n",
    "        # Vectorize each chunk with metadata\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            embedding = embedding_model.embed_documents([chunk])[0]  # Get embedding\n",
    "            \n",
    "            # Metadata includes file identifier and chunk index for reference\n",
    "            metadata = {\n",
    "                \"file_name\": filename,\n",
    "                \"file_id\": filename.replace(\"transcription_\", \"\").replace(\".txt\", \"\"),\n",
    "                \"chunk_idx\": chunk_idx,\n",
    "            }\n",
    "            all_vectorized_segments.append({\n",
    "                \"embedding\": embedding,\n",
    "                \"text\": chunk,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "        \n",
    "        print(f\"Vectorization complete for {filename}.\")\n",
    "\n",
    "    return all_vectorized_segments\n",
    "\n",
    "# Execute the vectorization task\n",
    "if __name__ == \"__main__\":\n",
    "    vectorized_segments = vectorize_individual_transcriptions()\n",
    "    print(\"All transcription files have been vectorized. Ready for storage in Pinecone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0035a0fd4b477bbd8e021367b77162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserting segments to Pinecone:   0%|          | 0/349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All segments upserted into Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# STEP 4 - UPSERT embeddings to Pinecone\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Function to upsert vectorized segments to Pinecone\n",
    "def upsert_embeddings_to_pinecone(vectorized_segments, index):\n",
    "    for segment in tqdm(vectorized_segments, desc=\"Upserting segments to Pinecone\"):\n",
    "        # Prepare data for upsert\n",
    "        segment_id = segment[\"metadata\"][\"file_id\"] + \"_\" + str(segment[\"metadata\"][\"chunk_idx\"])  # Unique ID for each chunk\n",
    "        embedding = segment[\"embedding\"]\n",
    "        metadata = segment[\"metadata\"]\n",
    "        metadata[\"text\"] = segment[\"text\"]  # Adds the chunk's text content\n",
    "\n",
    "        # Upsert the segment into Pinecone\n",
    "        try:\n",
    "            index.upsert(vectors=[(segment_id, embedding, metadata)])\n",
    "        except Exception as e:\n",
    "            print(f\"Error upserting segment {segment_id}: {e}\")\n",
    "\n",
    "    print(\"All segments upserted into Pinecone.\")\n",
    "\n",
    "# Example usage:\n",
    "upsert_embeddings_to_pinecone(vectorized_segments, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index statistics: {'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 349}},\n",
      " 'total_vector_count': 349}\n"
     ]
    }
   ],
   "source": [
    "print(\"Index statistics:\", index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community\n",
    "#!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install tavily-python (not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "#os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\") or getpass.getpass(\"Enter Tavily API key: \")\n",
    "\n",
    "# Fetch the SerpAPI key securely\n",
    "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\") or getpass.getpass(\"Enter SerpAPI key: \")\n",
    "\n",
    "# Example usage in SerpAPI tool setup\n",
    "serp_api_key = os.environ[\"SERPAPI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ[\"SERPAPI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE ROUTER, RETRIEVE from VectorstoreDB and Retrieve from Google search functions!\n",
    "\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from serpapi import GoogleSearch\n",
    "import os\n",
    "\n",
    "# Initialize the embedding model and vector store\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Pinecone index is already set up and embeddings are upserted\n",
    "vector_store = Pinecone(index=index, embedding=embedding_model, text_key=\"text\")\n",
    "\n",
    "# Initialize the router instructions\n",
    "router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "\n",
    "The vectorstore contains documents related to Accenture company Quarterly results, Revenue, Earnings for the last four quarters, sales, digital projects, cloud projects and employee details.\n",
    "\n",
    "Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "\n",
    "Return JSON with a single key, \"datasource\", that is either 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "\n",
    "# Function to route query based on instructions\n",
    "def route_query(query):\n",
    "    routing_response = chat_model.invoke(\n",
    "        [SystemMessage(content=router_instructions), HumanMessage(content=query)]\n",
    "    )\n",
    "    route_decision = json.loads(routing_response.content)\n",
    "    return route_decision[\"datasource\"]\n",
    "\n",
    "# Function to retrieve from vector database\n",
    "def retrieve_from_vectorstore(query):\n",
    "    retriever = vector_store.as_retriever()\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "    document_text = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "    # Summarize the retrieved documents using LLM \n",
    "    prompt = (\n",
    "        \"Based on the following documents, provide a concise answer to the question:\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        \"Documents:\\n\"\n",
    "        f\"{document_text}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )    \n",
    "    answer_response = chat_model.invoke([HumanMessage(content=prompt)])\n",
    "    return answer_response.content\n",
    "    \n",
    "# Function to retrieve from Google search\n",
    "def web_search(query):\n",
    "    search = GoogleSearch({\n",
    "        \"q\": query,\n",
    "        \"api_key\": serp_api_key, \n",
    "        \"num\": 3  # Limit to top 3 results for conciseness\n",
    "    })\n",
    "    results = search.get_dict()\n",
    "\n",
    "    # Extract relevant text from search results\n",
    "    combined_text = \"\\n\".join(\n",
    "        result.get(\"snippet\", \"No snippet available\")\n",
    "        for result in results.get(\"organic_results\", [])\n",
    "    )\n",
    "    \n",
    "    # Summarize results using the LLM\n",
    "    prompt = (\n",
    "        f\"Summarize the following search results to answer the question:\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Results:\\n{combined_text}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    summary_response = chat_model.invoke([HumanMessage(content=prompt)])\n",
    "    return summary_response.content  \n",
    "\n",
    "# Main function to handle the query routing and retrieval\n",
    "def handle_query(query):\n",
    "    datasource = route_query(query)\n",
    "    \n",
    "    if datasource == \"vectorstore\":\n",
    "        print(\"Routing to Vectorstore...\")\n",
    "        return retrieve_from_vectorstore(query)\n",
    "    elif datasource == \"websearch\":\n",
    "        print(\"Routing to Web Search...\")\n",
    "        return web_search(query)\n",
    "    else:\n",
    "        return \"Unknown data source. Please check the router configuration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE ALL THE TOOLS for ReACT agent! \n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from serpapi import GoogleSearch\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "\n",
    "# Define the Web search tool using SerpAPI\n",
    "web_search_tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    func=web_search,\n",
    "    description=\"Retrieve relevant information using google web search for general or current event questions.\"\n",
    ")\n",
    "\n",
    "# Define tool for vector store retrieval\n",
    "vector_store_tool = Tool(\n",
    "    name=\"VectorStore Retrieval\",\n",
    "    func=retrieve_from_vectorstore,\n",
    "    description=\"Retrieve relevant documents from the vector store for questions about Accenture's earnings, revenue, sales, projects, and employee details.\",\n",
    ")\n",
    "\n",
    "# Revised prompt for the agent with improved answer formatting\n",
    "agent_prompt = \"\"\"\n",
    "You are an expert assistant specializing in Accenture's financial data and competitor analysis.\n",
    "\n",
    "For each user query:\n",
    "1. Answer strictly based on provided context or vector data.\n",
    "2. Be concise and use up to three sentences.\n",
    "3. If the answer is found, respond directly by starting with \"Final Answer:\" without additional steps or thoughts.\n",
    "\n",
    "Now, respond to the question:\n",
    "{user_query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define the ReAct agent with the custom prompt in each tool\n",
    "react_agent = initialize_agent(\n",
    "    tools=[vector_store_tool, web_search_tool],\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    llm=chat_model,\n",
    "    memory=memory,  # Integrate conversation memory\n",
    "    verbose=True,\n",
    "    max_iterations=5  # Restrict maximum depth to avoid infinite loops\n",
    "    handle_parsing_errors=True  # Retry on output parsing errors\n",
    ")\n",
    "\n",
    "# Main function to handle the query using the ReAct agent with the custom prompt\n",
    "def handle_query_with_agent(query):\n",
    "    # Format the query with the agent's prompt\n",
    "    formatted_prompt = agent_prompt.format(user_query=query)\n",
    "    \n",
    "    # Use the prompt in the agent invocation\n",
    "    try:\n",
    "        response = react_agent({\"input\": formatted_prompt})\n",
    "        # If a valid answer is provided, return it\n",
    "        if \"output\" in response:\n",
    "            return response[\"output\"]\n",
    "        else:\n",
    "            # Fallback message if no answer found within the max iterations\n",
    "            return \"I don't know. Unable to find the answer within the provided context.\"\n",
    "    except Exception as e:\n",
    "        # In case of unexpected errors, provide a helpful message\n",
    "        return f\"Error encountered: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Accenture Q&A Chatbot! Ask any question about Accenture.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should retrieve the revenue information for Q3 2024 from the vector store.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Q3 2024 revenue Accenture\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe projected revenue for Accenture in Q3 2024 is expected to be in the range of 15.85 to 16.45 billion, with an estimated 2 to 5 percent growth in local currency compared to the previous year.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the revenue information for Q3 2024.\n",
      "Final Answer: The projected revenue for Accenture in Q3 2024 is expected to be in the range of 15.85 to 16.45 billion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent (using VectorDB):\n",
      "Question: revenue of Q3 2024?\n",
      "Answer: The projected revenue for Accenture in Q3 2024 is expected to be in the range of 15.85 to 16.45 billion.\n",
      "\n",
      "Agent: Do you have more questions?\n",
      "Agent: Thank you for using the Q&A chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# INTERACTIVE CHAT FUNCTION using ReACT Agent - REFINED 3! Works perfect! TUESDAY\n",
    "\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize chat model and memory\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Function to clear memory\n",
    "def reset_memory():\n",
    "    memory.clear()\n",
    "    print(\"Memory has been reset.\")\n",
    "\n",
    "# Interactive chat function with datasource-based source tagging\n",
    "def chatbot_interaction():\n",
    "    print(\"Welcome to the Accenture Q&A Chatbot! Ask any question about Accenture.\")\n",
    "    while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"no\"]:\n",
    "            print(\"Agent: Thank you for using the Q&A chatbot. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Determine the datasource\n",
    "        datasource = route_query(user_query)\n",
    "        \n",
    "        # Tag source based on datasource\n",
    "        source_label = \"VectorDB\" if datasource == \"vectorstore\" else \"Google Search\" if datasource == \"websearch\" else \"Unknown Source\"        \n",
    "        \n",
    "        try:\n",
    "            # Get response from the handle_query_with_agent function instead of calling react_agent directly\n",
    "            answer = handle_query_with_agent(user_query)\n",
    "            print(f\"\\nAgent (using {source_label}):\\nQuestion: {user_query}\\nAnswer: {answer}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}\")\n",
    "            print(\"Agent: Unable to process the request. Please try again.\")\n",
    "        \n",
    "        print(\"\\nAgent: Do you have more questions?\")\n",
    "\n",
    "# Start the chatbot interaction\n",
    "chatbot_interaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://05e3852bb6023b35a3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://05e3852bb6023b35a3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find this specific financial information.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: \"Accenture EPS fourth quarter 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe Accenture EPS for the fourth quarter of 2024 is not provided in the documents.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should try using Google Search to find this specific financial information.\n",
      "Action: Google Search\n",
      "Action Input: \"Accenture EPS fourth quarter 2024\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAccenture reported an adjusted EPS of $2.79 for the fourth quarter of fiscal 2024, which was a 3% increase over the previous quarter.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now have the final answer.\n",
      "Final Answer: The EPS of Accenture in the fourth quarter of 2024 was $2.79.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information about Accenture's earnings.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: \"Accenture earnings Q3 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported strong growth in FY25, with full fiscal year bookings of $81 billion and revenues of $65 billion. For the third quarter of fiscal 2024, they expect revenues to be in the range of 16.1 billion to 16.7 billion, with an estimated 3 to 7 percent growth in local currency. For the full fiscal year 2024, they expect revenue to be in the range of 2 to 5 percent growth in local currency.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need more specific information about the EPS.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: \"Accenture EPS Q3 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn Q3 2024, Accenture is expecting revenues to be in the range of 16.1 billion to 16.7 billion, with an estimated 3 to 7 percent growth in local currency. Adjusted earnings per share for FY24 was $11.67, reflecting a 9% growth over FY22 EPS. The company is committed to delivering enduring shareholder value proposition and creating value for all stakeholders.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The EPS of Accenture in the third quarter of 2024 is expected to be in the range of $11.67.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find specific financial information about Accenture's revenue for the third quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture revenue Q3 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported revenues of $65 billion for FY24, representing 2% growth in local currency.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to find out if there are any more recent updates on Accenture's revenue for Q3 2024.\n",
      "Action: Google Search\n",
      "Action Input: Accenture revenue Q3 2024 latest news\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mIn the third quarter of fiscal 2024, Accenture reported new bookings of $21.06 billion, showing a significant increase. The company's revenue is forecasted to grow at an average rate of 6.8% over the next 3 years, slightly lower than the 9.0% growth forecast for the IT industry in the US.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe revenue of Accenture in the third quarter of 2024 is $21.06 billion, with a forecasted growth rate of 6.8% over the next 3 years.\n",
      "Final Answer: The revenue of Accenture in the third quarter of 2024 is $21.06 billion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to figure out what the original question is before I can answer it.\n",
      "Action: Google Search\n",
      "Action Input: \"original question\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAn original question is one that demonstrates an attempt to connect different ideas or identify a conflict. The phrase \"original question\" is acceptable in written English and can be used when referring to a question being answered or considered.\u001b[0m\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information on Accenture's earnings in the fourth quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture earnings Q4 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported revenues of $65 billion for the full fiscal year 2023, representing 2 percent growth in local currency. They expect revenue growth of 2 to 5 percent for the full fiscal year 2024. They have positioned themselves for strong growth in FY25, focusing on transformative technology like Generative AI. They have also received accolades for their environmental initiatives and community work, such as supporting refugees.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need more specific information for the earnings in the fourth quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture earnings Q4 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported strong results for the 4th quarter of fiscal 2023, with full fiscal year bookings of $81 billion and revenues of $65 billion. They also announced a business outlook for the first quarter of fiscal 2024, expecting revenues to be in the range of 15.85 to 16.45 billion. For the full fiscal year 2024, they expect revenue to be in the range of 2 to 5% growth in local currency. They emphasized their focus on Gen AI technology and their commitment to sustainability and community initiatives.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Accenture reported revenues of $65 billion for the full fiscal year 2023 and had strong results in the 4th quarter of fiscal 2023, with bookings of $81 billion and revenues of $65 billion. They expect revenue growth of 2 to 5 percent for the full fiscal year 2024.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the VectorStore Retrieval tool to find this specific financial information.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: \"Accenture EPS fourth quarter 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported strong growth in FY24, positioning the company for continued growth in FY25. Revenues for the year were $65 billion, representing a 2% growth in local currency. The company also reported full fiscal year bookings of $81 billion, a 14% growth in local currency. Additionally, Accenture has increased its base of deep client relationships, with 310 diamond clients, an increase of 10 from the previous year. The company expects to see continued growth in the next fiscal year.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe information provided did not specifically mention the EPS in the fourth quarter of 2024. I should try another query to find this specific number.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: \"Accenture EPS Q4 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBased on the documents provided, Accenture's EPS for Q4 2024 is expected to be positive, with a strong growth outlook for the full fiscal year. The company has demonstrated strong financial performance and is well-positioned for growth in the coming year.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe specific EPS number for Q4 2024 was not found in the documents. I should try to find this information using a different approach.\n",
      "Action: Google Search\n",
      "Action Input: \"Accenture EPS Q4 2024\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAccenture reported adjusted EPS of $2.79 for Q4 2024, representing a 3% increase over the previous quarter. The GAAP EPS for Q4 2024 was $2.66, compared to $2.15 in Q4 2023. The financial results were reported for the fourth quarter and full fiscal year ended August 31, 2024.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The EPS of Accenture in the fourth quarter of 2024 was $2.79 (adjusted EPS) and $2.66 (GAAP EPS).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Google Search to find a list of Accenture's competitors\n",
      "Action: Google Search\n",
      "Action Input: \"Accenture competitors\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mSome of Accenture's top competitors and alternatives include IBM, PwC, Tata Consultancy Services (TCS), Capgemini, Deloitte, Cognizant, SAP, and DXC Technology. Other notable competitors are KPMG and Ernst & Young (EY). Deloitte Consulting, Capgemini Services, and Cognizant are considered some of the best alternatives to Accenture.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now have a list of Accenture's competitors\n",
      "Final Answer: Some of Accenture's competitors include IBM, PwC, Tata Consultancy Services (TCS), Capgemini, Deloitte, Cognizant, SAP, DXC Technology, KPMG, and Ernst & Young (EY).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search for a list of top consulting companies to find out who else is in the consulting space.\n",
      "Action: Google Search\n",
      "Action Input: top consulting companies list\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe search results provide a list of top consulting firms, with McKinsey, Boston Consulting Group (BCG), and Bain & Company being commonly mentioned as the top firms. The results advise aspiring consultants not to limit themselves to just the top three firms and explore other top consulting companies as well.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should consider exploring other top consulting companies mentioned in the search results.\n",
      "Action: Google Search\n",
      "Action Input: top consulting companies other than McKinsey, BCG, and Bain\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mIn addition to McKinsey, BCG, and Bain, some other top consulting companies include EY (Ernst & Young), PwC, Deloitte, and KPMG, which are known as the Big 4 consulting firms.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know some of the other top consulting companies in the space.\n",
      "Final Answer: Some other top consulting companies besides McKinsey, BCG, and Bain include EY, PwC, Deloitte, and KPMG.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to understand what the question is asking for.\n",
      "Action: Ask for clarification.\n",
      "Action Input: \"Can you please provide more context or rephrase the question?\"\u001b[0m\n",
      "Observation: Ask for clarification. is not a valid tool, try one of [VectorStore Retrieval, Google Search].\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should try to use either VectorStore Retrieval or Google Search to find the answer to the question. Let me rephrase the question to see if it is more specific.\n",
      "Action: Google Search\n",
      "Action Input: The rephrased question\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mRephrasing a question allows for a more clear and direct answer by changing the wording or structure of the original question.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should now try to find the answer based on the rephrased question.\n",
      "Final Answer: The final answer to the original input question.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to compare the EPS of Q3 and Q4 for Accenture.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: EPS Q3, EPS Q4 Accenture\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn Q3, Accenture reported revenues of $65 billion for the year, representing 2% growth in local currency, while continuing to take market share. In Q4, Accenture reported revenues of $64 billion for the year, representing 8% growth in local currency and delivering adjusted EPS growth of 9%.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe EPS for Q4 is higher than Q3 for Accenture.\n",
      "Final Answer: The EPS for Q4 is higher than Q3 for Accenture.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information about Accenture's earnings for Q3 and Q4 in 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: EPS Q3 2024, EPS Q4 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEPS Q3 2024: Adjusted EPS of $2.69, 6% growth over EPS last year, adjusted operating margin of 13.8%, free cash flow of $2.2 billion, returned $1.8 billion to shareholders.\n",
      "\n",
      "EPS Q4 2024: Adjusted EPS of $11.67, 9% growth over FY22 EPS, adjusted operating margin of 15.4%, free cash flow of $9 billion, returned $7.2 billion to shareholders, invested $2.5 billion in acquisitions, revenue growth of 9% in local currency.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the EPS of Q3 and Q4 in 2024.\n",
      "Final Answer: EPS of Q3 2024 was $2.69 and EPS of Q4 2024 was $11.67.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find the earnings per share (EPS) information for Q3 and Q4 in 2023 for Accenture.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: EPS Q3 2023, EPS Q4 2023 Accenture\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFor Q3 2023, Accenture expects revenues to be in the range of 16.1 billion to 16.7 billion, with an estimated 3 to 7 percent growth in local currency. For Q4 2023, Accenture anticipates revenues in the range of 15.85 to 16.45 billion, with an estimated negative 2% to positive 2% growth in local currency.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to extract the specific EPS values for Q3 and Q4 from the information I found.\n",
      "Action: Extraction\n",
      "Action Input: EPS Q3 2023, EPS Q4 2023\u001b[0m\n",
      "Observation: Extraction is not a valid tool, try one of [VectorStore Retrieval, Google Search].\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find the specific EPS values for Q3 and Q4 in 2023 for Accenture.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: EPS Q3 2023 Accenture, EPS Q4 2023 Accenture\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEPS Q3 2023 for Accenture was not provided in the documents. EPS Q4 2023 for Accenture was 9% growth.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The EPS growth for Q3 2023 was not provided, and the EPS growth for Q4 2023 was 9% for Accenture.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the VectorStore Retrieval tool to find information specifically about Accenture's revenue for the fourth quarter of 2023.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture revenue Q4 2023\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture is expected to have revenues in the range of $15.75 billion to $16.35 billion for Q4 2023.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have the specific revenue range for Accenture's fourth quarter 2023.\n",
      "Final Answer: Accenture is expected to have revenues in the range of $15.75 billion to $16.35 billion for Q4 2023.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to get information on Accenture's revenue for a specific quarter.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture revenue Q3 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported revenues of $65 billion for FY24, representing 2% growth in local currency.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to find out what percentage of Accenture's revenue comes from digital services.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture digital services revenue percentage\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture digital services revenue percentage is not explicitly mentioned in the provided documents.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should try using Google Search to find the percentage of Accenture's revenue that comes from digital services.\n",
      "Action: Google Search\n",
      "Action Input: Accenture digital services revenue percentage\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe Accenture digital services revenue percentage for the fourth quarter of fiscal 2024 was 4% for Communications, Media & Technology and 3% for Financial Services.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The revenue of Accenture for the third quarter of 2024 was $65 billion, with digital services contributing 4% for Communications, Media & Technology and 3% for Financial Services.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# GRADIO only with TEXT - REFINED! Works Perfect!\n",
    "\n",
    "import json\n",
    "import openai\n",
    "import gradio as gr\n",
    "\n",
    "def gradio_chatbot(user_query):\n",
    "    # Determine the source using the routing function\n",
    "    datasource = route_query(user_query)\n",
    "    \n",
    "    # Tag source based on the datasource returned\n",
    "    source_label = \"VectorDB\" if datasource == \"vectorstore\" else \"Google Search\" if datasource == \"websearch\" else \"Unknown Source\"\n",
    "    \n",
    "    try:\n",
    "        # Get response from the ReAct agent\n",
    "        response = react_agent({\"input\": user_query})\n",
    "\n",
    "        # Check if the response is not empty and parse the response JSON if possible\n",
    "        if response and \"output\" in response:\n",
    "            output_content = response[\"output\"]\n",
    "            \n",
    "            # Attempt to parse JSON response if available\n",
    "            try:\n",
    "                parsed_response = json.loads(output_content) if isinstance(output_content, str) else output_content\n",
    "                answer = parsed_response.get(\"answer\", output_content) if isinstance(parsed_response, dict) else output_content\n",
    "            except json.JSONDecodeError:\n",
    "                answer = output_content  # Fallback to raw output if JSON parsing fails\n",
    "\n",
    "            return f\"Answer (from {source_label}):\\n{answer}\"\n",
    "\n",
    "        else:\n",
    "            return \"Error: Received an empty or unexpected response format from the agent.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Capture any other errors\n",
    "        return f\"Error encountered: {str(e)}\"\n",
    "\n",
    "# Create the Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_chatbot,  # Function to call\n",
    "    inputs=\"text\",      # Input type: text\n",
    "    outputs=\"text\",     # Output type: text\n",
    "    title=\"Accenture Q&A Chatbot\",\n",
    "    description=\"Ask any question about Accenture and get answers with sourced information.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 14:46:20.283141: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-06 14:46:20.283212: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-06 14:46:20.284410: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 14:46:20.291897: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 14:46:21.516180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# GISKARD EVALUATION BELOW !!!\n",
    "\n",
    "#!pip install --default-timeout=100 \"giskard[llm]\"\n",
    "from giskard.rag import KnowledgeBase, generate_testset, evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing to Vectorstore...\n",
      "Routing to Vectorstore...\n",
      "Routing to Web Search...\n",
      "Routing to Web Search...\n",
      "Routing to Web Search...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The earnings of Accenture in the fourth quarte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The EPS of Accenture in the fourth quarter of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some of the competitors of Accenture in the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The size of GenAi project deals for Accenture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The total employee count in Accenture today is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The earnings of Accenture in the fourth quarte...\n",
       "1  The EPS of Accenture in the fourth quarter of ...\n",
       "2  Some of the competitors of Accenture in the co...\n",
       "3  The size of GenAi project deals for Accenture ...\n",
       "4  The total employee count in Accenture today is..."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the sample questions to test with `handle_query`\n",
    "# Define the list of queries to retrieve answers for\n",
    "queries = [\n",
    "    \"What is the Earnings of Accenture in fourth quarter of 2024?\",\n",
    "    \"What is the EPS of Accenture in fourth quarter of 2024?\",\n",
    "    \"Who are the competitors of Accenture?\",\n",
    "    \"What is the size of Genai project deals for Accenture in year 2024?\",\n",
    "    \"what is the total employee count in Accenture today??\"    \n",
    "]\n",
    "\n",
    "# Collect answers by running each query through the handle_query function\n",
    "answers = [handle_query(query) for query in queries]\n",
    "\n",
    "# Create a DataFrame with only the answers\n",
    "df_giskard = pd.DataFrame(answers, columns=[\"text\"])\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "df_giskard.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build KNOWLEDGEBASE for Giskard\n",
    "\n",
    "kb_giskard = KnowledgeBase(df_giskard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-06 14:58:15,128 pid:40 MainThread giskard.rag  INFO     Finding topics in the knowledge base.\n",
      "2024-11-06 14:58:15,774 pid:40 MainThread giskard.rag  INFO     Found 1 topics in the knowledge base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:2462: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802006b0f4654390a3ed43f710a95b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating questions:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Question using Giskard\n",
    "\n",
    "test_questions = generate_testset(\n",
    "    kb_giskard,\n",
    "    num_questions=10,\n",
    "    agent_description=\"Accenture Q&A chat assistant related to Quarterly Earnings, and general information like competition.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_context</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83f09d34-d6b1-4f19-a20c-05cb37d609ea</th>\n",
       "      <td>What is the size of GenAi project deals for Ac...</td>\n",
       "      <td>The size of GenAi project deals for Accenture ...</td>\n",
       "      <td>Document 3: The size of GenAi project deals fo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'simple', 'seed_document_id'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74a5c79f-7d26-4023-b011-49affd903c6f</th>\n",
       "      <td>Who are some of Accenture's competitors in the...</td>\n",
       "      <td>Some of the competitors of Accenture in the co...</td>\n",
       "      <td>Document 2: Some of the competitors of Accentu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'simple', 'seed_document_id'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291d1f5-1048-403b-a39f-3d810518a6fa</th>\n",
       "      <td>Could you name a few companies that pose a com...</td>\n",
       "      <td>Some of the competitors of Accenture in the co...</td>\n",
       "      <td>Document 2: Some of the competitors of Accentu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'complex', 'seed_document_id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c91ee639-bcdc-405e-8078-e75772e043c0</th>\n",
       "      <td>Could you provide me with the specific monetar...</td>\n",
       "      <td>The earnings of Accenture in the fourth quarte...</td>\n",
       "      <td>Document 0: The earnings of Accenture in the f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'complex', 'seed_document_id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172e63e6-06e9-40b5-b71c-d7da324d44f7</th>\n",
       "      <td>Considering the unavailability of the EPS for ...</td>\n",
       "      <td>The total employee count in Accenture today is...</td>\n",
       "      <td>Document 4: The total employee count in Accent...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               question  \\\n",
       "id                                                                                        \n",
       "83f09d34-d6b1-4f19-a20c-05cb37d609ea  What is the size of GenAi project deals for Ac...   \n",
       "74a5c79f-7d26-4023-b011-49affd903c6f  Who are some of Accenture's competitors in the...   \n",
       "3291d1f5-1048-403b-a39f-3d810518a6fa  Could you name a few companies that pose a com...   \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0  Could you provide me with the specific monetar...   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7  Considering the unavailability of the EPS for ...   \n",
       "\n",
       "                                                                       reference_answer  \\\n",
       "id                                                                                        \n",
       "83f09d34-d6b1-4f19-a20c-05cb37d609ea  The size of GenAi project deals for Accenture ...   \n",
       "74a5c79f-7d26-4023-b011-49affd903c6f  Some of the competitors of Accenture in the co...   \n",
       "3291d1f5-1048-403b-a39f-3d810518a6fa  Some of the competitors of Accenture in the co...   \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0  The earnings of Accenture in the fourth quarte...   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7  The total employee count in Accenture today is...   \n",
       "\n",
       "                                                                      reference_context  \\\n",
       "id                                                                                        \n",
       "83f09d34-d6b1-4f19-a20c-05cb37d609ea  Document 3: The size of GenAi project deals fo...   \n",
       "74a5c79f-7d26-4023-b011-49affd903c6f  Document 2: Some of the competitors of Accentu...   \n",
       "3291d1f5-1048-403b-a39f-3d810518a6fa  Document 2: Some of the competitors of Accentu...   \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0  Document 0: The earnings of Accenture in the f...   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7  Document 4: The total employee count in Accent...   \n",
       "\n",
       "                                     conversation_history  \\\n",
       "id                                                          \n",
       "83f09d34-d6b1-4f19-a20c-05cb37d609ea                   []   \n",
       "74a5c79f-7d26-4023-b011-49affd903c6f                   []   \n",
       "3291d1f5-1048-403b-a39f-3d810518a6fa                   []   \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0                   []   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7                   []   \n",
       "\n",
       "                                                                               metadata  \n",
       "id                                                                                       \n",
       "83f09d34-d6b1-4f19-a20c-05cb37d609ea  {'question_type': 'simple', 'seed_document_id'...  \n",
       "74a5c79f-7d26-4023-b011-49affd903c6f  {'question_type': 'simple', 'seed_document_id'...  \n",
       "3291d1f5-1048-403b-a39f-3d810518a6fa  {'question_type': 'complex', 'seed_document_id...  \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0  {'question_type': 'complex', 'seed_document_id...  \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7  {'question_type': 'distracting element', 'seed...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_questions = test_questions.to_pandas()\n",
    "df_test_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_agent(question, history=None):\n",
    "    # Call the react_agent with question input\n",
    "    try:\n",
    "        response = react_agent.invoke({\"input\": question})  # `invoke` ensures it’s a direct LLM call\n",
    "        return response[\"output\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during agent invocation: {e}\")\n",
    "        return \"Unable to process the question at this time.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16818b6f66c544ca94e3df81369722db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Asking questions to the agent:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find information about Accenture's projects and specifically the GenAi project deals in 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: GenAi project deals size in 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe GenAi project is expected to see significant growth in 2024, with over $2 billion in sales year to date and $500 million in revenue year to date. The company has invested in strategic acquisitions to expand into new growth areas and is positioned to lead in the early stages of GenAi technology. The company is focused on accelerating digital core transformation and sees opportunities for growth in helping companies with data migration to the cloud. The company has a strong workforce of approximately 57,000 practitioners in data and AI, with a goal of reaching 80,000 by the end of FY26.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the information retrieved, the size of GenAi project deals for Accenture in 2024 is over $2 billion in sales year to date and $500 million in revenue year to date.\n",
      "Final Answer: The size of GenAi project deals for Accenture in the year 2024 is over $2 billion in sales and $500 million in revenue.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Google Search to find information on Accenture's competitors in the consulting industry.\n",
      "Action: Google Search\n",
      "Action Input: \"Accenture competitors in consulting industry\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mSome of the top competitors to Accenture in the consulting industry include IBM, PwC, Tata Consultancy Services, Capgemini, Deloitte, Cognizant, SAP, and DXC Technology. Other notable competitors are IBM Consulting and Deloitte Consulting.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have a list of Accenture's competitors in the consulting industry now.\n",
      "Final Answer: Some of Accenture's competitors in the consulting industry include IBM, PwC, Tata Consultancy Services, Capgemini, Deloitte, Cognizant, SAP, and DXC Technology.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search for a list of companies that are considered competitors to Accenture in the consulting sector.\n",
      "Action: Google Search\n",
      "Action Input: List of consulting companies that compete with Accenture\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mSome consulting companies that compete with Accenture include IBM, PwC, Tata Consultancy Services (TCS), Capgemini, Deloitte, Cognizant, SAP, and DXC Technology. Other competitors and alternatives to Accenture include KPMG, Ernst & Young (EY), and SAP. The top Accenture alternatives are Deloitte Consulting, Capgemini Services, and Cognizant.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Some companies that pose a competitive threat to Accenture within the consulting sector include IBM, PwC, Tata Consultancy Services (TCS), Capgemini, Deloitte, Cognizant, SAP, and DXC Technology.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information about Accenture's earnings.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture earnings last quarter 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn the last quarter of fiscal year 2024, Accenture reported revenues of $65 billion, representing 2 percent growth in local currency. They also achieved significant milestones in their commitment to net zero and helping communities, such as welcoming refugees and supporting job seekers. The outlook for the first quarter of fiscal year 2024 is revenues in the range of $15.85 to $16.45 billion, with an estimated negative 2% to positive 2% growth in local currency. For the full fiscal year 2024, Accenture expects revenue growth of 2 to 5% in local currency.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now have the information I need to calculate Accenture's earnings in the last quarter of 2024.\n",
      "Final Answer: Accenture earned $65 billion in the last quarter of the year 2024.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe total employee count is likely to be available in the VectorStore Retrieval data.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Employee count in Accenture\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApproximately 733,000 employees work at Accenture.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Approximately 733,000 employees work at Accenture.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to compare the current total employee count at Accenture with its competitors like IBM Consulting, Deloitte, and Capgemini.\n",
      "Action: Google Search\n",
      "Action Input: \"Accenture total employee count\" \"IBM Consulting total employee count\" \"Deloitte total employee count\" \"Capgemini total employee count\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAccenture has a total of 774,000 employees as of fiscal year 2024, making it the company with the highest employee count among the mentioned companies. IBM Global Services also has a significant number of employees, but the exact figure is not provided in the search results. Deloitte and Capgemini's total employee counts are not mentioned in the search results.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the search results, Accenture has the highest total employee count compared to IBM Consulting, Deloitte, and Capgemini.\n",
      "Final Answer: Accenture has a total of 774,000 employees as of fiscal year 2024, making it the company with the highest employee count among the mentioned companies.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information on Accenture's project deals for the GenAi project in 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture GenAi project deals 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture's GenAI project is a significant investment aimed at transforming technology in the next decade. The company has successfully positioned itself for strong growth in FY25 through client partnerships and talent development. Accenture is focused on training its workforce in AI and GenAI, with a goal of reaching 80,000 practitioners by FY26. The company's responsible AI compliance program ensures ethical and sustainable use of AI technologies. Additionally, Accenture's collaboration with companies like McDonald's and BBBA demonstrates the potential for GenAI to drive innovation and enhance digital capabilities across various industries.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe information provided does not specifically mention the size of GenAi project deals for Accenture in 2024. I should try refining my search query to focus on the specific project deals size.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture GenAi project deals size 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture's Gen AI project is expected to have significant growth by 2024, with over $300 million in sales for the year and a focus on training employees in AI and Gen AI to drive innovation and reinvention for clients. The company is on track to meet its goal of having 80,000 practitioners by the end of FY26 and is investing in responsible AI practices to ensure ethical usage of AI technologies.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the information retrieved, the size of GenAi project deals for Accenture in the year 2024 is expected to be over $300 million. \n",
      "Final Answer: Over $300 million\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find the EPS information for Accenture.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture EPS Q4 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFor Q4 2024, Accenture expects revenues to be in the range of $15.75 billion to $16.35 billion, with an estimated 2% to 6% growth in local currency. Adjusted earnings per share for FY24 was $11.67, reflecting 9% growth over the previous year. The company is well-positioned for strong growth in FY25, with a focus on large-scale transformations and being a trusted AI partner for enterprises.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe EPS information for Accenture for the fourth quarter of 2024 is not provided in the retrieved documents. I should try a different approach\n",
      "Action: Google Search\n",
      "Action Input: Accenture EPS Q4 2024\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mFor Q4 2024, Accenture reported an adjusted EPS of $2.79, which was a 3% increase from the previous quarter. The diluted earnings per share (GAAP) for Q4 FY24 was $2.66 compared to $2.15 in Q4 FY23. The financial results were reported for the fourth quarter and full fiscal year ended August 31, 2024.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe EPS for Accenture in the fourth quarter of 2024 was $2.79.\n",
      "Final Answer: The EPS of Accenture for the fourth quarter of 2024 was $2.79.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find this specific financial information.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture earnings and EPS in Q4 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn Q4 2024, Accenture reported adjusted earnings per share of $11.67, reflecting 9% growth over the previous year.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the earnings and EPS of Accenture in the fourth quarter of 2024.\n",
      "Final Answer: In the fourth quarter of 2024, Accenture reported adjusted earnings per share of $11.67.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSize can refer to various aspects such as revenue, employee count, or market capitalization.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: \"Accenture size\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture has demonstrated growth and resilience in recent years, with impressive bookings and revenue figures. They have a significant workforce of approximately 733,000 employees and are focused on investing in their people and developing marketable skills, particularly in the area of Gen AI. They have a strong client base, with a large number of diamond clients and a focus on creating value for their stakeholders. Their leadership in the market is supported by their commitment to diversity and inclusion, as well as their innovation strategies.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe size of Accenture seems to refer to their employee count, which is around 733,000.\n",
      "Final Answer: Accenture has approximately 733,000 employees.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45752ee176794e838f543d86f7d229f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CorrectnessMetric evaluation:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report = evaluate(use_agent, testset=test_questions, knowledge_base=kb_giskard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>complex</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversational</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distracting element</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>situational</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     correctness\n",
       "question_type                   \n",
       "complex                      0.5\n",
       "conversational               0.0\n",
       "distracting element          0.5\n",
       "double                       0.0\n",
       "simple                       1.0\n",
       "situational                  0.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary with the results.\n",
    "report.correctness_by_question_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_context</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>metadata</th>\n",
       "      <th>agent_answer</th>\n",
       "      <th>correctness</th>\n",
       "      <th>correctness_reason</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c91ee639-bcdc-405e-8078-e75772e043c0</th>\n",
       "      <td>Could you provide me with the specific monetar...</td>\n",
       "      <td>The earnings of Accenture in the fourth quarte...</td>\n",
       "      <td>Document 0: The earnings of Accenture in the f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'complex', 'seed_document_id...</td>\n",
       "      <td>Accenture earned $65 billion in the last quart...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a specific amount for Accen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172e63e6-06e9-40b5-b71c-d7da324d44f7</th>\n",
       "      <td>Considering the unavailability of the EPS for ...</td>\n",
       "      <td>The total employee count in Accenture today is...</td>\n",
       "      <td>Document 4: The total employee count in Accent...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "      <td>Approximately 733,000 employees work at Accent...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that approximately 733,000 em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05780363-7a91-4cc3-a71a-68a3462a565a</th>\n",
       "      <td>As an investor, I'm interested in the size of ...</td>\n",
       "      <td>The size of GenAi project deals for Accenture ...</td>\n",
       "      <td>Document 3: The size of GenAi project deals fo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'situational', 'seed_documen...</td>\n",
       "      <td>Over $300 million</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that the size of GenAi projec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6f29920b-4e39-4732-aab3-d33301715ef7</th>\n",
       "      <td>As a financial analyst, I'm trying to find out...</td>\n",
       "      <td>The EPS of Accenture in the fourth quarter of ...</td>\n",
       "      <td>Document 1: The EPS of Accenture in the fourth...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'situational', 'seed_documen...</td>\n",
       "      <td>The EPS of Accenture for the fourth quarter of...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided an EPS value for Accenture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8d917784-ea8a-4422-ba08-b76eb97d472b</th>\n",
       "      <td>What were the earnings and EPS of Accenture in...</td>\n",
       "      <td>The earnings and EPS of Accenture in the fourt...</td>\n",
       "      <td>Document 0: The earnings of Accenture in the f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'double', 'original_question...</td>\n",
       "      <td>In the fourth quarter of 2024, Accenture repor...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided specific earnings per share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334932ce-a358-4e22-b2b0-6aee952daeda</th>\n",
       "      <td>What was their size?</td>\n",
       "      <td>The size of GenAi project deals for Accenture ...</td>\n",
       "      <td>Document 3: The size of GenAi project deals fo...</td>\n",
       "      <td>[{'role': 'user', 'content': 'I'm interested i...</td>\n",
       "      <td>{'question_type': 'conversational', 'seed_docu...</td>\n",
       "      <td>Accenture has approximately 733,000 employees.</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided the number of employees at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               question  \\\n",
       "id                                                                                        \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0  Could you provide me with the specific monetar...   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7  Considering the unavailability of the EPS for ...   \n",
       "05780363-7a91-4cc3-a71a-68a3462a565a  As an investor, I'm interested in the size of ...   \n",
       "6f29920b-4e39-4732-aab3-d33301715ef7  As a financial analyst, I'm trying to find out...   \n",
       "8d917784-ea8a-4422-ba08-b76eb97d472b  What were the earnings and EPS of Accenture in...   \n",
       "334932ce-a358-4e22-b2b0-6aee952daeda                               What was their size?   \n",
       "\n",
       "                                                                       reference_answer  \\\n",
       "id                                                                                        \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0  The earnings of Accenture in the fourth quarte...   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7  The total employee count in Accenture today is...   \n",
       "05780363-7a91-4cc3-a71a-68a3462a565a  The size of GenAi project deals for Accenture ...   \n",
       "6f29920b-4e39-4732-aab3-d33301715ef7  The EPS of Accenture in the fourth quarter of ...   \n",
       "8d917784-ea8a-4422-ba08-b76eb97d472b  The earnings and EPS of Accenture in the fourt...   \n",
       "334932ce-a358-4e22-b2b0-6aee952daeda  The size of GenAi project deals for Accenture ...   \n",
       "\n",
       "                                                                      reference_context  \\\n",
       "id                                                                                        \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0  Document 0: The earnings of Accenture in the f...   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7  Document 4: The total employee count in Accent...   \n",
       "05780363-7a91-4cc3-a71a-68a3462a565a  Document 3: The size of GenAi project deals fo...   \n",
       "6f29920b-4e39-4732-aab3-d33301715ef7  Document 1: The EPS of Accenture in the fourth...   \n",
       "8d917784-ea8a-4422-ba08-b76eb97d472b  Document 0: The earnings of Accenture in the f...   \n",
       "334932ce-a358-4e22-b2b0-6aee952daeda  Document 3: The size of GenAi project deals fo...   \n",
       "\n",
       "                                                                   conversation_history  \\\n",
       "id                                                                                        \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0                                                 []   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7                                                 []   \n",
       "05780363-7a91-4cc3-a71a-68a3462a565a                                                 []   \n",
       "6f29920b-4e39-4732-aab3-d33301715ef7                                                 []   \n",
       "8d917784-ea8a-4422-ba08-b76eb97d472b                                                 []   \n",
       "334932ce-a358-4e22-b2b0-6aee952daeda  [{'role': 'user', 'content': 'I'm interested i...   \n",
       "\n",
       "                                                                               metadata  \\\n",
       "id                                                                                        \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0  {'question_type': 'complex', 'seed_document_id...   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7  {'question_type': 'distracting element', 'seed...   \n",
       "05780363-7a91-4cc3-a71a-68a3462a565a  {'question_type': 'situational', 'seed_documen...   \n",
       "6f29920b-4e39-4732-aab3-d33301715ef7  {'question_type': 'situational', 'seed_documen...   \n",
       "8d917784-ea8a-4422-ba08-b76eb97d472b  {'question_type': 'double', 'original_question...   \n",
       "334932ce-a358-4e22-b2b0-6aee952daeda  {'question_type': 'conversational', 'seed_docu...   \n",
       "\n",
       "                                                                           agent_answer  \\\n",
       "id                                                                                        \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0  Accenture earned $65 billion in the last quart...   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7  Approximately 733,000 employees work at Accent...   \n",
       "05780363-7a91-4cc3-a71a-68a3462a565a                                  Over $300 million   \n",
       "6f29920b-4e39-4732-aab3-d33301715ef7  The EPS of Accenture for the fourth quarter of...   \n",
       "8d917784-ea8a-4422-ba08-b76eb97d472b  In the fourth quarter of 2024, Accenture repor...   \n",
       "334932ce-a358-4e22-b2b0-6aee952daeda     Accenture has approximately 733,000 employees.   \n",
       "\n",
       "                                      correctness  \\\n",
       "id                                                  \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0        False   \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7        False   \n",
       "05780363-7a91-4cc3-a71a-68a3462a565a        False   \n",
       "6f29920b-4e39-4732-aab3-d33301715ef7        False   \n",
       "8d917784-ea8a-4422-ba08-b76eb97d472b        False   \n",
       "334932ce-a358-4e22-b2b0-6aee952daeda        False   \n",
       "\n",
       "                                                                     correctness_reason  \n",
       "id                                                                                       \n",
       "c91ee639-bcdc-405e-8078-e75772e043c0  The agent provided a specific amount for Accen...  \n",
       "172e63e6-06e9-40b5-b71c-d7da324d44f7  The agent stated that approximately 733,000 em...  \n",
       "05780363-7a91-4cc3-a71a-68a3462a565a  The agent stated that the size of GenAi projec...  \n",
       "6f29920b-4e39-4732-aab3-d33301715ef7  The agent provided an EPS value for Accenture ...  \n",
       "8d917784-ea8a-4422-ba08-b76eb97d472b  The agent provided specific earnings per share...  \n",
       "334932ce-a358-4e22-b2b0-6aee952daeda  The agent provided the number of employees at ...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining the incorrect answers\n",
    "failures = report.get_failures()[:15]\n",
    "failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure 1: The agent provided a specific amount for Accenture's earnings in the last quarter of 2024, but the reference answer indicates that this information was not provided in the documents.\n",
      "\n",
      "Failure 2: The agent stated that approximately 733,000 employees work at Accenture, but the correct number is 774,000.\n",
      "\n",
      "Failure 3: The agent stated that the size of GenAi project deals for Accenture in the year 2024 was over $300 million, but should have said it was $2 billion.\n",
      "\n",
      "Failure 4: The agent provided an EPS value for Accenture in the fourth quarter of 2024, but the reference answer indicates that this information is not available in the provided documents.\n",
      "\n",
      "Failure 5: The agent provided specific earnings per share for Accenture in the fourth quarter of 2024, but the reference answer indicates that this information was not provided in the documents.\n",
      "\n",
      "Failure 6: The agent provided the number of employees at Accenture, but the user was asking about the size of the GenAi project deals for Accenture in 2024, which was $2 billion.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Giskard explains the reasons why it considers the answers to be incorrect.\n",
    "#failures['correctness_reason'].iloc[1]\n",
    "for i, reason in enumerate(failures['correctness_reason']):\n",
    "    print(f\"Failure {i+1}: {reason}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting whisper\n",
      "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from whisper) (1.16.0)\n",
      "Building wheels for collected packages: whisper\n",
      "  Building wheel for whisper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41121 sha256=bf6e3521a837bc3321020c65c120c8e8561aa32931ec8168c97d741b37779bd8\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/65/ee/4e6672aabfa486d3341a39a04f8f87c77e5156149299b5a7d0\n",
      "Successfully built whisper\n",
      "Installing collected packages: whisper\n",
      "Successfully installed whisper-1.1.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install openai==0.27.0\n",
    "# !pip install whisper # Install Whisper without OpenAI requirement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* Running on public URL: https://b306e0e2f8f7467908.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b306e0e2f8f7467908.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Working with local Whisper (/*** NOT using OpenAPI -  Works fine ***/\n",
    "\n",
    "import whisper\n",
    "import gradio as gr\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(\"base\")  # You can choose \"small\", \"medium\", etc.\n",
    "\n",
    "# Function to transcribe audio locally using Whisper\n",
    "def transcribe_audio_with_whisper(audio_path):\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Gradio Interface for Audio Transcription\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe_audio_with_whisper,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Record your question here\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Audio to Text Transcription\",\n",
    "    description=\"Record an audio query and get the transcribed text using Whisper.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* Running on public URL: https://1392029650c18d30ad.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1392029650c18d30ad.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transcribe Audio ONLY!\n",
    "\n",
    "import openai\n",
    "import gradio as gr\n",
    "\n",
    "# Function to transcribe audio using Whisper API\n",
    "def transcribe_audio(audio_path):\n",
    "    try:\n",
    "        with open(audio_path, \"rb\") as audio_file:\n",
    "            # Replace \"whisper-1\" with the appropriate Whisper model if needed\n",
    "            response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "            transcription = response.get(\"text\", \"Error: No transcription available.\")\n",
    "            return transcription\n",
    "    except Exception as e:\n",
    "        return f\"Error during transcription: {e}\"\n",
    "\n",
    "# Gradio Interface for Audio Transcription Only\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe_audio,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Record your question here\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Audio to Text Transcription\",\n",
    "    description=\"Record an audio query and get the transcribed text using Whisper.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://529a52f742182e5eec.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://529a52f742182e5eec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find the revenue information for Accenture's fourth quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture revenue Q4 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported revenues of $16.4 billion in Q4 2024, representing a 3% increase in US dollars and a 5% increase in local currency.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now have the final answer.\n",
      "Final Answer: Accenture reported revenues of $16.4 billion in the fourth quarter of 2024.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/__init__.py\", line 20, in init\n",
      "    eng = _activeEngines[driverName]\n",
      "          ~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/weakref.py\", line 136, in __getitem__\n",
      "    o = self.data[key]()\n",
      "        ~~~~~~~~~^^^^^\n",
      "KeyError: None\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2015, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1562, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_38/1511344083.py\", line 38, in gradio_chatbot\n",
      "    audio_output_path = text_to_speech(response)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_38/1511344083.py\", line 20, in text_to_speech\n",
      "    tts = pyttsx3.init()\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/__init__.py\", line 22, in init\n",
      "    eng = Engine(driverName, debug)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/engine.py\", line 30, in __init__\n",
      "    self.proxy = driver.DriverProxy(weakref.proxy(self), driverName, debug)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/driver.py\", line 50, in __init__\n",
      "    self._module = importlib.import_module(name)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/drivers/espeak.py\", line 12, in <module>\n",
      "    from . import _espeak, fromUtf8, toUtf8\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/drivers/_espeak.py\", line 48, in <module>\n",
      "    raise RuntimeError(\"This means you probably do not have eSpeak or eSpeak-ng installed!\")\n",
      "RuntimeError: This means you probably do not have eSpeak or eSpeak-ng installed!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2025, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1781, in postprocess_data\n",
      "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1736, in validate_outputs\n",
      "    raise ValueError(\n",
      "ValueError: A  function (gradio_chatbot) didn't return enough output values (needed: 2, returned: 1).\n",
      "    Output components:\n",
      "        [textbox, audio]\n",
      "    Output values returned:\n",
      "        [\"Error: No valid input provided. Please either type or speak your question.\"]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2025, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1781, in postprocess_data\n",
      "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1736, in validate_outputs\n",
      "    raise ValueError(\n",
      "ValueError: A  function (gradio_chatbot) didn't return enough output values (needed: 2, returned: 1).\n",
      "    Output components:\n",
      "        [textbox, audio]\n",
      "    Output values returned:\n",
      "        [\"Error: No valid input provided. Please either type or speak your question.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during transcription: \n",
      "\n",
      "You tried to access openai.Audio, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information about transcription errors in Accenture's earnings reports.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Transcription errors in Accenture's earnings reports\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture experienced transcription errors in its earnings reports, with discrepancies in the number of employees mentioned in the documents provided.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should find out more details about the transcription errors in Accenture's earnings reports.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Details about transcription errors in Accenture's earnings reports\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThere is no mention of transcription errors in Accenture's earnings reports in the provided documents.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should try to search for information about transcription errors in Accenture's earnings reports on Google.\n",
      "Action: Google Search\n",
      "Action Input: Transcription errors in Accenture's earnings reports\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe search results do not directly address transcription errors in Accenture's earnings reports.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/__init__.py\", line 20, in init\n",
      "    eng = _activeEngines[driverName]\n",
      "          ~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/weakref.py\", line 136, in __getitem__\n",
      "    o = self.data[key]()\n",
      "        ~~~~~~~~~^^^^^\n",
      "KeyError: None\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2015, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1562, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_38/1511344083.py\", line 38, in gradio_chatbot\n",
      "    audio_output_path = text_to_speech(response)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_38/1511344083.py\", line 20, in text_to_speech\n",
      "    tts = pyttsx3.init()\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/__init__.py\", line 22, in init\n",
      "    eng = Engine(driverName, debug)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/engine.py\", line 30, in __init__\n",
      "    self.proxy = driver.DriverProxy(weakref.proxy(self), driverName, debug)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/driver.py\", line 50, in __init__\n",
      "    self._module = importlib.import_module(name)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/drivers/espeak.py\", line 12, in <module>\n",
      "    from . import _espeak, fromUtf8, toUtf8\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pyttsx3/drivers/_espeak.py\", line 48, in <module>\n",
      "    raise RuntimeError(\"This means you probably do not have eSpeak or eSpeak-ng installed!\")\n",
      "RuntimeError: This means you probably do not have eSpeak or eSpeak-ng installed!\n"
     ]
    }
   ],
   "source": [
    "# Gradio with Audio input/output\n",
    "\n",
    "import gradio as gr\n",
    "import openai\n",
    "import json\n",
    "import pyttsx3\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Transcription function for audio input using Whisper API\n",
    "def transcribe_audio_with_whisper(audio_path):\n",
    "    try:\n",
    "        with open(audio_path, \"rb\") as audio_file:\n",
    "            response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "            transcription = response[\"text\"]\n",
    "            return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return \"Transcription Error\"\n",
    "\n",
    "# Text-to-speech function for generating audio output\n",
    "def text_to_speech(text):\n",
    "    tts = pyttsx3.init()\n",
    "    tts.save_to_file(text, 'response_audio.mp3')\n",
    "    return 'response_audio.mp3'\n",
    "\n",
    "# Main function to handle both text and audio input\n",
    "def gradio_chatbot(user_query=None, audio_query=None):\n",
    "    if audio_query is not None:\n",
    "        user_query = transcribe_audio_with_whisper(audio_query)\n",
    "        \n",
    "    if not user_query:\n",
    "        return \"Error: No valid input provided. Please either type or speak your question.\"\n",
    "    \n",
    "    # Use the chatbot with LangChain’s ReAct agent setup\n",
    "    datasource = route_query(user_query)\n",
    "    source_label = \"VectorDB\" if datasource == \"vectorstore\" else \"Google Search\" if datasource == \"websearch\" else \"Unknown Source\"\n",
    "    response = handle_query_with_agent(user_query)\n",
    "\n",
    "    # Generate audio response and return both text and audio\n",
    "    audio_output_path = text_to_speech(response)\n",
    "    return f\"Answer (from {source_label}):\\n{response}\", audio_output_path\n",
    "\n",
    "# Gradio interface with both text and audio inputs and outputs\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_chatbot,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Type your question here\"),          # Text input\n",
    "        gr.Audio(type=\"filepath\", label=\"Or speak your question here\")  # Audio input\n",
    "    ],\n",
    "    outputs=[\n",
    "        \"text\",                      # Text output for answer\n",
    "        gr.Audio(label=\"Audio Answer\")   # Audio output for TTS response\n",
    "    ],\n",
    "    title=\"Accenture Q&A Chatbot\",\n",
    "    description=\"Ask any question about Accenture and get answers with sourced information, through text or voice.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40/3036245762.py:108: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
      "/tmp/ipykernel_40/3036245762.py:109: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/tmp/ipykernel_40/3036245762.py:111: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  react_agent = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 1/4: https://www.youtube.com/watch?v=089JqnNmJq0\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=089JqnNmJq0\n",
      "[youtube] 089JqnNmJq0: Downloading webpage\n",
      "[youtube] 089JqnNmJq0: Downloading ios player API JSON\n",
      "[youtube] 089JqnNmJq0: Downloading mweb player API JSON\n",
      "[youtube] 089JqnNmJq0: Downloading player 4e23410d\n",
      "[youtube] 089JqnNmJq0: Downloading m3u8 information\n",
      "[info] 089JqnNmJq0: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_0\n",
      "[download] 100% of   49.02MiB in 00:00:01 at 43.39MiB/s    \n",
      "[ExtractAudio] Destination: audio_0.mp3\n",
      "Deleting original file audio_0 (pass -k to keep)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 181MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 of video 1\n",
      "Completed chunk 1 for video 1\n",
      "Processing chunk 2 of video 1\n",
      "Completed chunk 2 for video 1\n",
      "Processing chunk 3 of video 1\n",
      "Completed chunk 3 for video 1\n",
      "Processing chunk 4 of video 1\n",
      "Completed chunk 4 for video 1\n",
      "Processing chunk 5 of video 1\n",
      "Completed chunk 5 for video 1\n",
      "Processing chunk 6 of video 1\n",
      "Completed chunk 6 for video 1\n",
      "Processing chunk 7 of video 1\n",
      "Completed chunk 7 for video 1\n",
      "Processing chunk 8 of video 1\n",
      "Completed chunk 8 for video 1\n",
      "Processing chunk 9 of video 1\n",
      "Completed chunk 9 for video 1\n",
      "Processing chunk 10 of video 1\n",
      "Completed chunk 10 for video 1\n",
      "Processing chunk 11 of video 1\n",
      "Completed chunk 11 for video 1\n",
      "Processing chunk 12 of video 1\n",
      "Completed chunk 12 for video 1\n",
      "Processing chunk 13 of video 1\n",
      "Completed chunk 13 for video 1\n",
      "Processing chunk 14 of video 1\n",
      "Completed chunk 14 for video 1\n",
      "Processing chunk 15 of video 1\n",
      "Completed chunk 15 for video 1\n",
      "Processing chunk 16 of video 1\n",
      "Completed chunk 16 for video 1\n",
      "Processing chunk 17 of video 1\n",
      "Completed chunk 17 for video 1\n",
      "Processing chunk 18 of video 1\n",
      "Completed chunk 18 for video 1\n",
      "Processing chunk 19 of video 1\n",
      "Completed chunk 19 for video 1\n",
      "Processing chunk 20 of video 1\n",
      "Completed chunk 20 for video 1\n",
      "Processing chunk 21 of video 1\n",
      "Completed chunk 21 for video 1\n",
      "Processing chunk 22 of video 1\n",
      "Completed chunk 22 for video 1\n",
      "Processing chunk 23 of video 1\n",
      "Completed chunk 23 for video 1\n",
      "Processing chunk 24 of video 1\n",
      "Completed chunk 24 for video 1\n",
      "Processing chunk 25 of video 1\n",
      "Completed chunk 25 for video 1\n",
      "Processing chunk 26 of video 1\n",
      "Completed chunk 26 for video 1\n",
      "Processing chunk 27 of video 1\n",
      "Completed chunk 27 for video 1\n",
      "Processing chunk 28 of video 1\n",
      "Completed chunk 28 for video 1\n",
      "Processing chunk 29 of video 1\n",
      "Completed chunk 29 for video 1\n",
      "Processing chunk 30 of video 1\n",
      "Completed chunk 30 for video 1\n",
      "Processing chunk 31 of video 1\n",
      "Completed chunk 31 for video 1\n",
      "Processing chunk 32 of video 1\n",
      "Completed chunk 32 for video 1\n",
      "Processing chunk 33 of video 1\n",
      "Completed chunk 33 for video 1\n",
      "Processing chunk 34 of video 1\n",
      "Completed chunk 34 for video 1\n",
      "Processing chunk 35 of video 1\n",
      "Completed chunk 35 for video 1\n",
      "Processing chunk 36 of video 1\n",
      "Completed chunk 36 for video 1\n",
      "Processing chunk 37 of video 1\n",
      "Completed chunk 37 for video 1\n",
      "Processing chunk 38 of video 1\n",
      "Completed chunk 38 for video 1\n",
      "Processing video 2/4: https://www.youtube.com/watch?v=IskUO7MoV2s\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=IskUO7MoV2s\n",
      "[youtube] IskUO7MoV2s: Downloading webpage\n",
      "[youtube] IskUO7MoV2s: Downloading ios player API JSON\n",
      "[youtube] IskUO7MoV2s: Downloading mweb player API JSON\n",
      "[youtube] IskUO7MoV2s: Downloading m3u8 information\n",
      "[info] IskUO7MoV2s: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_1\n",
      "[download] 100% of   48.50MiB in 00:00:01 at 31.16MiB/s    \n",
      "[ExtractAudio] Destination: audio_1.mp3\n",
      "Deleting original file audio_1 (pass -k to keep)\n",
      "Processing chunk 1 of video 2\n",
      "Completed chunk 1 for video 2\n",
      "Processing chunk 2 of video 2\n",
      "Completed chunk 2 for video 2\n",
      "Processing chunk 3 of video 2\n",
      "Completed chunk 3 for video 2\n",
      "Processing chunk 4 of video 2\n",
      "Completed chunk 4 for video 2\n",
      "Processing chunk 5 of video 2\n",
      "Completed chunk 5 for video 2\n",
      "Processing chunk 6 of video 2\n",
      "Completed chunk 6 for video 2\n",
      "Processing chunk 7 of video 2\n",
      "Completed chunk 7 for video 2\n",
      "Processing chunk 8 of video 2\n",
      "Completed chunk 8 for video 2\n",
      "Processing chunk 9 of video 2\n",
      "Completed chunk 9 for video 2\n",
      "Processing chunk 10 of video 2\n",
      "Completed chunk 10 for video 2\n",
      "Processing chunk 11 of video 2\n",
      "Completed chunk 11 for video 2\n",
      "Processing chunk 12 of video 2\n",
      "Completed chunk 12 for video 2\n",
      "Processing chunk 13 of video 2\n",
      "Completed chunk 13 for video 2\n",
      "Processing chunk 14 of video 2\n",
      "Completed chunk 14 for video 2\n",
      "Processing chunk 15 of video 2\n",
      "Completed chunk 15 for video 2\n",
      "Processing chunk 16 of video 2\n",
      "Completed chunk 16 for video 2\n",
      "Processing chunk 17 of video 2\n",
      "Completed chunk 17 for video 2\n",
      "Processing chunk 18 of video 2\n",
      "Completed chunk 18 for video 2\n",
      "Processing chunk 19 of video 2\n",
      "Completed chunk 19 for video 2\n",
      "Processing chunk 20 of video 2\n",
      "Completed chunk 20 for video 2\n",
      "Processing chunk 21 of video 2\n",
      "Completed chunk 21 for video 2\n",
      "Processing chunk 22 of video 2\n",
      "Completed chunk 22 for video 2\n",
      "Processing chunk 23 of video 2\n",
      "Completed chunk 23 for video 2\n",
      "Processing chunk 24 of video 2\n",
      "Completed chunk 24 for video 2\n",
      "Processing chunk 25 of video 2\n",
      "Completed chunk 25 for video 2\n",
      "Processing chunk 26 of video 2\n",
      "Completed chunk 26 for video 2\n",
      "Processing chunk 27 of video 2\n",
      "Completed chunk 27 for video 2\n",
      "Processing chunk 28 of video 2\n",
      "Completed chunk 28 for video 2\n",
      "Processing chunk 29 of video 2\n",
      "Completed chunk 29 for video 2\n",
      "Processing chunk 30 of video 2\n",
      "Completed chunk 30 for video 2\n",
      "Processing chunk 31 of video 2\n",
      "Completed chunk 31 for video 2\n",
      "Processing chunk 32 of video 2\n",
      "Completed chunk 32 for video 2\n",
      "Processing chunk 33 of video 2\n",
      "Completed chunk 33 for video 2\n",
      "Processing chunk 34 of video 2\n",
      "Completed chunk 34 for video 2\n",
      "Processing chunk 35 of video 2\n",
      "Completed chunk 35 for video 2\n",
      "Processing chunk 36 of video 2\n",
      "Completed chunk 36 for video 2\n",
      "Processing chunk 37 of video 2\n",
      "Completed chunk 37 for video 2\n",
      "Processing chunk 38 of video 2\n",
      "Completed chunk 38 for video 2\n",
      "Processing video 3/4: https://www.youtube.com/watch?v=BbqwBMct8-0\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=BbqwBMct8-0\n",
      "[youtube] BbqwBMct8-0: Downloading webpage\n",
      "[youtube] BbqwBMct8-0: Downloading ios player API JSON\n",
      "[youtube] BbqwBMct8-0: Downloading mweb player API JSON\n",
      "[youtube] BbqwBMct8-0: Downloading m3u8 information\n",
      "[info] BbqwBMct8-0: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_2\n",
      "[download] 100% of   50.65MiB in 00:00:02 at 17.72MiB/s    \n",
      "[ExtractAudio] Destination: audio_2.mp3\n",
      "Deleting original file audio_2 (pass -k to keep)\n",
      "Processing chunk 1 of video 3\n",
      "Completed chunk 1 for video 3\n",
      "Processing chunk 2 of video 3\n",
      "Completed chunk 2 for video 3\n",
      "Processing chunk 3 of video 3\n",
      "Completed chunk 3 for video 3\n",
      "Processing chunk 4 of video 3\n",
      "Completed chunk 4 for video 3\n",
      "Processing chunk 5 of video 3\n",
      "Completed chunk 5 for video 3\n",
      "Processing chunk 6 of video 3\n",
      "Completed chunk 6 for video 3\n",
      "Processing chunk 7 of video 3\n",
      "Completed chunk 7 for video 3\n",
      "Processing chunk 8 of video 3\n",
      "Completed chunk 8 for video 3\n",
      "Processing chunk 9 of video 3\n",
      "Completed chunk 9 for video 3\n",
      "Processing chunk 10 of video 3\n",
      "Completed chunk 10 for video 3\n",
      "Processing chunk 11 of video 3\n",
      "Completed chunk 11 for video 3\n",
      "Processing chunk 12 of video 3\n",
      "Completed chunk 12 for video 3\n",
      "Processing chunk 13 of video 3\n",
      "Completed chunk 13 for video 3\n",
      "Processing chunk 14 of video 3\n",
      "Completed chunk 14 for video 3\n",
      "Processing chunk 15 of video 3\n",
      "Completed chunk 15 for video 3\n",
      "Processing chunk 16 of video 3\n",
      "Completed chunk 16 for video 3\n",
      "Processing chunk 17 of video 3\n",
      "Completed chunk 17 for video 3\n",
      "Processing chunk 18 of video 3\n",
      "Completed chunk 18 for video 3\n",
      "Processing chunk 19 of video 3\n",
      "Completed chunk 19 for video 3\n",
      "Processing chunk 20 of video 3\n",
      "Completed chunk 20 for video 3\n",
      "Processing chunk 21 of video 3\n",
      "Completed chunk 21 for video 3\n",
      "Processing chunk 22 of video 3\n",
      "Completed chunk 22 for video 3\n",
      "Processing chunk 23 of video 3\n",
      "Completed chunk 23 for video 3\n",
      "Processing chunk 24 of video 3\n",
      "Completed chunk 24 for video 3\n",
      "Processing chunk 25 of video 3\n",
      "Completed chunk 25 for video 3\n",
      "Processing chunk 26 of video 3\n",
      "Completed chunk 26 for video 3\n",
      "Processing chunk 27 of video 3\n",
      "Completed chunk 27 for video 3\n",
      "Processing chunk 28 of video 3\n",
      "Completed chunk 28 for video 3\n",
      "Processing chunk 29 of video 3\n",
      "Completed chunk 29 for video 3\n",
      "Processing chunk 30 of video 3\n",
      "Completed chunk 30 for video 3\n",
      "Processing chunk 31 of video 3\n",
      "Completed chunk 31 for video 3\n",
      "Processing chunk 32 of video 3\n",
      "Completed chunk 32 for video 3\n",
      "Processing chunk 33 of video 3\n",
      "Completed chunk 33 for video 3\n",
      "Processing chunk 34 of video 3\n",
      "Completed chunk 34 for video 3\n",
      "Processing chunk 35 of video 3\n",
      "Completed chunk 35 for video 3\n",
      "Processing chunk 36 of video 3\n",
      "Completed chunk 36 for video 3\n",
      "Processing chunk 37 of video 3\n",
      "Completed chunk 37 for video 3\n",
      "Processing video 4/4: https://www.youtube.com/watch?v=hQGtKBQc_iE\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=hQGtKBQc_iE\n",
      "[youtube] hQGtKBQc_iE: Downloading webpage\n",
      "[youtube] hQGtKBQc_iE: Downloading ios player API JSON\n",
      "[youtube] hQGtKBQc_iE: Downloading mweb player API JSON\n",
      "[youtube] hQGtKBQc_iE: Downloading m3u8 information\n",
      "[info] hQGtKBQc_iE: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_3\n",
      "[download] 100% of   60.94MiB in 00:00:03 at 18.03MiB/s    \n",
      "[ExtractAudio] Destination: audio_3.mp3\n",
      "Deleting original file audio_3 (pass -k to keep)\n",
      "Processing chunk 1 of video 4\n",
      "Completed chunk 1 for video 4\n",
      "Processing chunk 2 of video 4\n",
      "Completed chunk 2 for video 4\n",
      "Processing chunk 3 of video 4\n",
      "Completed chunk 3 for video 4\n",
      "Processing chunk 4 of video 4\n",
      "Completed chunk 4 for video 4\n",
      "Processing chunk 5 of video 4\n",
      "Completed chunk 5 for video 4\n",
      "Processing chunk 6 of video 4\n",
      "Completed chunk 6 for video 4\n",
      "Processing chunk 7 of video 4\n",
      "Completed chunk 7 for video 4\n",
      "Processing chunk 8 of video 4\n",
      "Completed chunk 8 for video 4\n",
      "Processing chunk 9 of video 4\n",
      "Completed chunk 9 for video 4\n",
      "Processing chunk 10 of video 4\n",
      "Completed chunk 10 for video 4\n",
      "Processing chunk 11 of video 4\n",
      "Completed chunk 11 for video 4\n",
      "Processing chunk 12 of video 4\n",
      "Completed chunk 12 for video 4\n",
      "Processing chunk 13 of video 4\n",
      "Completed chunk 13 for video 4\n",
      "Processing chunk 14 of video 4\n",
      "Completed chunk 14 for video 4\n",
      "Processing chunk 15 of video 4\n",
      "Completed chunk 15 for video 4\n",
      "Processing chunk 16 of video 4\n",
      "Completed chunk 16 for video 4\n",
      "Processing chunk 17 of video 4\n",
      "Completed chunk 17 for video 4\n",
      "Processing chunk 18 of video 4\n",
      "Completed chunk 18 for video 4\n",
      "Processing chunk 19 of video 4\n",
      "Completed chunk 19 for video 4\n",
      "Processing chunk 20 of video 4\n",
      "Completed chunk 20 for video 4\n",
      "Processing chunk 21 of video 4\n",
      "Completed chunk 21 for video 4\n",
      "Processing chunk 22 of video 4\n",
      "Completed chunk 22 for video 4\n",
      "Processing chunk 23 of video 4\n",
      "Completed chunk 23 for video 4\n",
      "Processing chunk 24 of video 4\n",
      "Completed chunk 24 for video 4\n",
      "Processing chunk 25 of video 4\n",
      "Completed chunk 25 for video 4\n",
      "Processing chunk 26 of video 4\n",
      "Completed chunk 26 for video 4\n",
      "Processing chunk 27 of video 4\n",
      "Completed chunk 27 for video 4\n",
      "Processing chunk 28 of video 4\n",
      "Completed chunk 28 for video 4\n",
      "Processing chunk 29 of video 4\n",
      "Completed chunk 29 for video 4\n",
      "Processing chunk 30 of video 4\n",
      "Completed chunk 30 for video 4\n",
      "Processing chunk 31 of video 4\n",
      "Completed chunk 31 for video 4\n",
      "Processing chunk 32 of video 4\n",
      "Completed chunk 32 for video 4\n",
      "Processing chunk 33 of video 4\n",
      "Completed chunk 33 for video 4\n",
      "Processing chunk 34 of video 4\n",
      "Completed chunk 34 for video 4\n",
      "Processing chunk 35 of video 4\n",
      "Completed chunk 35 for video 4\n",
      "Processing chunk 36 of video 4\n",
      "Completed chunk 36 for video 4\n",
      "Processing chunk 37 of video 4\n",
      "Completed chunk 37 for video 4\n",
      "Processing chunk 38 of video 4\n",
      "Completed chunk 38 for video 4\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40/3036245762.py:123: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = react_agent({\"input\": user_query, \"transcriptions\": transcriptions})  # Send entire list to agent once\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI should transcribe all the earnings podcasts from the YouTube links provided to consolidate the results.\n",
      "Action: TranscribeAllVideos\n",
      "Action Input: List of YouTube links to earnings podcasts\u001b[0mProcessing video 1/4: https://www.youtube.com/watch?v=089JqnNmJq0\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=089JqnNmJq0\n",
      "[youtube] 089JqnNmJq0: Downloading webpage\n",
      "[youtube] 089JqnNmJq0: Downloading ios player API JSON\n",
      "[youtube] 089JqnNmJq0: Downloading mweb player API JSON\n",
      "[youtube] 089JqnNmJq0: Downloading m3u8 information\n",
      "[info] 089JqnNmJq0: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_0\n",
      "[download] 100% of   49.02MiB in 00:00:01 at 43.23MiB/s    \n",
      "[ExtractAudio] Destination: audio_0.mp3\n",
      "Deleting original file audio_0 (pass -k to keep)\n",
      "Processing chunk 1 of video 1\n",
      "Completed chunk 1 for video 1\n",
      "Processing chunk 2 of video 1\n",
      "Completed chunk 2 for video 1\n",
      "Processing chunk 3 of video 1\n",
      "Completed chunk 3 for video 1\n",
      "Processing chunk 4 of video 1\n",
      "Completed chunk 4 for video 1\n",
      "Processing chunk 5 of video 1\n",
      "Completed chunk 5 for video 1\n",
      "Processing chunk 6 of video 1\n",
      "Completed chunk 6 for video 1\n",
      "Processing chunk 7 of video 1\n",
      "Completed chunk 7 for video 1\n",
      "Processing chunk 8 of video 1\n",
      "Completed chunk 8 for video 1\n",
      "Processing chunk 9 of video 1\n",
      "Completed chunk 9 for video 1\n",
      "Processing chunk 10 of video 1\n",
      "Completed chunk 10 for video 1\n",
      "Processing chunk 11 of video 1\n",
      "Completed chunk 11 for video 1\n",
      "Processing chunk 12 of video 1\n",
      "Completed chunk 12 for video 1\n",
      "Processing chunk 13 of video 1\n",
      "Completed chunk 13 for video 1\n",
      "Processing chunk 14 of video 1\n",
      "Completed chunk 14 for video 1\n",
      "Processing chunk 15 of video 1\n",
      "Completed chunk 15 for video 1\n",
      "Processing chunk 16 of video 1\n",
      "Completed chunk 16 for video 1\n",
      "Processing chunk 17 of video 1\n",
      "Completed chunk 17 for video 1\n",
      "Processing chunk 18 of video 1\n",
      "Completed chunk 18 for video 1\n",
      "Processing chunk 19 of video 1\n",
      "Completed chunk 19 for video 1\n",
      "Processing chunk 20 of video 1\n",
      "Completed chunk 20 for video 1\n",
      "Processing chunk 21 of video 1\n",
      "Completed chunk 21 for video 1\n",
      "Processing chunk 22 of video 1\n",
      "Completed chunk 22 for video 1\n",
      "Processing chunk 23 of video 1\n",
      "Completed chunk 23 for video 1\n",
      "Processing chunk 24 of video 1\n",
      "Completed chunk 24 for video 1\n",
      "Processing chunk 25 of video 1\n",
      "Completed chunk 25 for video 1\n",
      "Processing chunk 26 of video 1\n",
      "Completed chunk 26 for video 1\n",
      "Processing chunk 27 of video 1\n",
      "Completed chunk 27 for video 1\n",
      "Processing chunk 28 of video 1\n",
      "Completed chunk 28 for video 1\n",
      "Processing chunk 29 of video 1\n",
      "Completed chunk 29 for video 1\n",
      "Processing chunk 30 of video 1\n",
      "Completed chunk 30 for video 1\n",
      "Processing chunk 31 of video 1\n",
      "Completed chunk 31 for video 1\n",
      "Processing chunk 32 of video 1\n",
      "Completed chunk 32 for video 1\n",
      "Processing chunk 33 of video 1\n",
      "Completed chunk 33 for video 1\n",
      "Processing chunk 34 of video 1\n",
      "Completed chunk 34 for video 1\n",
      "Processing chunk 35 of video 1\n",
      "Completed chunk 35 for video 1\n",
      "Processing chunk 36 of video 1\n",
      "Completed chunk 36 for video 1\n",
      "Processing chunk 37 of video 1\n",
      "Completed chunk 37 for video 1\n",
      "Processing chunk 38 of video 1\n",
      "Completed chunk 38 for video 1\n",
      "Processing video 2/4: https://www.youtube.com/watch?v=IskUO7MoV2s\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=IskUO7MoV2s\n",
      "[youtube] IskUO7MoV2s: Downloading webpage\n",
      "[youtube] IskUO7MoV2s: Downloading ios player API JSON\n",
      "[youtube] IskUO7MoV2s: Downloading mweb player API JSON\n",
      "[youtube] IskUO7MoV2s: Downloading m3u8 information\n",
      "[info] IskUO7MoV2s: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_1\n",
      "[download] 100% of   48.50MiB in 00:00:02 at 17.96MiB/s    \n",
      "[ExtractAudio] Destination: audio_1.mp3\n",
      "Deleting original file audio_1 (pass -k to keep)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 132\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\" def perform_transcription_task():\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    user_query = \"Transcribe all earnings podcasts from the provided YouTube links.\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    response = react_agent({\"input\": user_query})\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    print(\"Agent's Transcription:\", response[\"output\"]) \"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Start the transcription task\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[43mperform_transcription_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 123\u001b[0m, in \u001b[0;36mperform_transcription_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscribe all earnings podcasts from the provided YouTube links.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m transcriptions \u001b[38;5;241m=\u001b[39m transcribe_all_videos()  \u001b[38;5;66;03m# Get transcriptions directly\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mreact_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscriptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscriptions\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Send entire list to agent once\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Transcription:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    387\u001b[0m }\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py:1629\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1629\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1637\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1638\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1639\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py:1335\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1328\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1332\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1335\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1345\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py:1335\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1328\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1332\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1335\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1345\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py:1420\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py:1442\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[0;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1450\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py:689\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[1;32m    688\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(error_to_raise)\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[1;32m    690\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[1;32m    691\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py:657\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run):\n\u001b[1;32m    656\u001b[0m     tool_kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 657\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_and_artifact\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/simple.py:92\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, config, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m     91\u001b[0m         kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync invocation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "Cell \u001b[0;32mIn[7], line 70\u001b[0m, in \u001b[0;36mtranscribe_all_videos\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing video \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(video_links)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m download_audio(video_url, AUDIO_OUTPUT_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(i))\n\u001b[0;32m---> 70\u001b[0m full_transcription \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio_with_whisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m transcription_chunks \u001b[38;5;241m=\u001b[39m chunk_text(full_transcription)  \u001b[38;5;66;03m# Divide transcription\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transcription_chunks):\n",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m, in \u001b[0;36mtranscribe_audio_with_whisper\u001b[0;34m(audio_path, model_name)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribe_audio_with_whisper\u001b[39m(audio_path, model_name\u001b[38;5;241m=\u001b[39mWHISPER_MODEL):\n\u001b[1;32m     43\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_model(model_name)\n\u001b[0;32m---> 44\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:279\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    276\u001b[0m mel_segment \u001b[38;5;241m=\u001b[39m pad_or_trim(mel_segment, N_FRAMES)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[1;32m    278\u001b[0m decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_tokens[prompt_reset_since:]\n\u001b[0;32m--> 279\u001b[0m result: DecodingResult \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(result\u001b[38;5;241m.\u001b[39mtokens)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:195\u001b[0m, in \u001b[0;36mtranscribe.<locals>.decode_with_fallback\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m    192\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    194\u001b[0m options \u001b[38;5;241m=\u001b[39m DecodingOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, temperature\u001b[38;5;241m=\u001b[39mt)\n\u001b[0;32m--> 195\u001b[0m decode_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m needs_fallback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    199\u001b[0m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m decode_result\u001b[38;5;241m.\u001b[39mcompression_ratio \u001b[38;5;241m>\u001b[39m compression_ratio_threshold\n\u001b[1;32m    201\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py:824\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    822\u001b[0m     options \u001b[38;5;241m=\u001b[39m replace(options, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 824\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py:737\u001b[0m, in \u001b[0;36mDecodingTask.run\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    734\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_group, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(audio_features\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[0;32m--> 737\u001b[0m tokens, sum_logprobs, no_speech_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[1;32m    740\u001b[0m audio_features \u001b[38;5;241m=\u001b[39m audio_features[:: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_group]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py:687\u001b[0m, in \u001b[0;36mDecodingTask._main_loop\u001b[0;34m(self, audio_features, tokens)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_len):\n\u001b[0;32m--> 687\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    690\u001b[0m             i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mno_speech \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    691\u001b[0m         ):  \u001b[38;5;66;03m# save no_speech_probs\u001b[39;00m\n\u001b[1;32m    692\u001b[0m             probs_at_sot \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msot_index]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py:163\u001b[0m, in \u001b[0;36mPyTorchInference.logits\u001b[0;34m(self, tokens, audio_features)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_token_length:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py:242\u001b[0m, in \u001b[0;36mTextDecoder.forward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    239\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(xa\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 242\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m    245\u001b[0m logits \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    246\u001b[0m     x \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    247\u001b[0m )\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py:169\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    167\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_ln(x), mask\u001b[38;5;241m=\u001b[39mmask, kv_cache\u001b[38;5;241m=\u001b[39mkv_cache)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn:\n\u001b[0;32m--> 169\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    170\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_ln(x))\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py:112\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    109\u001b[0m     v \u001b[38;5;241m=\u001b[39m kv_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue]\n\u001b[1;32m    111\u001b[0m wv, qk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv_attention(q, k, v, mask)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwv\u001b[49m\u001b[43m)\u001b[49m, qk\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py:45\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLinear\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\n\u001b[1;32m     47\u001b[0m             x,\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[1;32m     50\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# New Code 11PM SATURDAY # THIS CODE worked using ReAct agent for Transcription, but went in Infinite LOOOOP\n",
    "\n",
    "\"\"\" import os\n",
    "import yt_dlp\n",
    "from whisper import load_model\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# Default configurations\n",
    "VIDEO_LINKS_FILE = \"video_links.txt\"  # File with YouTube links\n",
    "AUDIO_OUTPUT_TEMPLATE = \"audio_{}\"  # Template for audio file names without .mp3 extension\n",
    "WHISPER_MODEL = \"base\"  # Whisper model for transcription\n",
    "\n",
    "# Load YouTube links from the file\n",
    "def load_video_links(filename=VIDEO_LINKS_FILE):\n",
    "    with open(filename, 'r') as file:\n",
    "        video_links = [link.strip() for link in file.readlines() if link.strip()]\n",
    "    return video_links\n",
    "\n",
    "# Download audio from YouTube video\n",
    "def download_audio(url, output_name):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [\n",
    "            {\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',\n",
    "            }\n",
    "        ],\n",
    "        'outtmpl': output_name,  # Filename with .mp3 extension\n",
    "        'ffmpeg_location': '/usr/bin/ffmpeg'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "    return f\"{output_name}.mp3\"\n",
    "\n",
    "# Transcribe audio using Whisper\n",
    "def transcribe_audio_with_whisper(audio_path, model_name=WHISPER_MODEL):\n",
    "    model = load_model(model_name)\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# ReAct Tool for downloading and transcribing all videos\n",
    "# Split transcriptions into chunks below the token limit - Process in Batches!\n",
    "def chunk_text(text, max_length=1500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    for word in words:\n",
    "        chunk.append(word)\n",
    "        if len(\" \".join(chunk)) >= max_length:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = []\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n",
    "\n",
    "# Process each transcription in chunks\n",
    "def transcribe_all_videos(input_data=None): # Accept an optional argument\n",
    "    video_links = load_video_links(VIDEO_LINKS_FILE)\n",
    "    all_transcriptions = []\n",
    "    \n",
    "    for i, video_url in enumerate(video_links):\n",
    "        print(f\"Processing video {i+1}/{len(video_links)}: {video_url}\")\n",
    "        audio_path = download_audio(video_url, AUDIO_OUTPUT_TEMPLATE.format(i))\n",
    "        full_transcription = transcribe_audio_with_whisper(audio_path)\n",
    "        transcription_chunks = chunk_text(full_transcription)  # Divide transcription\n",
    "        \n",
    "        for j, chunk in enumerate(transcription_chunks):\n",
    "            # print(f\"Processing chunk {j+1} of video {i+1}\")\n",
    "            # Send chunk to the ReAct agent\n",
    "            # response = react_agent({\"input\": chunk}) /* Don't call Agent each time */\n",
    "            all_transcriptions.append({\"title\": f\"Video {i+1} - Chunk {j+1}\", \"transcription\": chunk})\n",
    "            print(f\"Completed chunk {j+1} for video {i+1}\")\n",
    "        \n",
    "    return all_transcriptions\n",
    "\n",
    "\n",
    "def transcribe_all_videos(input_data=None):  # Accept an optional argument\n",
    "    video_links = load_video_links(VIDEO_LINKS_FILE)\n",
    "    all_transcriptions = []\n",
    "    for i, video_url in enumerate(video_links):\n",
    "        print(f\"Processing video {i+1}/{len(video_links)}: {video_url}\")\n",
    "        audio_path = download_audio(video_url, AUDIO_OUTPUT_TEMPLATE.format(i))\n",
    "        transcription = transcribe_audio_with_whisper(audio_path)\n",
    "        all_transcriptions.append({\"title\": f\"Video {i+1}\", \"transcription\": transcription})\n",
    "        print(f\"Completed transcription for video {i+1}\")\n",
    "    return all_transcriptions\n",
    "\n",
    "# Define structured output for better organization\n",
    "response_schema = [\n",
    "    ResponseSchema(name=\"transcription\", description=\"Transcribed text from each video in structured format.\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "\n",
    "# Define tool for the agent\n",
    "transcription_tool = Tool(\n",
    "    name=\"TranscribeAllVideos\",\n",
    "    func=transcribe_all_videos,\n",
    "    description=\"Transcribes all videos in the provided list and consolidates results.\"\n",
    ")\n",
    "\n",
    "# Initialize the ReAct agent with transcription tool\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "react_agent = initialize_agent(\n",
    "    tools=[transcription_tool],\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    llm=chat_model,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run agent to perform transcription\n",
    "def perform_transcription_task():\n",
    "    user_query = \"Transcribe all earnings podcasts from the provided YouTube links.\"\n",
    "    transcriptions = transcribe_all_videos()  # Get transcriptions directly\n",
    "    response = react_agent({\"input\": user_query, \"transcriptions\": transcriptions})  # Send entire list to agent once\n",
    "    print(\"Agent's Transcription:\", response[\"output\"])\n",
    "\n",
    "def perform_transcription_task():\n",
    "    user_query = \"Transcribe all earnings podcasts from the provided YouTube links.\"\n",
    "    response = react_agent({\"input\": user_query})\n",
    "    print(\"Agent's Transcription:\", response[\"output\"])\n",
    "\n",
    "# Start the transcription task\n",
    "perform_transcription_task()\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
