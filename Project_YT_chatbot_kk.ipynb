{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytube\n",
      "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytube\n",
      "Successfully installed pytube-15.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openai-whisper\n",
      "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numba (from openai-whisper)\n",
      "  Downloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.1.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.66.1)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from openai-whisper) (8.10.0)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2.0.0->openai-whisper) (3.13.1)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->openai-whisper)\n",
      "  Downloading llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Downloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803319 sha256=4f5fb952714a323c0890b3881ef935f29b58c624e7ff3262091b929658fa99f1\n",
      "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: llvmlite, tiktoken, numba, openai-whisper\n",
      "Successfully installed llvmlite-0.43.0 numba-0.60.0 openai-whisper-20240930 tiktoken-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting langchain\n",
      "  Downloading langchain-0.3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.9.1)\n",
      "Collecting langchain-core<0.4.0,>=0.3.13 (from langchain)\n",
      "  Downloading langchain_core-0.3.13-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.138-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.3)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.13->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.13->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.13->langchain) (4.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (2020.6.20)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/lib/python3/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.13->langchain) (2.4)\n",
      "Downloading langchain-0.3.5-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.13-py3-none-any.whl (408 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.1-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.138-py3-none-any.whl (299 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.0/299.0 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tenacity, pydantic-core, orjson, jsonpatch, h11, annotated-types, pydantic, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.14\n",
      "    Uninstalling pydantic-1.10.14:\n",
      "      Successfully uninstalled pydantic-1.10.14\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepspeed 0.10.3 requires pydantic<2.0.0, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 langchain-0.3.5 langchain-core-0.3.13 langchain-text-splitters-0.3.1 langsmith-0.1.138 orjson-3.10.10 pydantic-2.9.2 pydantic-core-2.23.4 tenacity-9.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openai\n",
      "  Downloading openai-1.53.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.66.1)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Downloading openai-1.53.0-py3-none-any.whl (387 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.1/387.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, jiter, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepspeed 0.10.3 requires pydantic<2.0.0, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed jiter-0.6.1 openai-1.53.0 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytube\n",
    "!pip install openai-whisper\n",
    "!pip install langchain\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, torch\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "def download_audio_from_youtube(url):\n",
    "    yt = YouTube(url)\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    audio_path = audio_stream.download(filename=\"audio.mp3\")\n",
    "    return audio_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "def transcribe_audio_with_whisper(audio_path):\n",
    "    model = whisper.load_model(\"base\")  # or \"small\", \"medium\", \"large\" depending on needs\n",
    "    transcription = model.transcribe(audio_path)\n",
    "    return transcription['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "def youtube_to_text_tool(url):\n",
    "    audio_path = download_audio_from_youtube(url)\n",
    "    transcription = transcribe_audio_with_whisper(audio_path)\n",
    "    return transcription\n",
    "\n",
    "youtube_transcription_tool = Tool(\n",
    "    name=\"YouTubeTranscriptionTool\",\n",
    "    func=youtube_to_text_tool,\n",
    "    description=\"Converts YouTube video audio to text.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Load Environment Variable\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set the environment variable directly in the script\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "#OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "#PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\") # or \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the environment variable with os.environ\n",
    "# print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytube in /usr/local/lib/python3.11/dist-packages (15.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting yt-dlp\n",
      "  Downloading yt_dlp-2024.10.22-py3-none-any.whl.metadata (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.6/171.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting brotli (from yt-dlp)\n",
      "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from yt-dlp) (2020.6.20)\n",
      "Collecting mutagen (from yt-dlp)\n",
      "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pycryptodomex (from yt-dlp)\n",
      "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting requests<3,>=2.32.2 (from yt-dlp)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.11/dist-packages (from yt-dlp) (2.0.7)\n",
      "Collecting websockets>=13.0 (from yt-dlp)\n",
      "  Downloading websockets-13.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.32.2->yt-dlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.32.2->yt-dlp) (3.3)\n",
      "Downloading yt_dlp-2024.10.22-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-13.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m137.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: brotli, websockets, requests, pycryptodomex, mutagen, yt-dlp\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.21.0 requests-2.32.3 websockets-13.1 yt-dlp-2024.10.22\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.11/dist-packages (2.33.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]) (1.26.3)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]) (9.5.0)\n",
      "Collecting imageio-ffmpeg (from imageio[ffmpeg])\n",
      "  Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]) (5.9.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from imageio-ffmpeg->imageio[ffmpeg]) (69.0.3)\n",
      "Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
      "Successfully installed imageio-ffmpeg-0.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\"\"\" !pip install --upgrade pytube\n",
    "!pip install yt-dlp\n",
    "!pip install imageio[ffmpeg] \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/root/.local/bin:/usr/local/cuda/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ[\"PATH\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "libavutil      56. 70.100 / 56. 70.100\n",
      "libavcodec     58.134.100 / 58.134.100\n",
      "libavformat    58. 76.100 / 58. 76.100\n",
      "libavdevice    58. 13.100 / 58. 13.100\n",
      "libavfilter     7.110.100 /  7.110.100\n",
      "libswscale      5.  9.100 /  5.  9.100\n",
      "libswresample   3.  9.100 /  3.  9.100\n",
      "libpostproc    55.  9.100 / 55.  9.100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ffmpeg', '-version'], returncode=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"ffmpeg\", \"-version\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Required\n",
    "#!apt-get update && apt-get install -y ffmpeg\n",
    "\n",
    "#!which ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/ffmpeg\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=rhiYfBctLOI\n",
      "[youtube] rhiYfBctLOI: Downloading webpage\n",
      "[youtube] rhiYfBctLOI: Downloading ios player API JSON\n",
      "[youtube] rhiYfBctLOI: Downloading mweb player API JSON\n",
      "[youtube] rhiYfBctLOI: Downloading player 4e23410d\n",
      "[youtube] rhiYfBctLOI: Downloading m3u8 information\n",
      "[info] rhiYfBctLOI: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_accent_1hr\n",
      "[download] 100% of   51.43MiB in 00:00:01 at 30.88MiB/s    \n",
      "[ExtractAudio] Destination: audio_accent_1hr.mp3\n",
      "Deleting original file audio_accent_1hr (pass -k to keep)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 179MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# to run it on PAPERSPACE!\n",
    "\n",
    "import yt_dlp\n",
    "\n",
    "def download_audio(url):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [\n",
    "            {\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',\n",
    "            }\n",
    "        ],\n",
    "        'outtmpl': 'audio_accent_1hr',\n",
    "        'ffmpeg_location': '/usr/bin/ffmpeg'  # Updated path to ffmpeg on Paperspace\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "    return 'audio_accent_1hr.mp3'\n",
    "\n",
    "# Assuming transcribe_audio_with_whisper is a function that takes an audio path and transcribes it with Whisper\n",
    "video_url = \"https://www.youtube.com/watch?v=rhiYfBctLOI\"\n",
    "audio_path = download_audio(video_url)\n",
    "transcription = transcribe_audio_with_whisper(audio_path)\n",
    "#print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved as HTML.\n"
     ]
    }
   ],
   "source": [
    "# Save transcription to HTML\n",
    "with open('transcription.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(\"<html><body><h1>Transcription</h1><ol>\")\n",
    "    for line in transcription.splitlines():  # Split by lines if it's a long string\n",
    "        file.write(f\"<li>{line}</li>\")\n",
    "    file.write(\"</ol></body></html>\")\n",
    "\n",
    "print(\"Transcription saved as HTML.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization complete. Ready for storage in a vector database.\n"
     ]
    }
   ],
   "source": [
    "# SEGMENT and VECTORIZE the Transcription with Langchain\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the OpenAI embedding model \n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=400)\n",
    "segments = text_splitter.split_text(transcription)\n",
    "\n",
    "# Vectorize each segment and store in a list with segment metadata\n",
    "vectorized_segments = []\n",
    "embeddings = embedding_model.embed_documents(segments)  # Generate embeddings for all segments\n",
    "\n",
    "for i, (segment, vector) in enumerate(zip(segments, embeddings)):\n",
    "    vectorized_segments.append({\n",
    "        \"id\": f\"segment_{i}\",\n",
    "        \"text\": segment,\n",
    "        \"embedding\": vector\n",
    "    })\n",
    "\n",
    "print(\"Vectorization complete. Ready for storage in a vector database.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/lib/python3/dist-packages (from pinecone-client) (2020.6.20)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.0.7)\n",
      "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\n",
      "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install pinecone-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Securely fetch OpenAI and Pinecone API keys                    \n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Enter Pinecone API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\"Enter OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ[\"PINECONE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EMBEDDINGS OpenAI-ada-002\n",
    "\n",
    "import os\n",
    "import time\n",
    "from getpass import getpass\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Securely fetch OpenAI and Pinecone API keys\n",
    "#os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\"Enter OpenAI API key: \")\n",
    "#os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Enter Pinecone API key: \")\n",
    "#os.environ[\"PINECONE_ENV\"] = os.getenv(\"PINECONE_ENV\") or \"us-west1-gcp\"  # Example environment\n",
    "\n",
    "# Initialize OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Initialize Pinecone client with the updated API\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "#spec = ServerlessSpec(cloud=\"aws\", region=\"us-west-1\")  # Specify Pinecone environment\n",
    "index_name = \"transcription-qa-index\"\n",
    "\n",
    "# Check if the index exists; create if it doesn’t\n",
    "if index_name not in [index_info[\"name\"] for index_info in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,  # OpenAI embedding dimension\n",
    "        metric=\"dotproduct\",\n",
    "        spec=spec\n",
    "    )\n",
    "    # Wait for the index to be ready\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "# Connect to the index for upsertion\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce40b7017a50457b92061c6f8b960fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All segments upserted into Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# Upsert embeddings to Pinecone in batches\n",
    "batch_size = 50\n",
    "for i in tqdm(range(0, len(vectorized_segments), batch_size)):\n",
    "    batch = vectorized_segments[i:i + batch_size]\n",
    "    ids = [segment[\"id\"] for segment in batch]\n",
    "    embeds = [segment[\"embedding\"] for segment in batch]\n",
    "    metadatas = [{\"text\": segment[\"text\"]} for segment in batch]\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "\n",
    "print(\"All segments upserted into Pinecone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 103}},\n",
       " 'total_vector_count': 103}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Subject: Summarize the following: of having James with us representing the academic world. And what's going on in the research world? We want to learn from that as the industry. So if that makes sense, and we'll have a dialogue with all of you, we have a few comments from the colleagues here, and then we'll open it up for a good interaction. So with that, we'll go to you and you. Maybe you give us a little update on what's going on in the policy world. OK, thanks, Arnapp. So I was given, I think, five minutes to talk about the state of AI policy globally. So we'll see how to do that. When I thought I might do, I'll try to give a brief overview of sort of where things stand, the trends that are emerging. I'll do a minute or two on the EUA IAC, both because we're here, and for reasons I think it's particularly important for this discussion. And I'll touch on sort of where global governance is, and where it may be going. But just to say, for someone in sort of the policy world, this is a super fascinating time, right? of view on that? Well, so one part of us founding the institute over five years ago was an anticipation of these technological break-throughs of things like open A, has done maybe a little faster than we were planning for it to occur and maybe the faster than they were planning. But it was the idea that these things were going to affect all of society and upend everyone's roles, our jobs, our education, et cetera. And so we need to get ahead of it and bring together a lot of people to talk about it and to try to come up with policy and to do it in more of a neutral framework. So we feel the university, we don't really have skin in the game in terms of, hey, we're going to make billions of dollars on this. Instead, it can be a neutral ground where even competitors can come and talk about these things in a way that they sometimes can't. I think one problem right now is there's so many now, different AI groups and it can be really hard to navigate. Like which one should I be a member of I'm sure you have many insights and we will hear from you shortly. So if that's OK, I would love to jump into our panel and turn to you, Hyatt. You've been at Microsoft and you've been driving a number of initiatives at Microsoft. And you're telling me about your past background in security. So that's phenomenal. Tell us a little bit about the whole, you know, responsibility I program, how to make it operational. What have you done at Microsoft? What can we learn from you? Yes, so I think we started on that journey in 2017 at the time I was in engineering, actually. We started with principles first. But we learned very quickly that principle was not sufficient. Because as a developer, I remember receiving the principle from our policy team and sitting there and saying, OK, what am I supposed to do with this? Like I'm developing a product, right? So then we realized we really need to get very concrete for all function as to what does that mean in your day today. So the first thing is system, we block things. And as you're building your applications, we give you as well testing capabilities. So you, yourself as a customer, can assess and benchmark and see what results you're getting is this meeting and the intention. So we've embedded all this in this studio so that our customers can, they don't have to think differently as they're building it. They can test real time. Fantastic, fantastic. Now this is great. I think we talked, this was the geeky part that I was referring to, hopefully you found it interesting. I think the piece that is becoming very clear, as all of you are speaking and Andrew, you talked about it also in your topic was the collaboration aspect, the collaboration between the business, academy, policy makers and also the global coordination. So maybe I'll go to you, James, because you sit in a very interesting position, you work with number of corporates, you work with government agencies. What is needed? And in your mind, what are the gaps today bring the three D's that James just talked about into actual solution integrated part of our general TBI implementation? Sure. So I will address this question from the technology angle. And by the way, this is a question that many, many clients ask me because we talk all day long about principle, the guidelines they're asking me, the question, okay, instead of being spooked, can you bring a solution? And can you be more practical and tell me how I should be solving this problems because the bias risk will never go away, but they want to mitigate that they want to reduce the risk. So that's why I don't know when I came together. We two of us worked together probably for longer a decade now, right? And we basically said, okay, how do we bring Accenture's expertise into the world? We're very lucky. We've got $3 billion investment from our firm. Of course, the large portion of this is going to be focusing on building this kind of repeatable solutions for cross industry clients. So how do\n"
     ]
    }
   ],
   "source": [
    "# EMBED user QUERY and retrieve RELEVANT SEGMENTS (K-5)\n",
    "# generate a SUMMARY at first # NEED TO relook at this requirement, as it is only appending the segments. or use GPT4>\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Step 1: Embed User Query and Retrieve Relevant Segments\n",
    "def retrieve_relevant_segments(query):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = index.query(vector=query_embedding, top_k=5, include_metadata=True)\n",
    "    segments = [match['metadata']['text'] for match in results['matches']]\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Step 2: Generate Summary from Retrieved Segments\n",
    "def generate_summary(segments):\n",
    "    # Concatenate segments to prepare for summarization\n",
    "    context = \" \".join(segments)\n",
    "    summary_prompt = PromptTemplate(input_variables=[\"context\"], template=\"Summarize the following: {context}\")\n",
    "    summary = summary_prompt.format(context=context)\n",
    "    return summary\n",
    "\n",
    "# Sample user query\n",
    "user_query = \"Tell me about this subject in simple terms.\"\n",
    "segments = retrieve_relevant_segments(user_query)\n",
    "summary = generate_summary(segments)\n",
    "\n",
    "print(\"Summary of Subject:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Subject: Summarize the following: of having James with us representing the academic world. And what's going on in the research world? We want to learn from that as the industry. So if that makes sense, and we'll have a dialogue with all of you, we have a few comments from the colleagues here, and then we'll open it up for a good interaction. So with that, we'll go to you and you. Maybe you give us a little update on what's going on in the policy world. OK, thanks, Arnapp. So I was given, I think, five minutes to talk about the state of AI policy globally. So we'll see how to do that. When I thought I might do, I'll try to give a brief overview of sort of where things stand, the trends that are emerging. I'll do a minute or two on the EUA IAC, both because we're here, and for reasons I think it's particularly important for this discussion. And I'll touch on sort of where global governance is, and where it may be going. But just to say, for someone in sort of the policy world, this is a super fascinating time, right? I'm sure you have many insights and we will hear from you shortly. So if that's OK, I would love to jump into our panel and turn to you, Hyatt. You've been at Microsoft and you've been driving a number of initiatives at Microsoft. And you're telling me about your past background in security. So that's phenomenal. Tell us a little bit about the whole, you know, responsibility I program, how to make it operational. What have you done at Microsoft? What can we learn from you? Yes, so I think we started on that journey in 2017 at the time I was in engineering, actually. We started with principles first. But we learned very quickly that principle was not sufficient. Because as a developer, I remember receiving the principle from our policy team and sitting there and saying, OK, what am I supposed to do with this? Like I'm developing a product, right? So then we realized we really need to get very concrete for all function as to what does that mean in your day today. So the first thing is of view on that? Well, so one part of us founding the institute over five years ago was an anticipation of these technological break-throughs of things like open A, has done maybe a little faster than we were planning for it to occur and maybe the faster than they were planning. But it was the idea that these things were going to affect all of society and upend everyone's roles, our jobs, our education, et cetera. And so we need to get ahead of it and bring together a lot of people to talk about it and to try to come up with policy and to do it in more of a neutral framework. So we feel the university, we don't really have skin in the game in terms of, hey, we're going to make billions of dollars on this. Instead, it can be a neutral ground where even competitors can come and talk about these things in a way that they sometimes can't. I think one problem right now is there's so many now, different AI groups and it can be really hard to navigate. Like which one should I be a member of bring the three D's that James just talked about into actual solution integrated part of our general TBI implementation? Sure. So I will address this question from the technology angle. And by the way, this is a question that many, many clients ask me because we talk all day long about principle, the guidelines they're asking me, the question, okay, instead of being spooked, can you bring a solution? And can you be more practical and tell me how I should be solving this problems because the bias risk will never go away, but they want to mitigate that they want to reduce the risk. So that's why I don't know when I came together. We two of us worked together probably for longer a decade now, right? And we basically said, okay, how do we bring Accenture's expertise into the world? We're very lucky. We've got $3 billion investment from our firm. Of course, the large portion of this is going to be focusing on building this kind of repeatable solutions for cross industry clients. So how do system, we block things. And as you're building your applications, we give you as well testing capabilities. So you, yourself as a customer, can assess and benchmark and see what results you're getting is this meeting and the intention. So we've embedded all this in this studio so that our customers can, they don't have to think differently as they're building it. They can test real time. Fantastic, fantastic. Now this is great. I think we talked, this was the geeky part that I was referring to, hopefully you found it interesting. I think the piece that is becoming very clear, as all of you are speaking and Andrew, you talked about it also in your topic was the collaboration aspect, the collaboration between the business, academy, policy makers and also the global coordination. So maybe I'll go to you, James, because you sit in a very interesting position, you work with number of corporates, you work with government agencies. What is needed? And in your mind, what are the gaps today\n",
      "Agent's Response: The speaker in the provided context suggests that instead of AI replacing humans in jobs, AI should be designed to augment human abilities and intelligence. The focus should be on empowering humans to focus on reasoning, logical thinking, higher-level thinking, and empathy, while AI handles tasks that are more intuition-based or repetitive. So, the perspective shared in the context leans towards using AI to enhance human capabilities rather than completely replacing humans in jobs.\n"
     ]
    }
   ],
   "source": [
    "# AGENT setup for QA interaction:\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize memory to track conversation context\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Initialize the Chat model (e.g., GPT-3.5)\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Wrap Pinecone index with LangChain's Pinecone vector store\n",
    "vector_store = Pinecone(index=index, embedding=embedding_model, text_key=\"text\")  # Specify \"text\" as the key for document content\n",
    "\n",
    "# Create Conversational Retrieval Chain using the wrapped vector store and memory\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat_model,\n",
    "    retriever=vector_store.as_retriever(),  # Use the vector store as the retriever\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Define a function to handle the initial interaction\n",
    "def initial_interaction(user_query):\n",
    "    # Step 1: Retrieve and summarize\n",
    "    segments = retrieve_relevant_segments(user_query)\n",
    "    summary = generate_summary(segments)\n",
    "    \n",
    "    # Step 2: Present the summary to the user and prompt for follow-ups\n",
    "    print(\"Summary of Subject:\", summary)\n",
    "    follow_up_query = input(\"Do you have any further questions on this topic? \")\n",
    "    \n",
    "    # Step 3: Run the follow-up query through the agent\n",
    "    response = qa_chain({\"question\": follow_up_query})\n",
    "    print(\"Agent's Response:\", response['answer'])\n",
    "\n",
    "# Example call to start the interaction\n",
    "user_query = \"Tell me about this subject in simple terms.\"\n",
    "initial_interaction(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ResponseParser' from 'langchain.output_parsers' (/usr/local/lib/python3.11/dist-packages/langchain/output_parsers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResponseParser\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define a simple parser to format responses into paragraphs with bullet points\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ResponseParser' from 'langchain.output_parsers' (/usr/local/lib/python3.11/dist-packages/langchain/output_parsers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a simple parser to format responses into paragraphs with bullet points\n",
    "class BulletPointParser(ResponseParser):\n",
    "    def parse(self, response: str) -> str:\n",
    "        # Transform each sentence into a bullet point\n",
    "        return \"\\n\".join([f\"- {sentence.strip()}\" for sentence in response.split(\". \") if sentence])\n",
    "\n",
    "# Define the response prompt with improved formatting\n",
    "response_template = PromptTemplate(\n",
    "    template=\"Answer the question in bullet points: {question}\"\n",
    ")\n",
    "\n",
    "# Initialize a formatted conversational loop\n",
    "def interactive_chat():\n",
    "    print(\"Welcome to the Q&A Chatbot! Ask me anything.\")\n",
    "    while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() == \"no\":\n",
    "            print(\"Agent: Thank you for using the Q&A chatbot. Goodbye!\")\n",
    "            break\n",
    "        response = qa_chain({\"question\": user_query})\n",
    "        # Parse and format the response\n",
    "        formatted_response = BulletPointParser().parse(response['answer'])\n",
    "        print(\"Agent's Response:\\n\", formatted_response)\n",
    "        print(\"\\nAgent: Do you have more questions?\")\n",
    "\n",
    "# Start the interactive chat\n",
    "interactive_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2.88G/2.88G [00:53<00:00, 57.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " இப்போது புதிய அறிமுகம் அச்சி ஹோட்டல் சாம்பார் 50 கிராம் ரும்பாய் 20 மட்டுமே சென்னை பெருநகர் மற்றும் புரனகரில் சுமார் ஒரு மணி நேரமாக வெளித்து வாங்கும் கணமழை ராயப் பேட்டை, தியாகராய நகர், எழும்பூர், அண்ணாசாலை, அண்ணா நகர் meantime anchor உள்ளிட்ட பகுதிகளில் இடி மின்னவுடன் கொட்டி தீர்க்கும் கணமழை சென்னையில் மேற்கு அண்ணா நகர் பகுதியில் ஒரு மணி நேரத்துயில் 90 cómbing kindergarten அளவிருக்கு கணமழை பதிபு கொளத்தூர் பெரம்பூர் அம்பத்தூர் அமைந்தகரை ஆகிய இடங்கள் அதிக மழை பொழிவு எதிர்க்கொண்டதாக வானிலை மயம் தகவன் பசும்புன் முத்துராமலிங்க தேவரி 117. ஜெயன்தி மற்றும் 62. குருபுஜகி உட்டி முதல்மைச்சர் முக்கு ச்டாலின் மரியாது பசும்புன்னில் உள்ள தேவரி நினிவிடத்தில் மளர்த்தூபி சிலைக்கு மாலை நிவித்த மரியாதை சொல்றினார் பசும்புன்னில் தேவர் வாழ்ந்த இளம் புதுப்பிக்கப்பட்டு 100 ஆண்டு அலங்கाர வளைவு நினைவிடத்தில் அணயா விளக்கு அமைப்பு தேவர் ஜெயன்தியின் போது பசும்புன்னில் கூட நரிசலை தவிர்க தேவர் அரங்கம் திருக்கப்பட்டுள்ளதாக முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி முதலாமைச்சர் முக்கஸ் டாலின் பேட்டி பத்தன்புதாயிரம் கிலோ பட்டாசுகள் பரிவுது தீபாவலி பண்டிகைக்கு பட்டாசு விடிக்க தடி விடிக்கப்பட்டுள நிலையில் சேவித்து வைத்ததால் நடபிடிக்கு வண்ண மின்விளக்குகளால் ஒளிந்த அமரிகாவின் ஞூயார்க் நகரில் உள்ள உலக வர்த்தகமைய கட்டடம் தீபாவலியை உட்டி மின்விளக்குகளால் உலிரூட்டப்பட்ட அமரிகாவின் மிக உயரமான கட்டடம் தீபாவலியை உட்டி அமரிகாவின் ஞூயார்க் நகரில் உள்ள பள்ளிகளுக்கு நவம்பர் ஒண்ராம் தேதி விடுமுரை அறிவிப்ப confidential நியோ யார்க் நகர வரலாட்டில் முதன் முறையாக விடுமுறை அறிவிக்கப்படுவதாக நகரமேயிர் அலுவலகம் தகவுள் நியூஸிலாந்துக்கு எதிரான கடைசி ஒரு நாள் கிரிக்கெட் போட்டிகள் இந்திய மகலிரணி வெற்றி மூன்று போட்டிகள் கொண்ட தொடரை இரண்டுக்கு ஒன்று என்ற கணக்கில் வென்றது இந்தியா திருப்பத்தூர் மாவட்டம் ஆம்பூரருகே அரச தொடர்கப்பட்டி என்றuciónல NFT dzенного சதவிட்டதால் போய்ப்பட்ட<|az|> stata எல்லனும் அலேகம் பேருந்து உட்டுமல்்றும் ஏ öğசி நினைக்கள தீவிறக் கணக்கானிப்பில் ஆதிப commit அரைக்கோணம் ரைல் நிலையத்தில் சிங்ணல் கோலாரு காரணமாக சென்னை மார்க்கத்தில் மின்சார ரைல்கள் தாமதம் ஒரு மணி நேரத்திற்கு மேலாக தாமதமானதால் நிலைய மேலாளர் அளுவலகத்தை முற்றுகையிட்டு வாக்குவாதம் செய்த பயனிகள் அரசு பொக்குவரத்தக்கழகங்களில் גாOohy whilst an அரசு பொக்குவரத்தக்கழகங்களில் காலியாகவுள்ளு 2777 இடங்களை நிறப்பully ஆறப் 움ட்டு நzyst்ல்து Hills பணியாளர்கள் 637 பணியாளர்களை நிற הס்து நடபுடிக்கு ரசிகர் ரெனுகாஸ்வாமி கொலை விழக்கில் கண்ணட நடிகர் pentru இடைகால ஜாமின் ஜாமின்\n"
     ]
    }
   ],
   "source": [
    "# TAMIL TRANSCRIPTION GENERATION \n",
    "\n",
    "\n",
    "\"\"\" from whisper import load_model\n",
    "\n",
    "# Load the Whisper model\n",
    "model1 = load_model(\"large\")  # or use \"small\" or \"base\" for faster processing with lower accuracy\n",
    "\n",
    "\n",
    "# Transcribe the Tamil audio file\n",
    "result = model1.transcribe(\"audio_tamil.mp3\", language=\"ta\")\n",
    "transcription = result[\"text\"]\n",
    "\n",
    "print(transcription) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # to run it on CPU!\\n\\nimport yt_dlp\\n\\ndef download_audio(url):\\n    ydl_opts = {\\n        \\'format\\': \\'bestaudio/best\\',\\n        \\'postprocessors\\': [\\n            {\\n                \\'key\\': \\'FFmpegExtractAudio\\',\\n                \\'preferredcodec\\': \\'mp3\\',\\n                \\'preferredquality\\': \\'192\\',\\n            }\\n        ],\\n        \\'outtmpl\\': \\'audio.mp3\\',\\n        \\'ffmpeg_location\\': \\'C:/Users/KK/AppData/Local/ffmpeg-essentials_build/bin\\'  # Explicit path to ffmpeg\\n    }\\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\\n        ydl.download([url])\\n    return \\'audio.mp3\\'\\n\\n# Assuming transcribe_audio_with_whisper is a function that takes an audio path and transcribes it with Whisper\\nvideo_url = \"https://www.youtube.com/watch?v=RRk9QlCjul0\"\\naudio_path = download_audio(video_url)\\ntranscription = transcribe_audio_with_whisper(audio_path)\\nprint(transcription) '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # to run it on CPU!\n",
    "\n",
    "import yt_dlp\n",
    "\n",
    "def download_audio(url):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [\n",
    "            {\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',\n",
    "            }\n",
    "        ],\n",
    "        'outtmpl': 'audio.mp3',\n",
    "        'ffmpeg_location': 'C:/Users/KK/AppData/Local/ffmpeg-essentials_build/bin'  # Explicit path to ffmpeg\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "    return 'audio.mp3'\n",
    "\n",
    "# Assuming transcribe_audio_with_whisper is a function that takes an audio path and transcribes it with Whisper\n",
    "video_url = \"https://www.youtube.com/watch?v=RRk9QlCjul0\"\n",
    "audio_path = download_audio(video_url)\n",
    "transcription = transcribe_audio_with_whisper(audio_path)\n",
    "print(transcription) \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
