{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytube\n",
      "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytube\n",
      "Successfully installed pytube-15.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openai-whisper\n",
      "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numba (from openai-whisper)\n",
      "  Downloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.1.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.66.1)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from openai-whisper) (8.10.0)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2.0.0->openai-whisper) (3.13.1)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->openai-whisper)\n",
      "  Downloading llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Downloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803319 sha256=cb9752f1cbd553c24f8dbd22601015400a9966716cbf1bfe29db4225bf1dddfa\n",
      "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: llvmlite, tiktoken, numba, openai-whisper\n",
      "Successfully installed llvmlite-0.43.0 numba-0.60.0 openai-whisper-20240930 tiktoken-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting langchain\n",
      "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.9.1)\n",
      "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain)\n",
      "  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.139-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.3)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.15->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (2020.6.20)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/lib/python3/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (2.4)\n",
      "Downloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.139-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tenacity, pydantic-core, orjson, jsonpatch, h11, annotated-types, pydantic, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.14\n",
      "    Uninstalling pydantic-1.10.14:\n",
      "      Successfully uninstalled pydantic-1.10.14\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepspeed 0.10.3 requires pydantic<2.0.0, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 langchain-0.3.7 langchain-core-0.3.15 langchain-text-splitters-0.3.2 langsmith-0.1.139 orjson-3.10.11 pydantic-2.9.2 pydantic-core-2.23.4 tenacity-9.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openai\n",
      "  Downloading openai-1.54.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.66.1)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Downloading openai-1.54.0-py3-none-any.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.3/389.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.3/327.3 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, jiter, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepspeed 0.10.3 requires pydantic<2.0.0, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed jiter-0.7.0 openai-1.54.0 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytube\n",
    "!pip install openai-whisper\n",
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain-community) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.9.1)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.15)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.1.139)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.3)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain-community) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain-community) (2020.6.20)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/lib/python3/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (1.1.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (2.23.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.3.5-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "  Attempting uninstall: marshmallow\n",
      "    Found existing installation: marshmallow 2.21.0\n",
      "    Uninstalling marshmallow-2.21.0:\n",
      "      Successfully uninstalled marshmallow-2.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\n",
      "gradient 2.0.6 requires marshmallow<3.0, but you have marshmallow 3.23.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.5 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 typing-inspect-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pinecone-client\n",
      "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/lib/python3/dist-packages (from pinecone-client) (2020.6.20)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.0.7)\n",
      "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\n",
      "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pytube in /usr/local/lib/python3.11/dist-packages (15.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting yt-dlp\n",
      "  Downloading yt_dlp-2024.11.4-py3-none-any.whl.metadata (172 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yt_dlp-2024.11.4-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: yt-dlp\n",
      "Successfully installed yt-dlp-2024.11.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.11/dist-packages (2.33.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]) (1.26.3)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]) (9.5.0)\n",
      "Collecting imageio-ffmpeg (from imageio[ffmpeg])\n",
      "  Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]) (5.9.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from imageio-ffmpeg->imageio[ffmpeg]) (69.0.3)\n",
      "Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
      "Successfully installed imageio-ffmpeg-0.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-community\n",
    "!pip install pinecone-client\n",
    "!pip install --upgrade pytube\n",
    "!pip install yt-dlp\n",
    "!pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, torch\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Load Environment Variable\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set the environment variable directly in the script\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "#OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "#PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\") # or \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the environment variable with os.environ\n",
    "#print(os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 1/4: https://www.youtube.com/watch?v=-RIfno4Sp0g\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=-RIfno4Sp0g\n",
      "[youtube] -RIfno4Sp0g: Downloading webpage\n",
      "[youtube] -RIfno4Sp0g: Downloading ios player API JSON\n",
      "[youtube] -RIfno4Sp0g: Downloading mweb player API JSON\n",
      "[youtube] -RIfno4Sp0g: Downloading m3u8 information\n",
      "[info] -RIfno4Sp0g: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_0\n",
      "[download] 100% of   48.70MiB in 00:00:01 at 37.92MiB/s    \n",
      "[ExtractAudio] Destination: audio_0.mp3\n",
      "Deleting original file audio_0 (pass -k to keep)\n",
      "Completed transcription for video 1\n",
      "Processing video 2/4: https://www.youtube.com/watch?v=3SCOVB1KDFc\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3SCOVB1KDFc\n",
      "[youtube] 3SCOVB1KDFc: Downloading webpage\n",
      "[youtube] 3SCOVB1KDFc: Downloading ios player API JSON\n",
      "[youtube] 3SCOVB1KDFc: Downloading mweb player API JSON\n",
      "[youtube] 3SCOVB1KDFc: Downloading m3u8 information\n",
      "[info] 3SCOVB1KDFc: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_1\n",
      "[download] 100% of   54.16MiB in 00:00:04 at 11.46MiB/s  \n",
      "[ExtractAudio] Destination: audio_1.mp3\n",
      "Deleting original file audio_1 (pass -k to keep)\n",
      "Completed transcription for video 2\n",
      "Processing video 3/4: https://www.youtube.com/watch?v=qMqP2IwhSnA\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=qMqP2IwhSnA\n",
      "[youtube] qMqP2IwhSnA: Downloading webpage\n",
      "[youtube] qMqP2IwhSnA: Downloading ios player API JSON\n",
      "[youtube] qMqP2IwhSnA: Downloading mweb player API JSON\n",
      "[youtube] qMqP2IwhSnA: Downloading m3u8 information\n",
      "[info] qMqP2IwhSnA: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_2\n",
      "[download] 100% of   60.30MiB in 00:00:11 at 5.40MiB/s   \n",
      "[ExtractAudio] Destination: audio_2.mp3\n",
      "Deleting original file audio_2 (pass -k to keep)\n",
      "Completed transcription for video 3\n",
      "Processing video 4/4: https://www.youtube.com/watch?v=5u9jOA3LCdU\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=5u9jOA3LCdU\n",
      "[youtube] 5u9jOA3LCdU: Downloading webpage\n",
      "[youtube] 5u9jOA3LCdU: Downloading ios player API JSON\n",
      "[youtube] 5u9jOA3LCdU: Downloading mweb player API JSON\n",
      "[youtube] 5u9jOA3LCdU: Downloading m3u8 information\n",
      "[info] 5u9jOA3LCdU: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_3\n",
      "[download] 100% of   65.24MiB in 00:00:09 at 7.20MiB/s     \n",
      "[ExtractAudio] Destination: audio_3.mp3\n",
      "Deleting original file audio_3 (pass -k to keep)\n",
      "Completed transcription for video 4\n",
      "Consolidation Complete. Transcriptions are ready for vectorization and further processing.\n"
     ]
    }
   ],
   "source": [
    "# Sunday 3-Nov\n",
    "# STEP 1: Transcription and Consolidation Tasks (without agent to improve the process and reduce the time)\n",
    "\n",
    "import os\n",
    "import yt_dlp\n",
    "from whisper import load_model\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Default configurations\n",
    "VIDEO_LINKS_FILE = \"video_links.txt\"\n",
    "AUDIO_OUTPUT_TEMPLATE = \"audio_{}\"\n",
    "WHISPER_MODEL = \"base\"\n",
    "TRANSCRIPT_DIR = \"transcriptions\"\n",
    "\n",
    "# Ensure directory exists for transcriptions\n",
    "os.makedirs(TRANSCRIPT_DIR, exist_ok=True)\n",
    "\n",
    "# Load YouTube links from the file\n",
    "def load_video_links(filename=VIDEO_LINKS_FILE):\n",
    "    with open(filename, 'r') as file:\n",
    "        return [link.strip() for link in file.readlines() if link.strip()]\n",
    "\n",
    "# Download audio from YouTube video\n",
    "def download_audio(url, output_name):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}],\n",
    "        'outtmpl': f\"{output_name}\",\n",
    "        'ffmpeg_location': '/usr/bin/ffmpeg'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "    return f\"{output_name}.mp3\"\n",
    "\n",
    "# Transcribe audio using Whisper\n",
    "def transcribe_audio_with_whisper(audio_path, model_name=WHISPER_MODEL):\n",
    "    model = load_model(model_name)\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Process each video, saving individual transcriptions\n",
    "def transcribe_all_videos():\n",
    "    video_links = load_video_links(VIDEO_LINKS_FILE)\n",
    "    all_transcriptions = []\n",
    "\n",
    "    for i, video_url in enumerate(video_links):\n",
    "        print(f\"Processing video {i+1}/{len(video_links)}: {video_url}\")\n",
    "        audio_path = download_audio(video_url, AUDIO_OUTPUT_TEMPLATE.format(i))\n",
    "        \n",
    "        # Transcribe in chunks to prevent reprocessing, saving each transcription file\n",
    "        transcription = transcribe_audio_with_whisper(audio_path)\n",
    "        transcript_file = os.path.join(TRANSCRIPT_DIR, f\"transcription_{i+1}.txt\")\n",
    "        \n",
    "        with open(transcript_file, 'w') as f:\n",
    "            f.write(transcription)\n",
    "        \n",
    "        all_transcriptions.append({\"title\": f\"Video {i+1}\", \"transcription\": transcription})\n",
    "        print(f\"Completed transcription for video {i+1}\")\n",
    "\n",
    "    return all_transcriptions\n",
    "\n",
    "# Consolidate transcriptions for LLM analysis\n",
    "def consolidate_transcriptions():\n",
    "    consolidated_text = \"\"\n",
    "    for file in sorted(os.listdir(TRANSCRIPT_DIR)):\n",
    "        file_path = os.path.join(TRANSCRIPT_DIR, file)\n",
    "        \n",
    "        # Check if the file_path is a file (not a directory)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                consolidated_text += f.read() + \"\\n\"\n",
    "    return consolidated_text\n",
    "\n",
    "\n",
    "# Execute process\n",
    "if __name__ == \"__main__\":\n",
    "    transcribe_all_videos()\n",
    "    consolidated_text = consolidate_transcriptions()\n",
    "    print(\"Consolidation Complete. Transcriptions are ready for vectorization and further processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization complete. Ready for storage in a vector database.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Chunk the consolidated text and Vectorization!\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Split consolidated text into manageable chunks\n",
    "def split_text_for_vectorization(consolidated_text, max_tokens=1500):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=max_tokens, chunk_overlap=200)\n",
    "    return text_splitter.split_text(consolidated_text)\n",
    "\n",
    "# Function to perform vectorization on each chunk\n",
    "def perform_vectorization_task(consolidated_text):\n",
    "    chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    memory = ConversationBufferMemory()\n",
    "    \n",
    "    # Initialize the embedding model and the agent with vectorization tool\n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    chunks = split_text_for_vectorization(consolidated_text)\n",
    "    \n",
    "    vectorized_segments = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # print(f\"Vectorizing chunk {i+1}/{len(chunks)}\")\n",
    "        \n",
    "        # Generate embeddings for this chunk\n",
    "        embeddings = embedding_model.embed_documents([chunk])\n",
    "        \n",
    "        # Store each embedding\n",
    "        vectorized_segments.append({\n",
    "            \"id\": f\"segment_{i}\",\n",
    "            \"text\": chunk,\n",
    "            \"embedding\": embeddings[0]  # embedding_model.embed_documents returns a list\n",
    "        })\n",
    "\n",
    "    print(\"Vectorization complete. Ready for storage in a vector database.\")\n",
    "    return vectorized_segments\n",
    "\n",
    "# Run vectorization task after consolidation\n",
    "if __name__ == \"__main__\":    \n",
    "    vectorized_segments = perform_vectorization_task(consolidated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 - define Pinecone as Vector DB & Define Index & Embeddings.\n",
    "\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Securely fetch OpenAI and Pinecone API keys                    \n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Enter Pinecone API key: \")\n",
    "#os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\"Enter OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ[\"PINECONE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index statistics: {'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Create INDEX using EMBEDDINGS OpenAI-ada-002 - creating the Index for 'Accenture Earnings'\n",
    "\n",
    "import os\n",
    "import time\n",
    "from getpass import getpass\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Initialize Pinecone client with the updated API\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "#spec = ServerlessSpec(cloud=\"aws\", region=\"us-west-1\")  # Specify Pinecone environment\n",
    "index_name = \"transcription-accnt-earn-index-2\"             # Create New Index\n",
    "\n",
    "# Check if the index exists; create if it doesn’t\n",
    "if index_name not in [index_info[\"name\"] for index_info in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,  # OpenAI embedding dimension\n",
    "        metric=\"dotproduct\",\n",
    "        spec=spec\n",
    "    )\n",
    "    # Wait for the index to be ready\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        print(\"Waiting for index to be ready...\")\n",
    "        time.sleep(1)\n",
    "\n",
    "# Connect to the index for upsertion\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "print(\"Index statistics:\", index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc0d5cef9274e9cad6183eb2fd5212b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All segments upserted into Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 - UPSERT embeddings to Pinecone in batches\n",
    "\n",
    "batch_size = 50\n",
    "for i in tqdm(range(0, len(vectorized_segments), batch_size)):\n",
    "    batch = vectorized_segments[i:i + batch_size]\n",
    "    ids = [segment[\"id\"] for segment in batch]\n",
    "    embeds = [segment[\"embedding\"] for segment in batch]\n",
    "    metadatas = [{\"text\": segment[\"text\"]} for segment in batch]\n",
    "\n",
    "    # Upsert each batch to Pinecone\n",
    "    try:\n",
    "        index.upsert(vectors=list(zip(ids, embeds, metadatas)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting batch {i // batch_size + 1}: {e}\")\n",
    "\n",
    "print(\"All segments upserted into Pinecone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index statistics: {'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 176}},\n",
      " 'total_vector_count': 176}\n"
     ]
    }
   ],
   "source": [
    "print(\"Index statistics:\", index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the revenue of Accenture last quarter?\n",
      "Routed to: vectorstore\n",
      "\n",
      "Query: What is the EPS for last quarter?\n",
      "Routed to: vectorstore\n",
      "\n",
      "Query: What is the total no of employee count in Accenture company?\n",
      "Routed to: vectorstore\n",
      "\n",
      "Query: Who are the competitors of Accenture?\n",
      "Routed to: websearch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define ROUTER for query to be passed to VectorDB or Websearch (google search) -  Example to demonstrate\n",
    "\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize LLM in JSON mode (replace 'gpt-3.5-turbo' with any model configured for JSON output)\n",
    "llm_json_mode = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Define the routing instructions\n",
    "router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "\n",
    "The vectorstore contains documents related to Accenture company Earnings results, Revenue, Earnings for last four quarters, sales, cloud projects, Genai projects and employee details.\n",
    "\n",
    "Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "\n",
    "Return JSON with a single key, \"datasource\", that is either 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "\n",
    "# Function to route based on the instructions\n",
    "def route_query(query):\n",
    "    routing_response = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=router_instructions), HumanMessage(content=query)]\n",
    "    )\n",
    "    route_decision = json.loads(routing_response.content)\n",
    "    return route_decision[\"datasource\"]\n",
    "\n",
    "# Example queries to test the router\n",
    "test_queries = [\n",
    "    \"What is the revenue of Accenture last quarter?\",\n",
    "    \"What is the EPS for last quarter?\",\n",
    "    \"What is the total no of employee count in Accenture company?\",\n",
    "    \"Who are the competitors of Accenture?\",\n",
    "]\n",
    "\n",
    "# Execute the router on test queries\n",
    "for query in test_queries:\n",
    "    datasource = route_query(query)\n",
    "    print(f\"Query: {query}\\nRouted to: {datasource}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-search-results\n",
    "\n",
    "## !pip install tavily-python (not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "#os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\") or getpass.getpass(\"Enter Tavily API key: \")\n",
    "\n",
    "# Fetch the SerpAPI key securely\n",
    "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\") or getpass.getpass(\"Enter SerpAPI key: \")\n",
    "\n",
    "# Example usage in SerpAPI tool setup\n",
    "serp_api_key = os.environ[\"SERPAPI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ[\"SERPAPI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38/3440100285.py:20: LangChainDeprecationWarning: The class `Pinecone` was deprecated in LangChain 0.0.18 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-pinecone package and should be used instead. To use it run `pip install -U :class:`~langchain-pinecone` and import as `from :class:`~langchain_pinecone import Pinecone``.\n",
      "  vector_store = Pinecone(index=index, embedding=embedding_model, text_key=\"text\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing to Vectorstore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38/3440100285.py:42: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The revenue of Accenture last quarter was $16.4 billion.\n",
      "Routing to Vectorstore...\n",
      "Response: The EPS of Accenture last quarter was $2.79.\n",
      "Routing to Vectorstore...\n",
      "Response: The change in Revenue between the Fourth and Third quarters is an increase of 3% in US dollars and 5% in local currency.\n",
      "Routing to Web Search...\n",
      "Response: The main competitors of Accenture in the consulting industry include IBM, Deloitte, Capgemini, PwC, Tata Consultancy Services, McKinsey & Co., Boston Consulting Group, Inc., and Bain & Co.\n"
     ]
    }
   ],
   "source": [
    "# DEFINE ROUTER, RETRIEVE from VectorstoreDB and Retrieve from Google search functions!\n",
    "\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "import os\n",
    "\n",
    "# Initialize the embedding model and vector store\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "#from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "# Assuming Pinecone index is already set up and embeddings are upserted\n",
    "vector_store = Pinecone(index=index, embedding=embedding_model, text_key=\"text\")\n",
    "\n",
    "# Initialize the router instructions\n",
    "router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "\n",
    "The vectorstore contains documents related to Accenture company Quarterly results, Revenue, Earnings for the last four quarters, sales, digital projects, cloud projects and employee details.\n",
    "\n",
    "Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "\n",
    "Return JSON with a single key, \"datasource\", that is either 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "\n",
    "# Function to route query based on instructions\n",
    "def route_query(query):\n",
    "    routing_response = chat_model.invoke(\n",
    "        [SystemMessage(content=router_instructions), HumanMessage(content=query)]\n",
    "    )\n",
    "    route_decision = json.loads(routing_response.content)\n",
    "    return route_decision[\"datasource\"]\n",
    "\n",
    "# Function to retrieve from vector database\n",
    "def retrieve_from_vectorstore(query):\n",
    "    retriever = vector_store.as_retriever()\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "    document_text = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "    # Summarize the retrieved documents using LLM \n",
    "    prompt = (\n",
    "        \"Based on the following documents, provide a concise answer to the question:\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        \"Documents:\\n\"\n",
    "        f\"{document_text}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    answer_response = chat_model.invoke([HumanMessage(content=prompt)])\n",
    "    return answer_response.content\n",
    "    # return \"\\n\\n\".join([doc.page_content for doc in results]) # can be removed if this modules functions right!\n",
    "\n",
    "# Function to retrieve from Google search using SerpaAPI\n",
    "def web_search(query):\n",
    "    search = GoogleSearch({\n",
    "        \"q\": query,\n",
    "        \"api_key\": serp_api_key, \n",
    "        \"num\": 3  # Limit to top 3 results for conciseness\n",
    "    })\n",
    "    results = search.get_dict()\n",
    "\n",
    "    # Extract relevant text from search results\n",
    "    combined_text = \"\\n\".join(\n",
    "        result.get(\"snippet\", \"No snippet available\")\n",
    "        for result in results.get(\"organic_results\", [])\n",
    "    )\n",
    "    \n",
    "    # Summarize results using the LLM\n",
    "    prompt = (\n",
    "        f\"Summarize the following search results to answer the question:\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Results:\\n{combined_text}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    summary_response = chat_model.invoke([HumanMessage(content=prompt)])\n",
    "    return summary_response.content\n",
    "\n",
    "\n",
    "# Function to perform web search (e.g., using Tavily - NOT USING IT)\n",
    "\"\"\" def web_search(query):\n",
    "    # Fetch results using Tavily\n",
    "    results = web_search_tool.run(query)\n",
    "    print(\"Results:\", results)  # Inspect the structure\n",
    "    combined_text = \"\\n\".join(result.get('text', 'No text available') for result in results if 'text' in result)\n",
    "\n",
    "    # Summarize results if multiple documents are returned\n",
    "    prompt = (\n",
    "        f\"Summarize the following web search results to answer the question:\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Results:\\n{combined_text}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    summary_response = chat_model.invoke([HumanMessage(content=prompt)])\n",
    "    return summary_response.content \"\"\"\n",
    "    \n",
    "\n",
    "# Main function to handle the query routing and retrieval\n",
    "def handle_query(query):\n",
    "    datasource = route_query(query)\n",
    "    \n",
    "    if datasource == \"vectorstore\":\n",
    "        print(\"Routing to Vectorstore...\")\n",
    "        return retrieve_from_vectorstore(query)\n",
    "    elif datasource == \"websearch\":\n",
    "        print(\"Routing to Web Search...\")\n",
    "        return web_search(query)\n",
    "    else:\n",
    "        return \"Unknown data source. Please check the router configuration.\"\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the revenue of Accenture last quarter??\"\n",
    "response = handle_query(query)\n",
    "print(\"Response:\", response)\n",
    "\n",
    "query = \"What is the EPS of Accenture last quarter?\"\n",
    "response = handle_query(query)\n",
    "print(\"Response:\", response)\n",
    "\n",
    "query = \"What is the change in Revenue between Fourth and Third quarters?\"\n",
    "response = handle_query(query)\n",
    "print(\"Response:\", response)\n",
    "\n",
    "query = \"Who is the main competitor of Accenture?\"\n",
    "response = handle_query(query)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Control the recursive depth to avoid Web_search error\\n\\nimport sys\\nsys.setrecursionlimit(2000)  # Limit to a reasonable depth '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Control the recursive depth to avoid Web_search error\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(2000)  # Limit to a reasonable depth \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information about Accenture's revenue for the fourth quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture revenue Q4 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported revenues of $16.4 billion for Q4 2024, which represents a 3% increase in US dollars and a 5% increase in local currency compared to the previous year.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the revenue of Accenture for the fourth quarter of 2024.\n",
      "Final Answer: Accenture reported revenues of $16.4 billion for Q4 2024.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: Accenture reported revenues of $16.4 billion for Q4 2024.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should first try to use VectorStore Retrieval to find information about Accenture's projects in 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture projects 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture is projecting strong growth in FY24 and FY25, with a focus on large-scale transformations and investment in Gen AI technology. The company aims to continue delivering shareholder value, earnings growth, and margin expansion while investing in talent and innovation. Strong demand is seen in cybersecurity, marketing, customer experience, digital manufacturing, and engineering services. Accenture's global leadership is demonstrated through its diverse and inclusive workplace, recognition in global diversity indexes, and strategic partnerships with leading companies.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe information provided did not specifically mention the size of the Genai project deals for Accenture in 2024. I should now try to use Google Search to find more details.\n",
      "Action: Google Search\n",
      "Action Input: Accenture Genai project deals size 2024\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAccenture's Genai project generated $900 million in business during Q3 2024, totaling $2 billion for the fiscal year. The deals are increasing in size, with some reaching $10 million or more.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the information gathered, the size of Genai project deals for Accenture in 2024 is $2 billion.\n",
      "Final Answer: The size of Genai project deals for Accenture in 2024 is $2 billion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: The size of Genai project deals for Accenture in 2024 is $2 billion.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Google Search to find this information.\n",
      "Action: Google Search\n",
      "Action Input: \"competitors of Accenture\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mSome competitors of Accenture include IBM, PwC, Tata Consultancy Services (TCS), Capgemini, Deloitte, Cognizant, SAP, and DXC Technology. Other competitors and alternatives include IBM Consulting, KPMG, Ernst & Young (EY), and Deloitte Consulting. The top alternatives to Accenture are Deloitte Consulting, Capgemini Services, and Cognizant.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: Some competitors of Accenture include IBM, PwC, Tata Consultancy Services (TCS), Capgemini, Deloitte, Cognizant, SAP, and DXC Technology. Other competitors and alternatives include IBM Consulting, KPMG, Ernst & Young (EY), and Deloitte Consulting. The top alternatives to Accenture are Deloitte Consulting, Capgemini Services, and Cognizant.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: Some competitors of Accenture include IBM, PwC, Tata Consultancy Services (TCS), Capgemini, Deloitte, Cognizant, SAP, and DXC Technology. Other competitors and alternatives include IBM Consulting, KPMG, Ernst & Young (EY), and Deloitte Consulting. The top alternatives to Accenture are Deloitte Consulting, Capgemini Services, and Cognizant.\n"
     ]
    }
   ],
   "source": [
    "# DEFINE ALL THE TOOLS for ReACT agent! No CHATBOT YET!\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "from langchain.agents import initialize_agent\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "# Define the Web search tool using SerpAPI\n",
    "web_search_tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    func=web_search,\n",
    "    description=\"Retrieve relevant information using google web search for general or current event questions.\"\n",
    ")\n",
    "\n",
    "# Define tool for vector store retrieval\n",
    "vector_store_tool = Tool(\n",
    "    name=\"VectorStore Retrieval\",\n",
    "    func=retrieve_from_vectorstore,\n",
    "    description=\"Retrieve relevant documents from the vector store for questions about Accenture's earnings, revenue, sales, projects, and employee details.\",\n",
    ")\n",
    "\n",
    "\"\"\" # Define tool for web search (Tavily - not Using it!)\n",
    "web_search_tool = Tool(\n",
    "    name=\"Web Search Retrieval\",\n",
    "    func=web_search,\n",
    "    description=\"Retrieve information using Tavily web search for general or current event questions not covered in the vector store, particularly about competitors or industry updates.\",\n",
    ")\n",
    " \"\"\"\n",
    "# Initialize the ReAct agent with both tools\n",
    "react_agent = initialize_agent(\n",
    "    tools=[vector_store_tool, web_search_tool],\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    llm=chat_model,\n",
    "    verbose=True,\n",
    "    max_iterations=3  # Restrict maximum depth to avoid infinite loops\n",
    ")\n",
    "\n",
    "# Main function to handle the query using the ReAct agent\n",
    "def handle_query_with_agent(query):\n",
    "    # Call the ReAct agent with the query; it will decide which tool to use\n",
    "    response = react_agent({\"input\": query})\n",
    "    return response[\"output\"]\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the revenue of Accenture fourth quarter of 2024?\"\n",
    "response = handle_query_with_agent(query)\n",
    "print(\"Response:\", response)\n",
    "\n",
    "query = \"What is the size of Genai project deals for Accenture in year 2024??\"\n",
    "response = handle_query_with_agent(query)\n",
    "print(\"Response:\", response)\n",
    "\n",
    "query = \"Who are the competitors of Accenture?\"\n",
    "response = handle_query_with_agent(query)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Accenture Q&A Chatbot! Ask any question about Accenture.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find specific financial information about Accenture's revenue for the fourth quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture revenue Q4 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported revenues of $16.4 billion for Q4 2024.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the revenue for Accenture in the fourth quarter of 2024.\n",
      "Final Answer: Accenture reported revenues of $16.4 billion for Q4 2024.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Error parsing response JSON, displaying raw answer instead.\n",
      "Agent (using VectorDB): Accenture reported revenues of $16.4 billion for Q4 2024.\n",
      "\n",
      "Agent: Do you have more questions?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to search for information on Genai project deals in 2024 for Accenture.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Genai project deals size 2024 Accenture\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture's Gen AI project is expected to have significant growth potential in FY24 and beyond, with bookings reaching $81 billion and a focus on large-scale transformations for clients. The company's investment in training and developing its workforce in data and AI skills is key to driving innovation and success in the market. Additionally, Accenture's leadership position in the industry and its ability to provide end-to-end solutions for clients positions it well to lead the shift from experimentation to scaling Gen AI technology for value.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe information provided suggests that Accenture's Gen AI project deals in 2024 are expected to be significant, with bookings reaching $81 billion.\n",
      "Final Answer: The size of Gen AI project deals for Accenture in 2024 is expected to reach $81 billion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Error parsing response JSON, displaying raw answer instead.\n",
      "Agent (using Google Search): The size of Gen AI project deals for Accenture in 2024 is expected to reach $81 billion.\n",
      "\n",
      "Agent: Do you have more questions?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Google Search to find a list of Accenture's competitors.\n",
      "Action: Google Search\n",
      "Action Input: \"Accenture competitors list\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mSome of the top competitors and alternatives to Accenture include IBM, PwC, Tata Consultancy Services (TCS), Capgemini, Deloitte, Cognizant, SAP, and DXC Technology. Other competitors mentioned are KPMG, Ernst & Young (EY), and Deloitte Consulting.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the competitors of Accenture.\n",
      "Final Answer: IBM, PwC, Tata Consultancy Services (TCS), Capgemini, Deloitte, Cognizant, SAP, DXC Technology, KPMG, Ernst & Young (EY), and Deloitte Consulting.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Error parsing response JSON, displaying raw answer instead.\n",
      "Agent (using Google Search): IBM, PwC, Tata Consultancy Services (TCS), Capgemini, Deloitte, Cognizant, SAP, DXC Technology, KPMG, Ernst & Young (EY), and Deloitte Consulting.\n",
      "\n",
      "Agent: Do you have more questions?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information on Accenture's earnings for the third quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture third quarter 2024 earnings\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported strong results for the third quarter of fiscal year 2024, with significant growth in bookings and revenues. The company's focus on large-scale transformations and deep client relationships has positioned them for continued growth in the future. They have invested in transformative technologies like Gen AI and are well positioned to capitalize on market opportunities in the coming year.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to find the specific EPS number for the third quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture EPS third quarter 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBased on the provided documents, Accenture's EPS for the third quarter of 2024 has not been explicitly stated. However, the company has highlighted its strong growth positioning for FY25, with successful performance in FY24, delivering on shareholder value proposition, growth, earnings growth, and margin expansion. The company continues to prioritize large-scale transformations for clients, with significant bookings and revenue growth.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to search for the specific EPS number for Accenture's third quarter of 2024.\n",
      "Action: Google Search\n",
      "Action Input: Accenture EPS third quarter 2024\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAccenture reported a GAAP EPS of $3.04 for the third quarter of fiscal 2024, which is a 3% decrease from the same quarter in 2023. The adjusted EPS was $3.13, reflecting a 2% decrease. The financial results were for the quarter ended May 31, 2024.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Error parsing response JSON, displaying raw answer instead.\n",
      "Agent (using VectorDB): Agent stopped due to iteration limit or time limit.\n",
      "\n",
      "Agent: Do you have more questions?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the VectorStore Retrieval tool to find information on Accenture's earnings for the fourth quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture fourth quarter 2024 earnings\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported strong growth in FY24, with full fiscal year bookings of $81 billion, representing 14 percent growth in local currency. Revenues for the year were $65 billion, representing 2 percent growth in local currency. The company successfully positioned itself for strong growth in FY25, with a focus on large-scale transformations and deep client relationships. The financial details, including income statement and balance sheet, were shared in the earnings conference calls for the second and third quarters of fiscal 2024. The company's business outlook for the fourth quarter and full fiscal year 2024 was also provided during these calls.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to find specific information on the EPS for the fourth quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture fourth quarter 2024 EPS\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe fourth quarter 2024 EPS for Accenture was $2.79, showing a growth of 2% compared to the adjusted EPS of $2.71 in the fourth quarter of the previous year.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The EPS for the fourth quarter of 2024 for Accenture was $2.79\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Error parsing response JSON, displaying raw answer instead.\n",
      "Agent (using Google Search): The EPS for the fourth quarter of 2024 for Accenture was $2.79\n",
      "\n",
      "Agent: Do you have more questions?\n",
      "Agent: Thank you for using the Q&A chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# INTERACTIVE CHAT FUNCTION using ReACT Agent - REFINED 3! TUESDAY\n",
    "\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize chat model and memory\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Interactive chat function with datasource-based source tagging\n",
    "def chatbot_interaction():\n",
    "    print(\"Welcome to the Accenture Q&A Chatbot! Ask any question about Accenture.\")\n",
    "    while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"no\"]:\n",
    "            print(\"Agent: Thank you for using the Q&A chatbot. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Determine the datasource\n",
    "        datasource = route_query(user_query)\n",
    "        \n",
    "        # Tag source based on datasource\n",
    "        source_label = \"VectorDB\" if datasource == \"vectorstore\" else \"Google Search\" if datasource == \"websearch\" else \"Unknown Source\"\n",
    "        \n",
    "        try:\n",
    "            # Get response from ReAct agent\n",
    "            response = react_agent({\"input\": user_query})\n",
    "            \n",
    "            # Parse and display response with source tag\n",
    "            if \"output\" in response and isinstance(response[\"output\"], str):\n",
    "                try:\n",
    "                    parsed_response = json.loads(response[\"output\"])\n",
    "                    answer = parsed_response.get(\"answer\", response[\"output\"])\n",
    "                    print(f\"\\nAgent (using {source_label}):\\nQuestion: {user_query}\\nAnswer: {answer}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Agent: Error parsing response JSON, displaying raw answer instead.\")\n",
    "                    print(f\"Agent (using {source_label}): {response['output']}\")\n",
    "            else:\n",
    "                print(\"Agent: Unable to process the request in the expected format. Please try again.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}\")\n",
    "            print(\"Agent: Unable to process the request. Please try again.\")\n",
    "        \n",
    "        print(\"\\nAgent: Do you have more questions?\")\n",
    "\n",
    "# Start the chatbot interaction\n",
    "chatbot_interaction()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Accenture Q&A Chatbot! Ask any question about Accenture.\n",
      "Agent: Thank you for using the Q&A chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# INTERACTIVE CHAT FUNCTION using ReACT Agent - REFINED 2!\n",
    "\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize chat model and memory if not already set up\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Define chatbot interaction with enhanced error handling and source tracking\n",
    "def chatbot_interaction():\n",
    "    print(\"Welcome to the Accenture Q&A Chatbot! Ask any question about Accenture.\")\n",
    "    while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"no\"]:\n",
    "            print(\"Agent: Thank you for using the Q&A chatbot. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Attempt to call the ReAct agent and capture response\n",
    "            response = react_agent({\"input\": user_query})\n",
    "            \n",
    "            # Determine the source tool based on response content\n",
    "            source_tool = \"VectorDB\" if \"VectorStore Retrieval\" in response.get(\"log\", \"\") else \"Websearch\"\n",
    "            \n",
    "            # Parse the response JSON if possible\n",
    "            if \"output\" in response and isinstance(response[\"output\"], str):\n",
    "                try:\n",
    "                    parsed_response = json.loads(response[\"output\"])\n",
    "                    # Display question, answer, and source tool\n",
    "                    if isinstance(parsed_response, dict) and \"answer\" in parsed_response:\n",
    "                        print(f\"\\nAgent (using {source_tool}):\\nQuestion: {user_query}\\nAnswer: {parsed_response['answer']}\")\n",
    "                    else:\n",
    "                        print(f\"\\nAgent (using {source_tool}):\\nQuestion: {user_query}\\nAnswer: {response['output']}\")  # Fallback to raw output\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Agent: Error parsing response JSON, displaying raw answer instead.\")\n",
    "                    print(f\"Agent (using {source_tool}): {response['output']}\")\n",
    "            else:\n",
    "                # Fallback for unparsed or unexpected response format\n",
    "                print(\"Agent: Unable to process the request in the expected format. Please try again.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Error encountered, displaying default error message.\")\n",
    "            print(\"Agent: Unable to process the request. Please try again.\")\n",
    "        \n",
    "        print(\"\\nAgent: Do you have more questions?\")\n",
    "\n",
    "# Start the chatbot interaction\n",
    "chatbot_interaction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio placeholder\n",
    "\n",
    "#!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://a1bf66759a0c9388b6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a1bf66759a0c9388b6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information about Accenture's revenue for the fourth quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture revenue Q4 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture's revenue for Q4 2024 was $16.4 billion, which was above the midpoint of their guided range and represented a 3% increase in US dollars and a 5% increase in local currency.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Accenture's revenue for the fourth quarter of 2024 was $16.4 billion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the VectorStore Retrieval tool to find information about Accenture's earnings for the fourth quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture fourth quarter 2024 earnings\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture reported strong growth in FY24 with revenues of $65 billion, representing 2 percent growth in local currency. The company's full fiscal year bookings reached $81 billion, showing a 14 percent growth in local currency. They also increased their diamond clients to 310 and had 125 clients with quarterly bookings greater than $100 million in the fourth quarter. Overall, Accenture is well-positioned for strong growth in FY25.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe information provided does not specifically mention the EPS for the fourth quarter of 2024. I should try to narrow down my search to focus specifically on EPS.\n",
      "Action: Google Search\n",
      "Action Input: Accenture EPS fourth quarter 2024\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAccenture reported an adjusted EPS of $2.79 for the fourth quarter of fiscal 2024, which represents a 3% increase over the previous year's EPS of $2.71.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The EPS of Accenture for the fourth quarter of 2024 was $2.79.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mWe can start by using a Google search to find a general description of Accenture's business.\n",
      "Action: Google Search\n",
      "Action Input: \"What does Accenture do?\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAccenture provides a range of services including strategy & consulting, interactive, technology, and operations to help clients solve their toughest challenges and build their digital core.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThis gives a general idea of Accenture's business focus.\n",
      "Final Answer: Accenture provides a range of services including strategy & consulting, interactive, technology, and operations to help clients solve their toughest challenges and build their digital core.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information on specific Genai deals in 2024 for Accenture.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Genai deals in 2024 for Accenture\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture has made significant investments in Genai technology and training, positioning the company for strong growth in FY25. They have prioritized large-scale transformations for clients and have achieved significant financial success, with full fiscal year bookings of $81 billion and revenues of $65 billion. The company's leadership in Genai technology is demonstrated by their successful implementation at companies such as BBBA, where Genai is helping to reinvent business models and improve operational efficiency. Accenture's focus on automation, innovation, and digital capabilities is driving value for clients and contributing to their continued success in the market.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should summarize the information on specific Genai deals in 2024 for Accenture.\n",
      "Action: Summarize\n",
      "Action Input: Genai deals in 2024 for Accenture\u001b[0m\n",
      "Observation: Summarize is not a valid tool, try one of [VectorStore Retrieval, Google Search].\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should use the information gathered from VectorStore Retrieval to provide a final answer.\n",
      "Final Answer: Accenture has made significant investments in Genai technology and training in 2024, positioning the company for strong growth.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information about Accenture's projects and their value in 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Genai projects value in 2024 Accenture\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn 2024, Accenture projects significant value in Gen AI, with $450 million in sales demonstrating leadership in the field. They have doubled down on their strategy to be the reinvention partner for clients, resulting in $81 billion in bookings and $65 billion in revenues. The company has invested in their workforce and talent development, with a goal of 80,000 practitioners by the end of FY26, and has received global recognition for diversity and inclusion efforts. Additionally, they have expanded their operating margin, delivered EPS growth, and returned $7.8 billion of cash to shareholders.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now have the information needed to answer the question.\n",
      "Final Answer: The value of Genai projects in 2024 for Accenture is $450 million.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information about Genai projects and their value in 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Genai projects value in 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBased on the documents provided, Genai projects are expected to continue to grow in value in 2024, with significant increases in Genai sales and revenue compared to the previous year. The company has achieved important milestones in Genai, with over $2 billion in sales year to date and $500 million in revenue year to date. The company is focused on expanding in new growth areas, strategic acquisitions, and building expertise in Genai to help clients reinvent and benefit from this technology. The company has also seen strong performance in client bookings and market share, with a record number of clients with large transformational wins and increased bookings in Genai. Additionally, the company is investing in its workforce and continuing to deliver value for stakeholders.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe value of Genai projects in 2024 is expected to be significant based on the information retrieved.\n",
      "Final Answer: The value of Genai projects in 2024 is expected to continue to grow significantly.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information about Accenture's earnings in the fourth quarter of 2024.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: EPS increase fourth quarter 2024 Accenture\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn the fourth quarter of 2024, Accenture experienced an increase in earnings per share (EPS) of 2% over the previous year. This growth was driven by the company's focus on helping clients with digital transformation and reinvention using technology, data, AI, and new ways of working, particularly through their investment in Gen AI. Additionally, Accenture delivered strong financial performance, with revenues of $65 billion for the year, representing 2% growth in local currency, and a cash flow of $8.6 billion, reflecting a strong cash flow to net income ratio of 1.2. The company also returned $7.8 billion in cash to shareholders and made significant investments in acquisitions to drive future growth.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: In the fourth quarter of 2024, Accenture's EPS increased by 2% over the previous year.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# GRADIO only with TEXT - REFINED!\n",
    "\n",
    "import json\n",
    "import openai\n",
    "import gradio as gr\n",
    "\n",
    "def gradio_chatbot(user_query):\n",
    "    # Determine the source using the routing function\n",
    "    datasource = route_query(user_query)\n",
    "    \n",
    "    # Tag source based on the datasource returned\n",
    "    source_label = \"VectorDB\" if datasource == \"vectorstore\" else \"Google Search\" if datasource == \"websearch\" else \"Unknown Source\"\n",
    "    \n",
    "    try:\n",
    "        # Get response from the ReAct agent\n",
    "        response = react_agent({\"input\": user_query})\n",
    "\n",
    "        # Check if the response is not empty and parse the response JSON if possible\n",
    "        if response and \"output\" in response:\n",
    "            output_content = response[\"output\"]\n",
    "            \n",
    "            # Attempt to parse JSON response if available\n",
    "            try:\n",
    "                parsed_response = json.loads(output_content) if isinstance(output_content, str) else output_content\n",
    "                answer = parsed_response.get(\"answer\", output_content) if isinstance(parsed_response, dict) else output_content\n",
    "            except json.JSONDecodeError:\n",
    "                answer = output_content  # Fallback to raw output if JSON parsing fails\n",
    "\n",
    "            return f\"Answer (from {source_label}):\\n{answer}\"\n",
    "\n",
    "        else:\n",
    "            return \"Error: Received an empty or unexpected response format from the agent.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Capture any other errors\n",
    "        return f\"Error encountered: {str(e)}\"\n",
    "\n",
    "# Create the Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_chatbot,  # Function to call\n",
    "    inputs=\"text\",      # Input type: text\n",
    "    outputs=\"text\",     # Output type: text\n",
    "    title=\"Accenture Q&A Chatbot\",\n",
    "    description=\"Ask any question about Accenture and get answers with sourced information.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 55\u001b[0m\n\u001b[1;32m     46\u001b[0m iface \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mInterface(\n\u001b[1;32m     47\u001b[0m     fn\u001b[38;5;241m=\u001b[39mgradio_chatbot,  \u001b[38;5;66;03m# Function to call\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m,      \u001b[38;5;66;03m# Input type: text\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsk any question about Accenture and get answers with sourced information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Launch the Gradio app\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43miface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshare\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py:2609\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, _frontend)\u001b[0m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2609\u001b[0m         share_url \u001b[38;5;241m=\u001b[39m \u001b[43mnetworking\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_tunnel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshare_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshare_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshare_server_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshare_server_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2615\u001b[0m         parsed_url \u001b[38;5;241m=\u001b[39m urlparse(share_url)\n\u001b[1;32m   2616\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_url \u001b[38;5;241m=\u001b[39m urlunparse(\n\u001b[1;32m   2617\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_server_protocol,) \u001b[38;5;241m+\u001b[39m parsed_url[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m   2618\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/networking.py:47\u001b[0m, in \u001b[0;36msetup_tunnel\u001b[0;34m(local_host, local_port, share_token, share_server_address)\u001b[0m\n\u001b[1;32m     45\u001b[0m     remote_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(remote_port)\n\u001b[1;32m     46\u001b[0m tunnel \u001b[38;5;241m=\u001b[39m Tunnel(remote_host, remote_port, local_host, local_port, share_token)\n\u001b[0;32m---> 47\u001b[0m address \u001b[38;5;241m=\u001b[39m \u001b[43mtunnel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_tunnel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m address\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/tunneling.py:103\u001b[0m, in \u001b[0;36mTunnel.start_tunnel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_tunnel\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_binary()\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_tunnel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBINARY_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/tunneling.py:138\u001b[0m, in \u001b[0;36mTunnel._start_tunnel\u001b[0;34m(self, binary)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m    135\u001b[0m     command, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkill)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_url_from_tunnel_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/tunneling.py:160\u001b[0m, in \u001b[0;36mTunnel._read_url_from_tunnel_stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GRADIO ONLY WITH TEXT!\n",
    "\n",
    "import json\n",
    "import openai\n",
    "import gradio as gr\n",
    "\n",
    "def gradio_chatbot(user_query):\n",
    "    # Function to interact with the chatbot using Gradio\n",
    "    try:\n",
    "        # Get response from the ReAct agent\n",
    "        response = react_agent({\"input\": user_query})\n",
    "\n",
    "        # Debug: Print log content to verify tool identification\n",
    "        log_content = response.get(\"log\", \"\")\n",
    "        print(\"Debug - Log Content:\", log_content)\n",
    "\n",
    "        # Check if the response is not empty and parse the response JSON if possible\n",
    "        if response and \"output\" in response:\n",
    "            output_content = response[\"output\"]\n",
    "            \n",
    "            # Attempt to parse JSON response if available\n",
    "            try:\n",
    "                parsed_response = json.loads(output_content) if isinstance(output_content, str) else output_content\n",
    "                answer = parsed_response.get(\"answer\", output_content) if isinstance(parsed_response, dict) else output_content\n",
    "            except json.JSONDecodeError:\n",
    "                answer = output_content  # Fallback to raw output if JSON parsing fails\n",
    "            \n",
    "            # Determine the source based on the actual tool used, with more robust checking\n",
    "            if \"Google Search\" in log_content:\n",
    "                source_tool = \"Websearch\"\n",
    "            elif \"VectorStore Retrieval\" in log_content or \"VectorDB\" in log_content:\n",
    "                source_tool = \"VectorDB\"\n",
    "            else:\n",
    "                source_tool = \"Unknown Source\"  # In case it fails to detect any known tool\n",
    "\n",
    "            return f\"Answer (from {source_tool}):\\n{answer}\"\n",
    "\n",
    "        else:\n",
    "            return \"Error: Received an empty or unexpected response format from the agent.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Capture any other errors\n",
    "        return f\"Error encountered: {str(e)}\"\n",
    "\n",
    "# Create the Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_chatbot,  # Function to call\n",
    "    inputs=\"text\",      # Input type: text\n",
    "    outputs=\"text\",     # Output type: text\n",
    "    title=\"Accenture Q&A Chatbot\",\n",
    "    description=\"Ask any question about Accenture and get answers with sourced information.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.27.0)\n",
      "Collecting openai\n",
      "  Using cached openai-1.53.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Using cached openai-1.53.1-py3-none-any.whl (387 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.27.0\n",
      "    Uninstalling openai-0.27.0:\n",
      "      Successfully uninstalled openai-0.27.0\n",
      "Successfully installed openai-1.53.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install openai 0.27.0\n",
    "# !pip install -U openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* Running on public URL: https://2a694c9c7d9ca5ea05.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2a694c9c7d9ca5ea05.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information about Accenture's revenue.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: \"Accenture revenue last quarter\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture's revenue last quarter was $15.8 billion, which was flat compared to the previous year.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should also double-check this information with a Google search to ensure accuracy.\n",
      "Action: Google Search\n",
      "Action Input: Accenture revenue last quarter\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAccenture reported a revenue of $12.8 billion for the last quarter.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThere seems to be a discrepancy in the revenue information between the vector store and Google search results. I should try to verify this information from a reliable source.\n",
      "Action: Google Search\n",
      "Action Input: Accenture financial report last quarter\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe search results provide information about Accenture's financial performance in the last quarter, including revenue, profit, and growth metrics.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information on Accenture's revenue.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture revenue last quarter\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture's revenue last quarter was $15.8 billion, which was flat compared to the previous year.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to find out the revenue for the current quarter.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture revenue current quarter\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccenture revenue for the current quarter was $15.8 billion, which was flat compared to the previous year but slightly above the midpoint of their range.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe revenue information for Accenture is consistent across quarters.\n",
      "Final Answer: The revenue of Accenture for the last quarter was $15.8 billion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use VectorStore Retrieval to find information on Accenture's earnings.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: \"Accenture fourth quarter 2024 EPS\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe EPS for Accenture's fourth quarter 2024 was not provided in the documents provided.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should try using Google Search instead to find this specific information.\n",
      "Action: Google Search\n",
      "Action Input: \"Accenture fourth quarter 2024 EPS\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe search results for Accenture fourth quarter 2024 EPS did not provide any specific information.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI may need to wait for the official earnings report to be released to find this specific information.\n",
      "Final Answer: The EPS for the fourth quarter 2024 for Accenture is currently not available.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the VectorStore Retrieval tool to find employee details for Accenture.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture employee details\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe Accenture employee details focus on talent strategy, diversity and inclusion, well-being, skill development, and community impact. Accenture is dedicated to creating a diverse and inclusive workplace, providing opportunities for skill development, promoting well-being, and making a positive impact in communities. The company has been recognized for its efforts in gender equality, diversity, and inclusion, as well as its commitment to environmental sustainability.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to be more specific in my query to find the total number of employees at Accenture.\n",
      "Action: VectorStore Retrieval\n",
      "Action Input: Accenture total number of employees\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApproximately 733,000 employees.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Approximately 733,000 employees.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# GRADIO WITH AUDIO! \n",
    "\n",
    "import openai\n",
    "import gradio as gr\n",
    "\n",
    "# Transcription function for audio input using the latest Whisper API\n",
    "def transcribe_audio(audio_path):\n",
    "    try:\n",
    "        with open(audio_path, \"rb\") as audio_file:\n",
    "            # Correct method for transcribing audio in latest OpenAI version\n",
    "            response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "            transcription = response[\"text\"]\n",
    "            print(f\"Transcription Result: {transcription}\")\n",
    "            return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return None\n",
    "\n",
    "def gradio_chatbot(user_query=None, audio_query=None):\n",
    "    # Transcribe audio if provided\n",
    "    if audio_query is not None:\n",
    "        user_query = transcribe_audio(audio_query)\n",
    "    \n",
    "    # Check if there's valid input from either text or audio\n",
    "    if not user_query:\n",
    "        return \"Error: No valid input provided. Please either type or speak your question.\"\n",
    "\n",
    "    # Process the query with the ReAct agent (replace 'react_agent' with your actual agent)\n",
    "    try:\n",
    "        response = react_agent({\"input\": user_query})\n",
    "        # Extract the output and source from the response\n",
    "        if \"output\" in response:\n",
    "            source_tool = \"VectorDB\" if \"VectorStore Retrieval\" in response.get(\"log\", \"\") else \"Websearch\"\n",
    "            answer = response[\"output\"]\n",
    "            return f\"Answer (from {source_tool}): {answer}\"\n",
    "        else:\n",
    "            return \"Error: Unable to process the request. Please try again.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}\")\n",
    "        return f\"Error encountered: {str(e)}\"\n",
    "\n",
    "# Gradio interface with audio and text inputs\n",
    "gr.Interface(\n",
    "    fn=gradio_chatbot,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Type your question here\"),\n",
    "        gr.Audio(type=\"filepath\", label=\"Or speak your question here\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Accenture Q&A Chatbot\",\n",
    "    description=\"Ask any question about Accenture and get answers with sourced information.\"\n",
    ").launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monday - Finetune Retrieval and QA and Chatbot!\n",
    "\n",
    "# Check Next code, separated for finetuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40/3036245762.py:108: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
      "/tmp/ipykernel_40/3036245762.py:109: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/tmp/ipykernel_40/3036245762.py:111: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  react_agent = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 1/4: https://www.youtube.com/watch?v=089JqnNmJq0\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=089JqnNmJq0\n",
      "[youtube] 089JqnNmJq0: Downloading webpage\n",
      "[youtube] 089JqnNmJq0: Downloading ios player API JSON\n",
      "[youtube] 089JqnNmJq0: Downloading mweb player API JSON\n",
      "[youtube] 089JqnNmJq0: Downloading player 4e23410d\n",
      "[youtube] 089JqnNmJq0: Downloading m3u8 information\n",
      "[info] 089JqnNmJq0: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_0\n",
      "[download] 100% of   49.02MiB in 00:00:01 at 43.39MiB/s    \n",
      "[ExtractAudio] Destination: audio_0.mp3\n",
      "Deleting original file audio_0 (pass -k to keep)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 181MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 of video 1\n",
      "Completed chunk 1 for video 1\n",
      "Processing chunk 2 of video 1\n",
      "Completed chunk 2 for video 1\n",
      "Processing chunk 3 of video 1\n",
      "Completed chunk 3 for video 1\n",
      "Processing chunk 4 of video 1\n",
      "Completed chunk 4 for video 1\n",
      "Processing chunk 5 of video 1\n",
      "Completed chunk 5 for video 1\n",
      "Processing chunk 6 of video 1\n",
      "Completed chunk 6 for video 1\n",
      "Processing chunk 7 of video 1\n",
      "Completed chunk 7 for video 1\n",
      "Processing chunk 8 of video 1\n",
      "Completed chunk 8 for video 1\n",
      "Processing chunk 9 of video 1\n",
      "Completed chunk 9 for video 1\n",
      "Processing chunk 10 of video 1\n",
      "Completed chunk 10 for video 1\n",
      "Processing chunk 11 of video 1\n",
      "Completed chunk 11 for video 1\n",
      "Processing chunk 12 of video 1\n",
      "Completed chunk 12 for video 1\n",
      "Processing chunk 13 of video 1\n",
      "Completed chunk 13 for video 1\n",
      "Processing chunk 14 of video 1\n",
      "Completed chunk 14 for video 1\n",
      "Processing chunk 15 of video 1\n",
      "Completed chunk 15 for video 1\n",
      "Processing chunk 16 of video 1\n",
      "Completed chunk 16 for video 1\n",
      "Processing chunk 17 of video 1\n",
      "Completed chunk 17 for video 1\n",
      "Processing chunk 18 of video 1\n",
      "Completed chunk 18 for video 1\n",
      "Processing chunk 19 of video 1\n",
      "Completed chunk 19 for video 1\n",
      "Processing chunk 20 of video 1\n",
      "Completed chunk 20 for video 1\n",
      "Processing chunk 21 of video 1\n",
      "Completed chunk 21 for video 1\n",
      "Processing chunk 22 of video 1\n",
      "Completed chunk 22 for video 1\n",
      "Processing chunk 23 of video 1\n",
      "Completed chunk 23 for video 1\n",
      "Processing chunk 24 of video 1\n",
      "Completed chunk 24 for video 1\n",
      "Processing chunk 25 of video 1\n",
      "Completed chunk 25 for video 1\n",
      "Processing chunk 26 of video 1\n",
      "Completed chunk 26 for video 1\n",
      "Processing chunk 27 of video 1\n",
      "Completed chunk 27 for video 1\n",
      "Processing chunk 28 of video 1\n",
      "Completed chunk 28 for video 1\n",
      "Processing chunk 29 of video 1\n",
      "Completed chunk 29 for video 1\n",
      "Processing chunk 30 of video 1\n",
      "Completed chunk 30 for video 1\n",
      "Processing chunk 31 of video 1\n",
      "Completed chunk 31 for video 1\n",
      "Processing chunk 32 of video 1\n",
      "Completed chunk 32 for video 1\n",
      "Processing chunk 33 of video 1\n",
      "Completed chunk 33 for video 1\n",
      "Processing chunk 34 of video 1\n",
      "Completed chunk 34 for video 1\n",
      "Processing chunk 35 of video 1\n",
      "Completed chunk 35 for video 1\n",
      "Processing chunk 36 of video 1\n",
      "Completed chunk 36 for video 1\n",
      "Processing chunk 37 of video 1\n",
      "Completed chunk 37 for video 1\n",
      "Processing chunk 38 of video 1\n",
      "Completed chunk 38 for video 1\n",
      "Processing video 2/4: https://www.youtube.com/watch?v=IskUO7MoV2s\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=IskUO7MoV2s\n",
      "[youtube] IskUO7MoV2s: Downloading webpage\n",
      "[youtube] IskUO7MoV2s: Downloading ios player API JSON\n",
      "[youtube] IskUO7MoV2s: Downloading mweb player API JSON\n",
      "[youtube] IskUO7MoV2s: Downloading m3u8 information\n",
      "[info] IskUO7MoV2s: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_1\n",
      "[download] 100% of   48.50MiB in 00:00:01 at 31.16MiB/s    \n",
      "[ExtractAudio] Destination: audio_1.mp3\n",
      "Deleting original file audio_1 (pass -k to keep)\n",
      "Processing chunk 1 of video 2\n",
      "Completed chunk 1 for video 2\n",
      "Processing chunk 2 of video 2\n",
      "Completed chunk 2 for video 2\n",
      "Processing chunk 3 of video 2\n",
      "Completed chunk 3 for video 2\n",
      "Processing chunk 4 of video 2\n",
      "Completed chunk 4 for video 2\n",
      "Processing chunk 5 of video 2\n",
      "Completed chunk 5 for video 2\n",
      "Processing chunk 6 of video 2\n",
      "Completed chunk 6 for video 2\n",
      "Processing chunk 7 of video 2\n",
      "Completed chunk 7 for video 2\n",
      "Processing chunk 8 of video 2\n",
      "Completed chunk 8 for video 2\n",
      "Processing chunk 9 of video 2\n",
      "Completed chunk 9 for video 2\n",
      "Processing chunk 10 of video 2\n",
      "Completed chunk 10 for video 2\n",
      "Processing chunk 11 of video 2\n",
      "Completed chunk 11 for video 2\n",
      "Processing chunk 12 of video 2\n",
      "Completed chunk 12 for video 2\n",
      "Processing chunk 13 of video 2\n",
      "Completed chunk 13 for video 2\n",
      "Processing chunk 14 of video 2\n",
      "Completed chunk 14 for video 2\n",
      "Processing chunk 15 of video 2\n",
      "Completed chunk 15 for video 2\n",
      "Processing chunk 16 of video 2\n",
      "Completed chunk 16 for video 2\n",
      "Processing chunk 17 of video 2\n",
      "Completed chunk 17 for video 2\n",
      "Processing chunk 18 of video 2\n",
      "Completed chunk 18 for video 2\n",
      "Processing chunk 19 of video 2\n",
      "Completed chunk 19 for video 2\n",
      "Processing chunk 20 of video 2\n",
      "Completed chunk 20 for video 2\n",
      "Processing chunk 21 of video 2\n",
      "Completed chunk 21 for video 2\n",
      "Processing chunk 22 of video 2\n",
      "Completed chunk 22 for video 2\n",
      "Processing chunk 23 of video 2\n",
      "Completed chunk 23 for video 2\n",
      "Processing chunk 24 of video 2\n",
      "Completed chunk 24 for video 2\n",
      "Processing chunk 25 of video 2\n",
      "Completed chunk 25 for video 2\n",
      "Processing chunk 26 of video 2\n",
      "Completed chunk 26 for video 2\n",
      "Processing chunk 27 of video 2\n",
      "Completed chunk 27 for video 2\n",
      "Processing chunk 28 of video 2\n",
      "Completed chunk 28 for video 2\n",
      "Processing chunk 29 of video 2\n",
      "Completed chunk 29 for video 2\n",
      "Processing chunk 30 of video 2\n",
      "Completed chunk 30 for video 2\n",
      "Processing chunk 31 of video 2\n",
      "Completed chunk 31 for video 2\n",
      "Processing chunk 32 of video 2\n",
      "Completed chunk 32 for video 2\n",
      "Processing chunk 33 of video 2\n",
      "Completed chunk 33 for video 2\n",
      "Processing chunk 34 of video 2\n",
      "Completed chunk 34 for video 2\n",
      "Processing chunk 35 of video 2\n",
      "Completed chunk 35 for video 2\n",
      "Processing chunk 36 of video 2\n",
      "Completed chunk 36 for video 2\n",
      "Processing chunk 37 of video 2\n",
      "Completed chunk 37 for video 2\n",
      "Processing chunk 38 of video 2\n",
      "Completed chunk 38 for video 2\n",
      "Processing video 3/4: https://www.youtube.com/watch?v=BbqwBMct8-0\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=BbqwBMct8-0\n",
      "[youtube] BbqwBMct8-0: Downloading webpage\n",
      "[youtube] BbqwBMct8-0: Downloading ios player API JSON\n",
      "[youtube] BbqwBMct8-0: Downloading mweb player API JSON\n",
      "[youtube] BbqwBMct8-0: Downloading m3u8 information\n",
      "[info] BbqwBMct8-0: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_2\n",
      "[download] 100% of   50.65MiB in 00:00:02 at 17.72MiB/s    \n",
      "[ExtractAudio] Destination: audio_2.mp3\n",
      "Deleting original file audio_2 (pass -k to keep)\n",
      "Processing chunk 1 of video 3\n",
      "Completed chunk 1 for video 3\n",
      "Processing chunk 2 of video 3\n",
      "Completed chunk 2 for video 3\n",
      "Processing chunk 3 of video 3\n",
      "Completed chunk 3 for video 3\n",
      "Processing chunk 4 of video 3\n",
      "Completed chunk 4 for video 3\n",
      "Processing chunk 5 of video 3\n",
      "Completed chunk 5 for video 3\n",
      "Processing chunk 6 of video 3\n",
      "Completed chunk 6 for video 3\n",
      "Processing chunk 7 of video 3\n",
      "Completed chunk 7 for video 3\n",
      "Processing chunk 8 of video 3\n",
      "Completed chunk 8 for video 3\n",
      "Processing chunk 9 of video 3\n",
      "Completed chunk 9 for video 3\n",
      "Processing chunk 10 of video 3\n",
      "Completed chunk 10 for video 3\n",
      "Processing chunk 11 of video 3\n",
      "Completed chunk 11 for video 3\n",
      "Processing chunk 12 of video 3\n",
      "Completed chunk 12 for video 3\n",
      "Processing chunk 13 of video 3\n",
      "Completed chunk 13 for video 3\n",
      "Processing chunk 14 of video 3\n",
      "Completed chunk 14 for video 3\n",
      "Processing chunk 15 of video 3\n",
      "Completed chunk 15 for video 3\n",
      "Processing chunk 16 of video 3\n",
      "Completed chunk 16 for video 3\n",
      "Processing chunk 17 of video 3\n",
      "Completed chunk 17 for video 3\n",
      "Processing chunk 18 of video 3\n",
      "Completed chunk 18 for video 3\n",
      "Processing chunk 19 of video 3\n",
      "Completed chunk 19 for video 3\n",
      "Processing chunk 20 of video 3\n",
      "Completed chunk 20 for video 3\n",
      "Processing chunk 21 of video 3\n",
      "Completed chunk 21 for video 3\n",
      "Processing chunk 22 of video 3\n",
      "Completed chunk 22 for video 3\n",
      "Processing chunk 23 of video 3\n",
      "Completed chunk 23 for video 3\n",
      "Processing chunk 24 of video 3\n",
      "Completed chunk 24 for video 3\n",
      "Processing chunk 25 of video 3\n",
      "Completed chunk 25 for video 3\n",
      "Processing chunk 26 of video 3\n",
      "Completed chunk 26 for video 3\n",
      "Processing chunk 27 of video 3\n",
      "Completed chunk 27 for video 3\n",
      "Processing chunk 28 of video 3\n",
      "Completed chunk 28 for video 3\n",
      "Processing chunk 29 of video 3\n",
      "Completed chunk 29 for video 3\n",
      "Processing chunk 30 of video 3\n",
      "Completed chunk 30 for video 3\n",
      "Processing chunk 31 of video 3\n",
      "Completed chunk 31 for video 3\n",
      "Processing chunk 32 of video 3\n",
      "Completed chunk 32 for video 3\n",
      "Processing chunk 33 of video 3\n",
      "Completed chunk 33 for video 3\n",
      "Processing chunk 34 of video 3\n",
      "Completed chunk 34 for video 3\n",
      "Processing chunk 35 of video 3\n",
      "Completed chunk 35 for video 3\n",
      "Processing chunk 36 of video 3\n",
      "Completed chunk 36 for video 3\n",
      "Processing chunk 37 of video 3\n",
      "Completed chunk 37 for video 3\n",
      "Processing video 4/4: https://www.youtube.com/watch?v=hQGtKBQc_iE\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=hQGtKBQc_iE\n",
      "[youtube] hQGtKBQc_iE: Downloading webpage\n",
      "[youtube] hQGtKBQc_iE: Downloading ios player API JSON\n",
      "[youtube] hQGtKBQc_iE: Downloading mweb player API JSON\n",
      "[youtube] hQGtKBQc_iE: Downloading m3u8 information\n",
      "[info] hQGtKBQc_iE: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_3\n",
      "[download] 100% of   60.94MiB in 00:00:03 at 18.03MiB/s    \n",
      "[ExtractAudio] Destination: audio_3.mp3\n",
      "Deleting original file audio_3 (pass -k to keep)\n",
      "Processing chunk 1 of video 4\n",
      "Completed chunk 1 for video 4\n",
      "Processing chunk 2 of video 4\n",
      "Completed chunk 2 for video 4\n",
      "Processing chunk 3 of video 4\n",
      "Completed chunk 3 for video 4\n",
      "Processing chunk 4 of video 4\n",
      "Completed chunk 4 for video 4\n",
      "Processing chunk 5 of video 4\n",
      "Completed chunk 5 for video 4\n",
      "Processing chunk 6 of video 4\n",
      "Completed chunk 6 for video 4\n",
      "Processing chunk 7 of video 4\n",
      "Completed chunk 7 for video 4\n",
      "Processing chunk 8 of video 4\n",
      "Completed chunk 8 for video 4\n",
      "Processing chunk 9 of video 4\n",
      "Completed chunk 9 for video 4\n",
      "Processing chunk 10 of video 4\n",
      "Completed chunk 10 for video 4\n",
      "Processing chunk 11 of video 4\n",
      "Completed chunk 11 for video 4\n",
      "Processing chunk 12 of video 4\n",
      "Completed chunk 12 for video 4\n",
      "Processing chunk 13 of video 4\n",
      "Completed chunk 13 for video 4\n",
      "Processing chunk 14 of video 4\n",
      "Completed chunk 14 for video 4\n",
      "Processing chunk 15 of video 4\n",
      "Completed chunk 15 for video 4\n",
      "Processing chunk 16 of video 4\n",
      "Completed chunk 16 for video 4\n",
      "Processing chunk 17 of video 4\n",
      "Completed chunk 17 for video 4\n",
      "Processing chunk 18 of video 4\n",
      "Completed chunk 18 for video 4\n",
      "Processing chunk 19 of video 4\n",
      "Completed chunk 19 for video 4\n",
      "Processing chunk 20 of video 4\n",
      "Completed chunk 20 for video 4\n",
      "Processing chunk 21 of video 4\n",
      "Completed chunk 21 for video 4\n",
      "Processing chunk 22 of video 4\n",
      "Completed chunk 22 for video 4\n",
      "Processing chunk 23 of video 4\n",
      "Completed chunk 23 for video 4\n",
      "Processing chunk 24 of video 4\n",
      "Completed chunk 24 for video 4\n",
      "Processing chunk 25 of video 4\n",
      "Completed chunk 25 for video 4\n",
      "Processing chunk 26 of video 4\n",
      "Completed chunk 26 for video 4\n",
      "Processing chunk 27 of video 4\n",
      "Completed chunk 27 for video 4\n",
      "Processing chunk 28 of video 4\n",
      "Completed chunk 28 for video 4\n",
      "Processing chunk 29 of video 4\n",
      "Completed chunk 29 for video 4\n",
      "Processing chunk 30 of video 4\n",
      "Completed chunk 30 for video 4\n",
      "Processing chunk 31 of video 4\n",
      "Completed chunk 31 for video 4\n",
      "Processing chunk 32 of video 4\n",
      "Completed chunk 32 for video 4\n",
      "Processing chunk 33 of video 4\n",
      "Completed chunk 33 for video 4\n",
      "Processing chunk 34 of video 4\n",
      "Completed chunk 34 for video 4\n",
      "Processing chunk 35 of video 4\n",
      "Completed chunk 35 for video 4\n",
      "Processing chunk 36 of video 4\n",
      "Completed chunk 36 for video 4\n",
      "Processing chunk 37 of video 4\n",
      "Completed chunk 37 for video 4\n",
      "Processing chunk 38 of video 4\n",
      "Completed chunk 38 for video 4\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40/3036245762.py:123: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = react_agent({\"input\": user_query, \"transcriptions\": transcriptions})  # Send entire list to agent once\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI should transcribe all the earnings podcasts from the YouTube links provided to consolidate the results.\n",
      "Action: TranscribeAllVideos\n",
      "Action Input: List of YouTube links to earnings podcasts\u001b[0mProcessing video 1/4: https://www.youtube.com/watch?v=089JqnNmJq0\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=089JqnNmJq0\n",
      "[youtube] 089JqnNmJq0: Downloading webpage\n",
      "[youtube] 089JqnNmJq0: Downloading ios player API JSON\n",
      "[youtube] 089JqnNmJq0: Downloading mweb player API JSON\n",
      "[youtube] 089JqnNmJq0: Downloading m3u8 information\n",
      "[info] 089JqnNmJq0: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_0\n",
      "[download] 100% of   49.02MiB in 00:00:01 at 43.23MiB/s    \n",
      "[ExtractAudio] Destination: audio_0.mp3\n",
      "Deleting original file audio_0 (pass -k to keep)\n",
      "Processing chunk 1 of video 1\n",
      "Completed chunk 1 for video 1\n",
      "Processing chunk 2 of video 1\n",
      "Completed chunk 2 for video 1\n",
      "Processing chunk 3 of video 1\n",
      "Completed chunk 3 for video 1\n",
      "Processing chunk 4 of video 1\n",
      "Completed chunk 4 for video 1\n",
      "Processing chunk 5 of video 1\n",
      "Completed chunk 5 for video 1\n",
      "Processing chunk 6 of video 1\n",
      "Completed chunk 6 for video 1\n",
      "Processing chunk 7 of video 1\n",
      "Completed chunk 7 for video 1\n",
      "Processing chunk 8 of video 1\n",
      "Completed chunk 8 for video 1\n",
      "Processing chunk 9 of video 1\n",
      "Completed chunk 9 for video 1\n",
      "Processing chunk 10 of video 1\n",
      "Completed chunk 10 for video 1\n",
      "Processing chunk 11 of video 1\n",
      "Completed chunk 11 for video 1\n",
      "Processing chunk 12 of video 1\n",
      "Completed chunk 12 for video 1\n",
      "Processing chunk 13 of video 1\n",
      "Completed chunk 13 for video 1\n",
      "Processing chunk 14 of video 1\n",
      "Completed chunk 14 for video 1\n",
      "Processing chunk 15 of video 1\n",
      "Completed chunk 15 for video 1\n",
      "Processing chunk 16 of video 1\n",
      "Completed chunk 16 for video 1\n",
      "Processing chunk 17 of video 1\n",
      "Completed chunk 17 for video 1\n",
      "Processing chunk 18 of video 1\n",
      "Completed chunk 18 for video 1\n",
      "Processing chunk 19 of video 1\n",
      "Completed chunk 19 for video 1\n",
      "Processing chunk 20 of video 1\n",
      "Completed chunk 20 for video 1\n",
      "Processing chunk 21 of video 1\n",
      "Completed chunk 21 for video 1\n",
      "Processing chunk 22 of video 1\n",
      "Completed chunk 22 for video 1\n",
      "Processing chunk 23 of video 1\n",
      "Completed chunk 23 for video 1\n",
      "Processing chunk 24 of video 1\n",
      "Completed chunk 24 for video 1\n",
      "Processing chunk 25 of video 1\n",
      "Completed chunk 25 for video 1\n",
      "Processing chunk 26 of video 1\n",
      "Completed chunk 26 for video 1\n",
      "Processing chunk 27 of video 1\n",
      "Completed chunk 27 for video 1\n",
      "Processing chunk 28 of video 1\n",
      "Completed chunk 28 for video 1\n",
      "Processing chunk 29 of video 1\n",
      "Completed chunk 29 for video 1\n",
      "Processing chunk 30 of video 1\n",
      "Completed chunk 30 for video 1\n",
      "Processing chunk 31 of video 1\n",
      "Completed chunk 31 for video 1\n",
      "Processing chunk 32 of video 1\n",
      "Completed chunk 32 for video 1\n",
      "Processing chunk 33 of video 1\n",
      "Completed chunk 33 for video 1\n",
      "Processing chunk 34 of video 1\n",
      "Completed chunk 34 for video 1\n",
      "Processing chunk 35 of video 1\n",
      "Completed chunk 35 for video 1\n",
      "Processing chunk 36 of video 1\n",
      "Completed chunk 36 for video 1\n",
      "Processing chunk 37 of video 1\n",
      "Completed chunk 37 for video 1\n",
      "Processing chunk 38 of video 1\n",
      "Completed chunk 38 for video 1\n",
      "Processing video 2/4: https://www.youtube.com/watch?v=IskUO7MoV2s\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=IskUO7MoV2s\n",
      "[youtube] IskUO7MoV2s: Downloading webpage\n",
      "[youtube] IskUO7MoV2s: Downloading ios player API JSON\n",
      "[youtube] IskUO7MoV2s: Downloading mweb player API JSON\n",
      "[youtube] IskUO7MoV2s: Downloading m3u8 information\n",
      "[info] IskUO7MoV2s: Downloading 1 format(s): 251\n",
      "[download] Destination: audio_1\n",
      "[download] 100% of   48.50MiB in 00:00:02 at 17.96MiB/s    \n",
      "[ExtractAudio] Destination: audio_1.mp3\n",
      "Deleting original file audio_1 (pass -k to keep)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 132\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\" def perform_transcription_task():\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    user_query = \"Transcribe all earnings podcasts from the provided YouTube links.\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    response = react_agent({\"input\": user_query})\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    print(\"Agent's Transcription:\", response[\"output\"]) \"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Start the transcription task\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[43mperform_transcription_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 123\u001b[0m, in \u001b[0;36mperform_transcription_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscribe all earnings podcasts from the provided YouTube links.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m transcriptions \u001b[38;5;241m=\u001b[39m transcribe_all_videos()  \u001b[38;5;66;03m# Get transcriptions directly\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mreact_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscriptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscriptions\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Send entire list to agent once\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Transcription:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    387\u001b[0m }\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py:1629\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1629\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1637\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1638\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1639\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py:1335\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1328\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1332\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1335\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1345\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py:1335\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1328\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1332\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1335\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1345\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py:1420\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py:1442\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[0;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1450\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py:689\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[1;32m    688\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(error_to_raise)\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[1;32m    690\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[1;32m    691\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py:657\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run):\n\u001b[1;32m    656\u001b[0m     tool_kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 657\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_and_artifact\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/simple.py:92\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, config, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m     91\u001b[0m         kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync invocation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "Cell \u001b[0;32mIn[7], line 70\u001b[0m, in \u001b[0;36mtranscribe_all_videos\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing video \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(video_links)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m download_audio(video_url, AUDIO_OUTPUT_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(i))\n\u001b[0;32m---> 70\u001b[0m full_transcription \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio_with_whisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m transcription_chunks \u001b[38;5;241m=\u001b[39m chunk_text(full_transcription)  \u001b[38;5;66;03m# Divide transcription\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transcription_chunks):\n",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m, in \u001b[0;36mtranscribe_audio_with_whisper\u001b[0;34m(audio_path, model_name)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribe_audio_with_whisper\u001b[39m(audio_path, model_name\u001b[38;5;241m=\u001b[39mWHISPER_MODEL):\n\u001b[1;32m     43\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_model(model_name)\n\u001b[0;32m---> 44\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:279\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    276\u001b[0m mel_segment \u001b[38;5;241m=\u001b[39m pad_or_trim(mel_segment, N_FRAMES)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[1;32m    278\u001b[0m decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_tokens[prompt_reset_since:]\n\u001b[0;32m--> 279\u001b[0m result: DecodingResult \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(result\u001b[38;5;241m.\u001b[39mtokens)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:195\u001b[0m, in \u001b[0;36mtranscribe.<locals>.decode_with_fallback\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m    192\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    194\u001b[0m options \u001b[38;5;241m=\u001b[39m DecodingOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, temperature\u001b[38;5;241m=\u001b[39mt)\n\u001b[0;32m--> 195\u001b[0m decode_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m needs_fallback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    199\u001b[0m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m decode_result\u001b[38;5;241m.\u001b[39mcompression_ratio \u001b[38;5;241m>\u001b[39m compression_ratio_threshold\n\u001b[1;32m    201\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py:824\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    822\u001b[0m     options \u001b[38;5;241m=\u001b[39m replace(options, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 824\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py:737\u001b[0m, in \u001b[0;36mDecodingTask.run\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    734\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_group, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(audio_features\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[0;32m--> 737\u001b[0m tokens, sum_logprobs, no_speech_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[1;32m    740\u001b[0m audio_features \u001b[38;5;241m=\u001b[39m audio_features[:: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_group]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py:687\u001b[0m, in \u001b[0;36mDecodingTask._main_loop\u001b[0;34m(self, audio_features, tokens)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_len):\n\u001b[0;32m--> 687\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    690\u001b[0m             i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mno_speech \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    691\u001b[0m         ):  \u001b[38;5;66;03m# save no_speech_probs\u001b[39;00m\n\u001b[1;32m    692\u001b[0m             probs_at_sot \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msot_index]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py:163\u001b[0m, in \u001b[0;36mPyTorchInference.logits\u001b[0;34m(self, tokens, audio_features)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_token_length:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py:242\u001b[0m, in \u001b[0;36mTextDecoder.forward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    239\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(xa\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 242\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m    245\u001b[0m logits \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    246\u001b[0m     x \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    247\u001b[0m )\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py:169\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    167\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_ln(x), mask\u001b[38;5;241m=\u001b[39mmask, kv_cache\u001b[38;5;241m=\u001b[39mkv_cache)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn:\n\u001b[0;32m--> 169\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    170\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_ln(x))\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py:112\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    109\u001b[0m     v \u001b[38;5;241m=\u001b[39m kv_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue]\n\u001b[1;32m    111\u001b[0m wv, qk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv_attention(q, k, v, mask)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwv\u001b[49m\u001b[43m)\u001b[49m, qk\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py:45\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLinear\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\n\u001b[1;32m     47\u001b[0m             x,\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[1;32m     50\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# New Code 11PM SATURDAY # THIS CODE worked using ReAct agent for Transcription, but went in Infinite LOOOOP\n",
    "\n",
    "\"\"\" import os\n",
    "import yt_dlp\n",
    "from whisper import load_model\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# Default configurations\n",
    "VIDEO_LINKS_FILE = \"video_links.txt\"  # File with YouTube links\n",
    "AUDIO_OUTPUT_TEMPLATE = \"audio_{}\"  # Template for audio file names without .mp3 extension\n",
    "WHISPER_MODEL = \"base\"  # Whisper model for transcription\n",
    "\n",
    "# Load YouTube links from the file\n",
    "def load_video_links(filename=VIDEO_LINKS_FILE):\n",
    "    with open(filename, 'r') as file:\n",
    "        video_links = [link.strip() for link in file.readlines() if link.strip()]\n",
    "    return video_links\n",
    "\n",
    "# Download audio from YouTube video\n",
    "def download_audio(url, output_name):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [\n",
    "            {\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',\n",
    "            }\n",
    "        ],\n",
    "        'outtmpl': output_name,  # Filename with .mp3 extension\n",
    "        'ffmpeg_location': '/usr/bin/ffmpeg'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "    return f\"{output_name}.mp3\"\n",
    "\n",
    "# Transcribe audio using Whisper\n",
    "def transcribe_audio_with_whisper(audio_path, model_name=WHISPER_MODEL):\n",
    "    model = load_model(model_name)\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# ReAct Tool for downloading and transcribing all videos\n",
    "# Split transcriptions into chunks below the token limit - Process in Batches!\n",
    "def chunk_text(text, max_length=1500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    for word in words:\n",
    "        chunk.append(word)\n",
    "        if len(\" \".join(chunk)) >= max_length:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = []\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n",
    "\n",
    "# Process each transcription in chunks\n",
    "def transcribe_all_videos(input_data=None): # Accept an optional argument\n",
    "    video_links = load_video_links(VIDEO_LINKS_FILE)\n",
    "    all_transcriptions = []\n",
    "    \n",
    "    for i, video_url in enumerate(video_links):\n",
    "        print(f\"Processing video {i+1}/{len(video_links)}: {video_url}\")\n",
    "        audio_path = download_audio(video_url, AUDIO_OUTPUT_TEMPLATE.format(i))\n",
    "        full_transcription = transcribe_audio_with_whisper(audio_path)\n",
    "        transcription_chunks = chunk_text(full_transcription)  # Divide transcription\n",
    "        \n",
    "        for j, chunk in enumerate(transcription_chunks):\n",
    "            # print(f\"Processing chunk {j+1} of video {i+1}\")\n",
    "            # Send chunk to the ReAct agent\n",
    "            # response = react_agent({\"input\": chunk}) /* Don't call Agent each time */\n",
    "            all_transcriptions.append({\"title\": f\"Video {i+1} - Chunk {j+1}\", \"transcription\": chunk})\n",
    "            print(f\"Completed chunk {j+1} for video {i+1}\")\n",
    "        \n",
    "    return all_transcriptions\n",
    "\n",
    "\n",
    "def transcribe_all_videos(input_data=None):  # Accept an optional argument\n",
    "    video_links = load_video_links(VIDEO_LINKS_FILE)\n",
    "    all_transcriptions = []\n",
    "    for i, video_url in enumerate(video_links):\n",
    "        print(f\"Processing video {i+1}/{len(video_links)}: {video_url}\")\n",
    "        audio_path = download_audio(video_url, AUDIO_OUTPUT_TEMPLATE.format(i))\n",
    "        transcription = transcribe_audio_with_whisper(audio_path)\n",
    "        all_transcriptions.append({\"title\": f\"Video {i+1}\", \"transcription\": transcription})\n",
    "        print(f\"Completed transcription for video {i+1}\")\n",
    "    return all_transcriptions\n",
    "\n",
    "# Define structured output for better organization\n",
    "response_schema = [\n",
    "    ResponseSchema(name=\"transcription\", description=\"Transcribed text from each video in structured format.\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "\n",
    "# Define tool for the agent\n",
    "transcription_tool = Tool(\n",
    "    name=\"TranscribeAllVideos\",\n",
    "    func=transcribe_all_videos,\n",
    "    description=\"Transcribes all videos in the provided list and consolidates results.\"\n",
    ")\n",
    "\n",
    "# Initialize the ReAct agent with transcription tool\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "react_agent = initialize_agent(\n",
    "    tools=[transcription_tool],\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    llm=chat_model,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run agent to perform transcription\n",
    "def perform_transcription_task():\n",
    "    user_query = \"Transcribe all earnings podcasts from the provided YouTube links.\"\n",
    "    transcriptions = transcribe_all_videos()  # Get transcriptions directly\n",
    "    response = react_agent({\"input\": user_query, \"transcriptions\": transcriptions})  # Send entire list to agent once\n",
    "    print(\"Agent's Transcription:\", response[\"output\"])\n",
    "\n",
    "def perform_transcription_task():\n",
    "    user_query = \"Transcribe all earnings podcasts from the provided YouTube links.\"\n",
    "    response = react_agent({\"input\": user_query})\n",
    "    print(\"Agent's Transcription:\", response[\"output\"])\n",
    "\n",
    "# Start the transcription task\n",
    "perform_transcription_task()\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
